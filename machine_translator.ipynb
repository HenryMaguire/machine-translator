{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the preprocessed datasets\n",
    "Need the preprocessed data, the id_to_word dictionaries and word_to_id dictionaries for both source and target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 6003.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_obj\n",
    "\n",
    "word2id_s = load_obj(\"DATA/preprocess/word2id_s\")\n",
    "word2id_t = load_obj(\"DATA/preprocess/word2id_t\")\n",
    "id2word_s = load_obj(\"DATA/preprocess/id2word_s\")\n",
    "id2word_t = load_obj(\"DATA/preprocess/id2word_t\")\n",
    "\n",
    "text_source = load_obj(\"DATA/preprocess/source_train\") # Preprocessed already\n",
    "text_target = load_obj(\"DATA/preprocess/target_train\") # Preprocessed already\n",
    "\n",
    "#raw_source_test = load_obj(\"DATA/preprocess/raw_source_test\")\n",
    "#raw_source_test = [seq.split(\" \") for seq in raw_source_test]\n",
    "\n",
    "#raw_target_test = load_obj(\"DATA/preprocess/raw_target_test\")\n",
    "#raw_target_test = [seq.split(\" \") for seq in raw_target_test]\n",
    "\"\"\"\n",
    "\n",
    "def format_text(text):\n",
    "    text = text.replace(\" &apos;\", \"'\")\n",
    "    return text\n",
    "\n",
    "#raw_source_test = eval(\"test_\"+lang_s).read()\n",
    "#raw_target_test = eval(\"test_\"+lang_t).read()\n",
    "print len(raw_source_test.split(\"\\n\")), len(raw_target_test.split(\"\\n\"))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "source_train, source_test = text_source[0:10000], text_source[10000:500]\n",
    "target_train, target_test = text_target[0:10000], text_target[10000:500]\n",
    "print \"Vocab size is {}.\".format(len(word2id_s.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6003\n",
      "this had to be <UNK> since , before the <UNK> citizenship legislation , a married woman was usually seen to share the nationality of her husband . \n",
      "par conséquent , <UNK> fut également l&apos; année au cours de laquelle il <UNK> possible d &quot; être citoyen canadien , en vertu de la nouvelle loi sur la citoyenneté canadienne . \n"
     ]
    }
   ],
   "source": [
    "#from utils import ids_to_phrases\n",
    "print len(word2id_s.keys())\n",
    "vocab_size_t = len(word2id_t.keys())\n",
    "vocab_size_s = len(word2id_s.keys())\n",
    "lang_s = 'fr'\n",
    "lang_t = 'en'\n",
    "print ids_to_phrases(target_train[60], id2word_t)\n",
    "\n",
    "print ids_to_phrases(source_train[60], id2word_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation metric:     BLEU Score\n",
    "\n",
    "Here I use the NLTK implementation of BLEU score to measure how successful the machine translation was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "from utils import remove_EOS_PAD\n",
    "\n",
    "p_phrase1 = [4,5,4,5,4,5, 1, 0]\n",
    "t_phrase = [4,5,6,34,8,76, 87, 1]\n",
    "# Truncate sequences at [1]\n",
    "t_phrase = remove_EOS_PAD(t_phrase)\n",
    "p_phrase1 = remove_EOS_PAD(p_phrase1)\n",
    "\n",
    "print \"BLEU1 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [p_phrase1], weights=([1])))\n",
    "print \"BLEU2 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [p_phrase1], weights=([0.5,0.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Benchmark model\n",
    "\n",
    "Here I use the [GoogleTrans]() python package to translate the corpus by translating each individual word in the text. There seems to be an issue with the number of JSON requests the model makes to the Google Translate service, so it needs to work in passes. If a phrase is skipped due to a JSON error, keep it stored and try again later (with a new Translator instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON object could be decoded for phrase 46\n",
      "No JSON object could be decoded for phrase 87\n",
      "No JSON object could be decoded for phrase 132\n",
      "No JSON object could be decoded for phrase 166\n",
      "No JSON object could be decoded for phrase 217\n",
      "No JSON object could be decoded for phrase 263\n",
      "No JSON object could be decoded for phrase 316\n",
      "No JSON object could be decoded for phrase 373\n",
      "No JSON object could be decoded for phrase 430\n",
      "No JSON object could be decoded for phrase 479\n",
      "No JSON object could be decoded for phrase 526\n",
      "No JSON object could be decoded for phrase 581\n",
      "No JSON object could be decoded for phrase 630\n",
      "No JSON object could be decoded for phrase 683\n",
      "No JSON object could be decoded for phrase 742\n",
      "No JSON object could be decoded for phrase 798\n",
      "No JSON object could be decoded for phrase 1025\n",
      "No JSON object could be decoded for phrase 1074\n",
      "No JSON object could be decoded for phrase 1117\n",
      "No JSON object could be decoded for phrase 1329\n",
      "No JSON object could be decoded for phrase 1519\n",
      "No JSON object could be decoded for phrase 1729\n",
      "No JSON object could be decoded for phrase 1933\n",
      "No JSON object could be decoded for phrase 2115\n",
      "No JSON object could be decoded for phrase 2159\n",
      "No JSON object could be decoded for phrase 2221\n",
      "No JSON object could be decoded for phrase 2303\n",
      "No JSON object could be decoded for phrase 2519\n",
      "No JSON object could be decoded for phrase 2572\n",
      "No JSON object could be decoded for phrase 2763\n",
      "No JSON object could be decoded for phrase 3025\n",
      "No JSON object could be decoded for phrase 3296\n",
      "No JSON object could be decoded for phrase 3428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-5897d8416c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrans_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mBM_translated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_word_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_source_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Corpus translated from {} to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-5897d8416c9f>\u001b[0m in \u001b[0;36mtranslate_word_by_word\u001b[0;34m(source_text)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Need to keep track of phrase ordering by labelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msource_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrans_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipped_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogletrans_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     print \"There are {} phrases which could not be translated first time around.\".format(\n\u001b[1;32m     24\u001b[0m                                                                     len(skipped_phrases))\n",
      "\u001b[0;32m<ipython-input-75-5897d8416c9f>\u001b[0m in \u001b[0;36mgoogletrans_pass\u001b[0;34m(s_text, target_lang)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             trans_corpus.append((i, [trans.text for \n\u001b[0;32m---> 11\u001b[0;31m                             trans in translator.translate(phrase, dest=target_lang)]))\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Making a new Translator instance seems to help JSON errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/googletrans/client.pyc\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/googletrans/client.pyc\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/googletrans/client.pyc\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                     token=token)\n\u001b[1;32m     60\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    516\u001b[0m         }\n\u001b[1;32m    517\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     self.__class__)\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "def googletrans_pass(s_text, target_lang='fr'):\n",
    "    translator = Translator()\n",
    "    trans_corpus = []\n",
    "    skipped_phrases = []\n",
    "    for i, phrase in s_text:\n",
    "        try:\n",
    "            trans_corpus.append((i, [trans.text for \n",
    "                            trans in translator.translate(phrase, dest=target_lang)]))\n",
    "        except ValueError as err:\n",
    "            # Making a new Translator instance seems to help JSON errors\n",
    "            translator = Translator()\n",
    "            skipped_phrases.append((i, phrase))\n",
    "            print \"{} for phrase {}\".format(err, i)\n",
    "    return trans_corpus, skipped_phrases\n",
    "\n",
    "def translate_word_by_word(source_text):\n",
    "    # Need to keep track of phrase ordering by labelling\n",
    "    source_text = [(i, phr) for i, phr in enumerate(source_text)]\n",
    "    trans_text, skipped_phrases = googletrans_pass(source_text, target_lang=lang_t)\n",
    "    print \"There are {} phrases which could not be translated first time around.\".format(\n",
    "                                                                    len(skipped_phrases))\n",
    "    # Keep making passes until there are no more untranslated phrases left\n",
    "    j = 2\n",
    "    while len(skipped_phrases)>0:\n",
    "        translated_corpus = []\n",
    "        tc_s, skipped_phrases = googletrans_pass(skipped_phrases, target_lang=lang_t)\n",
    "        trans_text += tc_s\n",
    "        print \"There are {} phrases which could not be translated in pass {}.\".format(\n",
    "                                                            len(skipped_phrases), j)\n",
    "        j+=1\n",
    "    # Sort the phrases via their labels.\n",
    "    # Sorted function gives ([indices], [phrases]) so just need 2nd element\n",
    "    trans_text = zip(*sorted(zip(*BM_translated)))[1]\n",
    "    return trans_text\n",
    "\n",
    "BM_translated = translate_word_by_word(raw_source_test)\n",
    "print \"Corpus translated from {} to {}\".format(lang_s, lang_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une des raisons de l&apos; échec de &quot; The Hunting Party &quot; est que l&apos; on ne laisse simplement pas Simon Hunt être le cynique déserté doté d&apos; un cœur , comme il nous est présenté au début .\n",
      "A of the reasons from l&apos; failure from &quot; The Hunting Party &quot; East what l&apos; on born leash simply not Simon Hunt be the cynical deserted with d&apos; a heart , as the we East present the beginning .\n",
      "One of the reasons why &quot; The Hunting Party &quot; fails is that Simon Hunt isn &apos;t allowed to simply be the run-down cynic with a heart of gold we &apos;re introduced to at the beginning .\n"
     ]
    }
   ],
   "source": [
    "print \" \".join(raw_source_test[60])\n",
    "print \" \".join(BM_translated[60])\n",
    "print \" \".join(raw_target_test[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_targets = [[ids_to_phrases(phrase,id_to_word_t)]for phrase in test_t]\n",
    "\n",
    "BM_BLEU4 = bleu_score.corpus_bleu(test_targets, BM_translated, weights=(0.25,0.25, 0.25,0.25))\n",
    "BM_BLEU2 = bleu_score.corpus_bleu(test_targets, BM_translated, weights=(0.5,0.5))\n",
    "BM_BLEU1 = bleu_score.corpus_bleu(test_targets, BM_translated, weights=([1]))\n",
    "\n",
    "print \"Actual: \\n\", (\" \".join(test_targets[10])).encode('utf-8')\n",
    "print \"Prediction: \\n\", (\" \".join(BM_translated_corp[10])).encode('utf-8')\n",
    "print \"Unigram BLEU score is {}.\".format(BM_BLEU1)\n",
    "print \"Bigram BLEU score is {}.\".format(BM_BLEU2)\n",
    "print \"4-Gram BLEU: {}\".format(BM_BLEU4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word IDs and word-to-vec vectors\n",
    "\n",
    "Since we are interested in the process of learning weights within the RNNs to predict seq2seq mappings rather than embeddings I have chosen to use pretrained word embeddings ino order to cut down training time. [THis is](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Download-the-Embeddings) where the embeddings are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding matrix for fr has 6003 columns and 64 rows.\n",
      "284 vocab words were not in the fr embeddings file.\n",
      "The embedding matrix for en has 6003 columns and 64 rows.\n",
      "201 vocab words were not in the en embeddings file.\n",
      "[u'co-operation', u'90', u'18th', u'1970s', u'wasn', u'2nd', u'250', u'vice-president', u'on-line', u'cihr', u'didn', u'2', u'ngos', u'follow-up', u'2007', u'secretary-general', u'hasn', u'6', u'\\xe2', u'long-term', u'34', u'&quot;', u'qu\\xe9bec', u'2015', u'2011', u'2010', u'2013', u'2012', u'\\u25aa', u'cross-border', u'so-called', u'1991', u'1990', u'1993', u'1992', u'1995', u'1994', u'1997', u'1996', u'1999', u'1998', u'fran\\xe7ois', u'50,000', u'\\u20ac', u'meps', u'1977', u'1976', u'1975', u'1973', u'1970', u'&#93;', u'1980s', u'100,000', u'&#91;', u'couldn', u'150', u'&apos;ll', u'aren', u'600', u'--', u'0', u'&apos;re', u'well-known', u'7', u'&apos;', u'large-scale', u'000', u'doesn', u'fran\\xe7ais', u'20,000', u'&apos;ve', u'2.5', u'well-being', u'120', u'\\u25e6', u'&apos;s', u'42', u'40', u'e-mail', u'andr\\xe9', u'100', u'10,000', u'isn', u'00', u'short-term', u'1990s', u'21st', u'3rd', u'11', u'10', u'13', u'12', u'15', u'14', u'17', u'16', u'19', u'18', u'200', u'4', u'20th', u'\\xe0', u'montr\\xe9al', u'24', u'25', u'26', u'\\u2122', u'20', u'21', u'22', u'23', u'28', u'29', u'01', u'\\u2022', u'8', u'full-time', u'300', u'cbsa', u'2002', u'2003', u'2000', u'2001', u'2006', u'2004', u'2005', u'2008', u'2009', u'39', u'38', u'33', u'32', u'31', u'30', u'37', u'36', u'35', u'3,000', u'2,000', u'1,000', u'&amp;', u'1986', u'1987', u'1984', u'1985', u'1982', u'1983', u'1980', u'1981', u'1988', u'1989', u'48', u'49', u'46', u'47', u'44', u'45', u'43', u'41', u'non-governmental', u'400', u'1979', u'10th', u'\\ufffd', u'1960s', u'weren', u'decision-making', u'55', u'54', u'50', u'53', u'56', u'smes', u'1st', u'iphone', u'1', u'&gt;', u'27', u'60', u'65', u'66', u'500', u'5', u'1945', u'wouldn', u'75', u'72', u'70', u'5,000', u'2.2', u'2.1', u'&apos;d', u'&apos;m', u'&apos;t', u'1950s', u'9', u'19th', u'80', u'1.5', u'1.2', u'i.e.']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "def get_embeddings(id_to_word, lang):\n",
    "    # We load pretrained word2vec embeddings from polyglot to save on training time\n",
    "    filename ='DATA/polyglot-'+lang+'.pkl'\n",
    "    pretrain_vocab, pretrain_embed = pickle.load(open(filename, 'rb'))\n",
    "    embed_vocab = [pretrain_embed[pretrain_vocab.index('<PAD>')], pretrain_embed[pretrain_vocab.index('</S>')]]\n",
    "    skip_count = 0\n",
    "    skipped_words = []\n",
    "    for idx, word in sorted(id_to_word.items()[2::]):\n",
    "        try:\n",
    "            pretrain_idx = pretrain_vocab.index(word)\n",
    "            embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # it could be that the word is a name which needs to \n",
    "                # be capitalized. Try this...\n",
    "                pretrain_idx = pretrain_vocab.index(str(word.title()))\n",
    "                embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # it could be that the word is an achronym which needs to \n",
    "                    # be upper case. Try this...\n",
    "                    pretrain_idx = pretrain_vocab.index(word.upper())\n",
    "                    embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "                except ValueError:\n",
    "                    # Give up trying to find an embedding.\n",
    "                    # How many words are skipped? Which ones?\n",
    "                    skip_count +=1\n",
    "                    skipped_words.append(word)\n",
    "                    # Let's just initialise the embedding to a random normal distribution\n",
    "                    embed_vocab.append(np.random.normal(loc=0.0, scale=np.sqrt(2)/4, size=64))\n",
    "    embed_vocab = np.array(embed_vocab, dtype=np.float32)\n",
    "    print \"The embedding matrix for {} has {} columns and {} rows.\".format(lang, \n",
    "                                                embed_vocab.shape[0], embed_vocab.shape[1])\n",
    "    print \"{} vocab words were not in the {} embeddings file.\".format(skip_count, lang)\n",
    "    return embed_vocab, skipped_words\n",
    "# the ith word in words corresponds to the ith embedding \n",
    "\n",
    "embed_vocab_s, skipped_s = get_embeddings(id2word_s, lang=lang_s)\n",
    "embed_vocab_t, skipped_t = get_embeddings(id2word_t, lang=lang_t)\n",
    "print skipped_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** We can see above ** that the English words which were not in the embedding files are fairly specialist words or numerical values (which are the same in French) so hopefully they won't be too much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 2 4 1]\n",
      " [2 0 2 2]\n",
      " [3 0 0 0]]\n",
      "[[3704 4749 2175 3388 2376 3664 5774]\n",
      " [ 343 3146    2 2644 2063 4274 3447]\n",
      " [3146  446 1170 3939 5806  577 5076]\n",
      " [4274 1187 1843 1201 2894 2072 3146]\n",
      " [4979 5226 2442 4105  509 3146 3341]\n",
      " [ 450 5801 1201 4313 2043 3525 2355]\n",
      " [   2 1170  574 4105 1703 1325  610]\n",
      " [1201 5502 1091 2880 1004 2003 3409]\n",
      " [1187 1703 5746 2355 2009  225    2]\n",
      " [2633 4865 1703 1348 1703 1091 2009]\n",
      " [1707 1527 5816 3320 3013 4110 3409]\n",
      " [ 381 3146 4963 1703 4313  226 3309]\n",
      " [2622 5226 1170 3079 4274    1 3830]\n",
      " [2946 3396 3792  226 4639    0 1362]\n",
      " [3050    2 1538    1 4313    0 4939]\n",
      " [ 143 4313 2410    0 3674    0 3146]\n",
      " [1928 4276 1091    0 3487    0 2009]\n",
      " [ 226 1503  138    0  712    0 3396]\n",
      " [   1 3851 1654    0 2602    0 2644]\n",
      " [   0 5045 2982    0 3326    0 3938]\n",
      " [   0 2355 4082    0 4082    0 4274]\n",
      " [   0  610 2003    0 2355    0 5213]\n",
      " [   0 4865 4989    0 1091    0 2009]\n",
      " [   0 5114 4313    0 4075    0 1187]\n",
      " [   0 1201 3487    0  116    0    2]\n",
      " [   0 1170    2    0  610    0 1703]\n",
      " [   0 5481  753    0 5432    0 3663]\n",
      " [   0    2    2    0  226    0  226]\n",
      " [   0 2848 2009    0    1    0    1]\n",
      " [   0 1098 3783    0    0    0    0]\n",
      " [   0 1538 2982    0    0    0    0]\n",
      " [   0  509 4992    0    0    0    0]\n",
      " [   0  189  226    0    0    0    0]\n",
      " [   0 2935    1    0    0    0    0]\n",
      " [   0 4276    0    0    0    0    0]\n",
      " [   0  611    0    0    0    0    0]\n",
      " [   0 1098    0    0    0    0    0]\n",
      " [   0 4439    0    0    0    0    0]\n",
      " [   0 3146    0    0    0    0    0]\n",
      " [   0 3056    0    0    0    0    0]\n",
      " [   0 4313    0    0    0    0    0]\n",
      " [   0  320    0    0    0    0    0]\n",
      " [   0 2581    0    0    0    0    0]\n",
      " [   0 2178    0    0    0    0    0]\n",
      " [   0 3396    0    0    0    0    0]\n",
      " [   0 2003    0    0    0    0    0]\n",
      " [   0 5292    0    0    0    0    0]\n",
      " [   0 5529    0    0    0    0    0]\n",
      " [   0 1720    0    0    0    0    0]\n",
      " [   0    2    0    0    0    0    0]\n",
      " [   0  226    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "#from utils import format_batch\n",
    "test_x = [[5,2,3],[2], [4,2], [1,2]]\n",
    "# it's going to go from the number of cols being the sequence length/ num of rows being batch size\n",
    "# to the number of rows being the max sequence length/ num cols being batch size\n",
    "# Essentially like a padding and then transpose\n",
    "def format_batch(x):\n",
    "    seq_lengths = [len(row) for row in x]\n",
    "    n_batches = len(x)\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    outputs = np.zeros(shape=(max_seq_length, n_batches),dtype=np.int32)\n",
    "    for i in range(len(seq_lengths)):\n",
    "        for j in range(seq_lengths[i]):\n",
    "            outputs[j][i] = x[i][j]\n",
    "    return outputs\n",
    "\n",
    "print format_batch(test_x)\n",
    "print np.array(format_batch(source_train[0:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Very pleasing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "input_embedding_size = 64 # Fixed due to pretrained embedding files\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = encoder_hidden_units # Must be the same at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "print encoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_s, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_t, decoder_inputs)\n",
    "print encoder_inputs_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded,\n",
    "                                                         dtype=tf.float32, time_major=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"plain_decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "                                decoder_cell, decoder_inputs_embedded,\n",
    "                                initial_state=encoder_final_state,\n",
    "                                dtype=tf.float32, time_major=True, \n",
    "                                scope=\"plain_decoder\")\n",
    "print decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(?, ?, 6003), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size_t], -0.5, 0.5), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size_t]), dtype=tf.float32)\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "# why do we only flatten the tensor so it's rank 2?\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#feed flattened tensor through projection\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "# make the logits the shape of the \n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size_t))\n",
    "\n",
    "print decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?, ?), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size_t)\n",
    "#print decoder_logits_2\n",
    "decoder_prediction = tf.argmax(decoder_logits, axis=2)\n",
    "print decoder_prediction\n",
    "#help(tf.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_4:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "timestep_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size_t, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "print timestep_cross_entropy\n",
    "# loss is the mean of the cross entropy\n",
    "loss = tf.reduce_mean(timestep_cross_entropy)\n",
    "print loss\n",
    "# We use AdaM which combines AdaGrad (parameters updated less often get updated more strongly)\n",
    "# and momentum (updates depend on the slope of previous updates - avoiding local minima)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[  2  24   9]\n",
      " [124 523  82]\n",
      " [243  23   0]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "decoder predictions:\n",
      "[[ 435 1621 3758]\n",
      " [3758  249 3758]\n",
      " [3758 3758 3758]\n",
      " [3758 3758 3758]]\n"
     ]
    }
   ],
   "source": [
    "# Test format_batch and make sure that the decoder\n",
    "# and encoder accepts inputs with a forward pass\n",
    "\n",
    "batch_ = [[2,124,243], [24,523,23], [9, 82]]\n",
    "\n",
    "batch_ = format_batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_ = format_batch(np.ones(shape=(3, 4), dtype=np.int32))\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_inputs:0\", shape=(?, ?), dtype=int32)\n",
      "par conséquent , le tribunal se <UNK> pour l&apos; application du droit étranger relatif aux droits voisins . <EOS> \n",
      "[3704  343 3146 4274 4979  450    2 1201 1187 2633 1707  381 2622 2946 3050\n",
      "  143 1928  226    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0] \n",
      "\n",
      "Reversed as in Sutskever et al. \n",
      "[ 226 1928  143 3050 2946 2622  381 1707 2633 1187 1201    2  450 4979 4274\n",
      " 3146  343 3704    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "Tensor(\"decoder_inputs:0\", shape=(?, ?), dtype=int32)\n",
      "<EOS> by contrast , german law does not impose <UNK> <UNK> , it looks to the ultimate result . \n",
      "[   1 1041  762 3089 2091 1585 1436 1990 5021    2    2 3089 3250 5745  736\n",
      " 1784 4612  213  243    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "Tensor(\"decoder_targets:0\", shape=(?, ?), dtype=int32)\n",
      "by contrast , german law does not impose <UNK> <UNK> , it looks to the ultimate result . <EOS> \n",
      "[1041  762 3089 2091 1585 1436 1990 5021    2    2 3089 3250 5745  736 1784\n",
      " 4612  213  243    1    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "Decoder inputs at test time\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "from utils import ids_to_phrases\n",
    "\n",
    "def batch_source_target(source, target, batch_size):\n",
    "    assert len(source) == len(target)\n",
    "    for start in range(0, len(source), batch_size):\n",
    "        end = min(start + batch_size, len(source))\n",
    "        #print type(source[start:end])\n",
    "        #print len(target[start:end])\n",
    "        yield source[start:end], target[start:end]     \n",
    "\n",
    "\n",
    "def make_feed_dict(fd_keys, s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_inputs_ = format_batch([[1]+sequence[0:-1] for sequence in t_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def make_test_feed_dict(fd_keys,s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    # At testing time, we can't supervise the decoder layer with\n",
    "    # the 'gold truth' example as input, so we instead feed in\n",
    "    # word generated at  previous timestep. This is (apparently)\n",
    "    # equivalent to feeding in zeros for the decoder inputs\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    decoder_inputs_ = format_batch([[0]*len(sequence) for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test everything is working okay\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for s_sample_batch, t_sample_batch in batch_source_target(source_train[0:2], target_train[0:2], batch_size):\n",
    "    fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "    fd = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch)\n",
    "    fd_r = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= True)\n",
    "    fd_t = make_test_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= False)\n",
    "    assert len(fd.values()[0].T[0]) == len(fd_r.values()[0]) # reversed list must be the same length as original\n",
    "    print fd.keys()[0]\n",
    "    print ids_to_phrases(np.array(fd.values()[0]).T[0], id2word_s)\n",
    "    print np.array(fd.values()[0]).T[0], \"\\n\"\n",
    "    print \"Reversed as in Sutskever et al. \"\n",
    "    print np.array(fd_r.values()[0]).T[0]\n",
    "    assert len(fd.values()[1].T[0]) == len(fd.values()[1].T[1]) # decoder inputs and targets must be the same\n",
    "    \n",
    "    for i in range(1, len(fd.keys())):\n",
    "        print fd.keys()[i]\n",
    "        ph = ids_to_phrases(np.array(fd.values()[i]).T[0], id2word_t)\n",
    "        print ph\n",
    "        print np.array(fd.values()[i]).T[0]\n",
    "    \n",
    "    print \"Decoder inputs at test time\"\n",
    "    print np.array(fd_t.values()[1]).T[0]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there will be 64 samples in the final batch\n",
      "training has begun...\n",
      "epoch 1\n",
      "batch 0\n",
      "loss: 2.13353276253\n",
      "  sample 1:\n",
      "    input     > [1187, 1529, 1703, 4602, 2, 3396, 1078, 1703, 2, 718, 3556, 2644, 2433, 1187, 3503, 1538, 2, 1170, 4970, 226, 1] \n",
      " l&apos; associé de m. <UNK> s&apos; occupe de <UNK> au canada est aussi l&apos; agent qui <UNK> les fournisseurs . \n",
      "    actual     > [2077, 2, 286, 2344, 4404, 3255, 3499, 707, 5482, 3443, 2, 1022, 243, 1] \n",
      " mr. <UNK> has a partner in canada as his agent <UNK> suppliers . \n",
      "    predicted     > [2, 2, 2, 2, 2, 2, 2] \n",
      " <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "[2 2 2 2 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2355, 941, 2118, 3146, 4276, 4902, 1703, 1362, 1098, 2, 1410, 1098, 3108, 2, 226, 1] \n",
      " a première vue , la profondeur de ce &quot; <UNK> politique &quot; semble <UNK> . \n",
      "    actual     > [708, 3516, 2, 3089, 1784, 2613, 5233, 1191, 1084, 3483, 4038, 1084, 5198, 3575, 243, 1] \n",
      " at first <UNK> , the depth of this &quot; policy gap &quot; seems surprising . \n",
      "    predicted     > [2, 2, 2, 2, 2, 2, 2, 2, 2] \n",
      " <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> \n",
      "[2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "training interrupted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def format_idx(idx):\n",
    "    # Just cuts out the padding of word index lists\n",
    "    li = []\n",
    "    for i in idx:\n",
    "        if i ==0:\n",
    "            break\n",
    "        else:\n",
    "            li.append(i)\n",
    "    return li\n",
    "\n",
    "BLEU = []\n",
    "epochs = 30 # How many times we loop over the whole training data\n",
    "batch_size = 92 # After how many sequences do we update the weights?\n",
    "print \"there will be {} samples in the final batch\".format(len(source_train)%batch_size)\n",
    "fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "try:\n",
    "    batch_n = 0\n",
    "    ti = time.time()\n",
    "    print \"training has begun...\"\n",
    "    for epoch in range(epochs):    \n",
    "        for s_batch, t_batch in batch_source_target(source_train, target_train, batch_size):\n",
    "            feed_dict = make_feed_dict(fd_keys, s_batch, t_batch)\n",
    "            _, l = sess.run([train_op, loss], feed_dict)\n",
    "            \n",
    "            #if batch_n == 0 or batch_n == 60:\n",
    "            #    \n",
    "            if (batch_n==0) or (batch_n%100) == 0:\n",
    "                loss_track.append(l)\n",
    "                print \"epoch {}\".format(epoch+1)\n",
    "                print 'batch {}'.format(batch_n)\n",
    "                print 'loss: {}'.format(sess.run(loss, feed_dict))\n",
    "                predict_ = sess.run(decoder_prediction, feed_dict)\n",
    "                #predictions = [remove_EOS_PAD(pred) for pred in predict_.T]\n",
    "                #actuals = [[remove_EOS_PAD(act)] for act in fd[decoder_targets].T]\n",
    "                #BLEU2 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.5,0.5))\n",
    "                #BLEU.append(BLEU2)\n",
    "                i =1\n",
    "                for (inp, act, pred) in zip(feed_dict[encoder_inputs].T,\n",
    "                                                         feed_dict[decoder_targets].T,\n",
    "                                                         predict_.T)[10:12]:\n",
    "                    print '  sample {}:'.format(i)\n",
    "                    print '    input     > {} \\n {}'.format(\n",
    "                        format_idx(inp), ids_to_phrases(inp, id2word_s))\n",
    "                    print '    actual     > {} \\n {}'.format(\n",
    "                        format_idx(act), ids_to_phrases(act, id2word_t).encode(\"utf-8\"))\n",
    "                    print '    predicted     > {} \\n {}'.format(\n",
    "                        format_idx(pred), ids_to_phrases(pred, id2word_t).encode(\"utf-8\"))\n",
    "                    i+=1\n",
    "            batch_n += 1\n",
    "            \n",
    "    print 'Training is complete'\n",
    "except KeyboardInterrupt:\n",
    "    print 'training interrupted'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(loss_track)), loss_track)\n",
    "#l = [s for i,s in sorted(zip([len(row) for row in l], l))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
