{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Text preprocessing and exploratory data analysis\n",
    "\n",
    "In this notebook I will perform some analysis on the part of the WMT'14 dataset that I am going to use for my translation system in order to gain some intuition about how to preprocess it. I will then write the code to preprocess the data and save them to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The dataset\n",
    "\n",
    "Here I will introduce the dataset, experiment with it, tokenize it, preprocess it and save it to a new set of files. I will also write a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRENCH: \n",
      ", son épouse enseignante de jardin d&apos; enfants , Roseanna , et Geneviève se préparaient à partir en voyage en famille .\n",
      "Ce changement d&apos; époque est en même temps le problème de toute la démocratie chrétienne , et \n",
      "***************\n",
      "ENGLISH: \n",
      "and Genevieve were getting ready to go on a family trip .\n",
      "The beginning of a new era means a problem for all Christian democracy , including Angela Merkel as well .\n",
      "Obama ? The first anti-American Pre\n",
      "\n",
      "Character length of both corpora: 155125826, 128673246.0. French is 20.56% 'bulkier' than English.\n",
      "Phrase length of both corpora: 1013625, 1013625\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "lang_t = 'en'\n",
    "lang_s = 'fr'\n",
    "prep_path = \"/DATA/\"\n",
    "\n",
    "f1_fr = codecs.open(\"DATA/en-fr_paropt/dev.tok.fr\", encoding='utf-8')\n",
    "f2_fr = codecs.open(\"DATA/en-fr_paropt/train.fr\", encoding='utf-8')\n",
    "#test_fr = codecs.open(\"DATA/en-fr_paropt/test.fr\", encoding='utf-8')\n",
    "source = eval(\"f1_\"+lang_s).read()+\"\\n\"+eval(\"f2_\"+lang_s).read()\n",
    "\n",
    "\n",
    "f1_en = codecs.open(\"DATA/en-fr_paropt/dev.tok.en\", encoding='utf-8')\n",
    "f2_en = codecs.open(\"DATA/en-fr_paropt/train.en\", encoding='utf-8')\n",
    "#test_en = codecs.open(\"DATA/en-fr_paropt/test.en\", encoding='utf-8')\n",
    "target = eval(\"f1_\"+lang_t).read()+\"\\n\"+eval(\"f2_\"+lang_t).read()\n",
    "\n",
    "\n",
    "#i = source.index(u'pr\\xe9sident m. rohan')\n",
    "#print i, source[i:i+60]\n",
    "char_length_s = len(source)\n",
    "char_length_t = float(len(target))\n",
    "s_length = len(source.split('\\n'))\n",
    "t_length = len(target.split('\\n'))\n",
    "print \"FRENCH: \\n\", source[99:320], \"\\n***************\\nENGLISH: \\n\", target[100:300]\n",
    "print \"\\nCharacter length of both corpora: {}, {}. French is {:0.2f}% 'bulkier' than English.\".format(\n",
    "                                char_length_s, char_length_t, 100*((char_length_s/char_length_t)-1))\n",
    "print \"Phrase length of both corpora: {}, {}\".format(s_length, t_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A few observations so far: \n",
    "- French takes 20% more characters to convey similar information. We haven't looked at tokens/words yet but I predict the French corpus will have more words than the English, rather than having a higher average number of characters per word.\n",
    "- The data is in unicode `utf-8` format, so we need to be careful with string manipulations and encoders/decoders.\n",
    "- There are plenty of commas, full stops (periods) and other bits of punctuation, which may affect the understanding of a sentence to a native speaker, as made famous in [Eats, Shoots & Leaves](https://en.wikipedia.org/wiki/Eats,_Shoots_%26_Leaves), however for our intents and purposes all punctuation apart from apostrophes will be removed from the texts. This is because these tokens will occur very often in the text and so the machine learning algorithms will weight their importance far higher than it actually is in conveying meaning. Some exceptions will be full stops indicating abbreviations, for example in F.B.I.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is now 3.99968474624 less bulky than before stripping punctuation.\n",
      "Target text is now 3.78273662265 less bulky than before stripping punctuation..\n"
     ]
    }
   ],
   "source": [
    "def remove_punc(text):\n",
    "    # Needs to keep stops like L.A.P.D.\n",
    "    # almost all other punctuation needs to be carefully removed\n",
    "    text = unicode.strip(text)\n",
    "    text = unicode.replace(text, \" .\", u\"\")\n",
    "    text = unicode.replace(text,\",\", u\"\")\n",
    "    text = unicode.replace(text,u\" .\", u\"\")\n",
    "    text = unicode.replace(text,u\" ,\", u\"\")\n",
    "    text = unicode.strip(text, \",:)(][}{!\")\n",
    "    text = unicode.strip(text, u\",:)(][}{;!\")\n",
    "    text = unicode.replace(text,u':', u'')\n",
    "    text = text.replace(u';', u'')\n",
    "    text = text.replace(')', u'')\n",
    "    text = text.replace(\"(\", u'')\n",
    "    text = text.replace(\"!\", u':')\n",
    "    #text = text.replace(u\"     \", \" \")\n",
    "    text = text.replace(u\"    \", \" \")\n",
    "    text = text.replace(u\"   \", \" \")\n",
    "    text = text.replace(u\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "source = remove_punc(source)\n",
    "target = remove_punc(target)\n",
    "print \"Source text is now {} less bulky than before stripping punctuation.\".format(\n",
    "                                        100*((float(char_length_s)-len(source))/float(char_length_s)))\n",
    "print \"Target text is now {} less bulky than before stripping punctuation..\".format(\n",
    "                                        100*((float(char_length_t)-len(target))/float(char_length_t)))\n",
    "source = source.split(\"\\n\") # split text up by lines\n",
    "target = target.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Stripping punctuation decreases the corpora length by around 4%, this is certainly non-negligible and will help a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenising the data\n",
    "\n",
    "Thankfully, the dataset I have used is already split up by whitespace into appropriate tokens. For example in French \"j'\" and \"ai\" are treated as separate tokens which make up \"j'ai\" to mean \"I have\" in English. Similarly, \"didn\" and \"'t\" are separated by whitespace. This makes tokenising the data as trivial as calling the `split()` method on each sequence string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpora still same length: True\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def tokenize_sentences(text_source, text_target):\n",
    "    new_text_s = []\n",
    "    new_text_t = []\n",
    "    for sent_s, sent_t in zip(text_source, text_target):\n",
    "        # This splits up tokens within a sentence and removes whitespace from the ends\n",
    "        tok_s = (unicode.strip(sent_s.lower())).split(' ')\n",
    "        tok_t = (unicode.strip(sent_t.lower())).split(' ')\n",
    "        \n",
    "        \n",
    "        # I'm keeping the final punctuation and appending the\n",
    "        # <EOS> tag after it\n",
    "        new_text_s.append(tok_s)\n",
    "        new_text_t.append(tok_t)\n",
    "    #print \"There are now {} sequence pairs remaining.\".format(len(new_text_t))\n",
    "    print \"Corpora still same length: {}\".format(len(new_text_t)==len(new_text_s))\n",
    "    return new_text_s, new_text_t\n",
    "\n",
    "source_long, target_long = tokenize_sentences(source, target)\n",
    "del source, target\n",
    "print source_long[10], \"\\n\", target_long[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sequence length\n",
    "\n",
    "Now I will look at a few statistics in order to find a reasonable sequence length limit and a justification for doing so. I found that reducing the range of sequence lengths makes the algorithm orders of magnitude faster to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "Min: 1  Max: 124  Mean: 24.00   Median: 21.0   Mode: 14   1st Quart.: 13.0   IQR: 18.0   3rd Quart.: 31.0\n",
      "en\n",
      "Min: 1  Max: 125  Mean: 20.17   Median: 18.0   Mode: 12   1st Quart.: 11.0   IQR: 15.0   3rd Quart.: 26.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def sequence_stats(text):\n",
    "    lengths = sorted([len(seq) for seq in text])\n",
    "    l_min = np.min(lengths)\n",
    "    l_max = np.max(lengths)\n",
    "    l_mean = np.mean(lengths)\n",
    "    l_median = np.median(lengths)\n",
    "    l_mode = mode(lengths)[0][0]\n",
    "    percentile_25 = np.percentile(lengths, 25)\n",
    "    percentile_75 = np.percentile(lengths, 75)\n",
    "    print \"Min: {}  Max: {}  Mean: {:0.2f}   Median: {}   Mode: {}   1st Quart.: {}   IQR: {}   3rd Quart.: {}\".format(\n",
    "                                            l_min,l_max,l_mean, l_median, l_mode, percentile_25, percentile_75-percentile_25,percentile_75)\n",
    "    return\n",
    "print \"Language:\", lang_s\n",
    "sequence_stats(source_long)\n",
    "print \"Language:\", lang_t\n",
    "sequence_stats(target_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.0\n",
      "48.5\n"
     ]
    }
   ],
   "source": [
    "print 31+1.5*18\n",
    "print 26+1.5*15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Although the maximum sequence lengths are very high in both the French and English corpora (124 and 125 words respectively), the mean and median are far lower, with a mean larger than median inidicating that the sequence length distribution is skewed. In fact, 25% of the Fr and En data is made up of sequences smaller than 13 and 11 tokens long, respectively and 50% by the medians 21 and 18. Due to the skewedness of the data, the interquartile range overlaps with zero in each case so there is no obvious lower bound to define outliers by using $Median-1.5IQR$. An upper bound of $Median+1.5IQR$ would mean that any sequences longer than 58 and 49 could be deemed outliers, using the standard approach. However, these sequences are still very long, considering that most of the data is a third of the length of this upper bound. Processing these sequences will be very challenging as their presence will cause batches to be padded with a large number of `<PAD>` tokens. The single layered LSTM encoder/decoder will also find it very difficult to compress/extract all of the information in the very long sequences.\n",
    "\n",
    "In order to make the model easier to train I will only consider sequences up to the median length. This means that French (English) sequences longer than 21 (18) tokens long are discarded while maintaining half of the overall dataset. Although this is a harsh approximation, I think it is justified since the aim of this experiment is to learn how to implement sequence-to-sequence modelling and not to create a cutting edge solution, so the decrease in rigour is worth the increase in trainability. Going from around 1,000,000 training examples to 500,000 should still be sufficient to train a toy model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Removing long seqences\n",
    "\n",
    "Here I implement the discussion above and generate some word frequency statistics along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 469624 sequence pairs remaining.\n",
      "Language: fr\n",
      "Min: 1  Max: 21  Mean: 12.82   Median: 13.0   Mode: 14   1st Quart.: 9.0   IQR: 8.0   3rd Quart.: 17.0\n",
      "Language: en\n",
      "Min: 1  Max: 19  Mean: 10.97   Median: 11.0   Mode: 12   1st Quart.: 8.0   IQR: 6.0   3rd Quart.: 14.0\n"
     ]
    }
   ],
   "source": [
    "def shorten_data(source_, target_):\n",
    "    flat_s = []\n",
    "    flat_t = []\n",
    "    source = []\n",
    "    target = []\n",
    "    for seq_s, seq_t in zip(source_, target_):\n",
    "        bool_short_s = (len(seq_s)<=21)\n",
    "        bool_short_t = (len(seq_t)<=19)\n",
    "        bool_asymm = (abs(len(seq_t)-len(seq_s))<10)\n",
    "        if bool_short_s and bool_short_t:\n",
    "            flat_s += seq_s\n",
    "            flat_t += seq_t\n",
    "            source.append(seq_s)\n",
    "            target.append(seq_t)\n",
    "    print \"There are now {} sequence pairs remaining.\".format(len(source))\n",
    "    return source, nltk.FreqDist(flat_s), target, nltk.FreqDist(flat_t)\n",
    "\n",
    "\n",
    "source, freq_s, target, freq_t = shorten_data(source_long, target_long)\n",
    "print \"Language:\", lang_s\n",
    "sequence_stats(source)\n",
    "print \"Language:\", lang_t\n",
    "sequence_stats(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Limiting the vocabulary\n",
    "\n",
    "Here I'll check that the word statistics of the dataset are what I expect and then use some of the insights to find an appropriate upper bound on the number of vocabulary words I use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 269274), (u'and', 132831), (u'of', 131523), (u'to', 111982)]\n",
      "[(u'de', 288132), (u'la', 153223), (u'les', 133904), (u'et', 124541)]\n",
      "English Vocabulary size: 140873\n",
      "Total proportion are hapaxes: 54.1310258176\n",
      "French Vocabulary size: 184012\n",
      "Total proportion are hapaxes: 52.6259157011\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAEBCAYAAADB4ikBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCLE/wPmOZzPRtJb/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120db76d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print freq_t.most_common(4)\n",
    "print freq_s.most_common(4)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "def plot_pareto(freq, ax, l):\n",
    "    counts_list = sorted(set(freq.values()))\n",
    "    total_counts = float(sum(freq.values()))\n",
    "    vocab_total= len(freq.keys())\n",
    "    print \"{} Vocabulary size: {}\".format(l, vocab_total)\n",
    "    print \"Total proportion are hapaxes: {}\".format((len(freq.hapaxes())/float(vocab_total))*100)\n",
    "    d = dict()\n",
    "    for word, count in freq.items():\n",
    "        if word not in string.punctuation.encode('utf-8'):\n",
    "            if count in d.keys():\n",
    "                pass\n",
    "            else:\n",
    "                d[count] = word\n",
    "    y, words = tuple(zip(*d.items()))\n",
    "    x, y = zip(*[(n, count/total_counts) for n,count in enumerate(sorted(y,reverse=True),1)])\n",
    "    ax.loglog(x,y,label=l)\n",
    "    ax.set_ylabel(\"Normalised count (log)\")\n",
    "    ax.set_xlabel(\"Word Rank (log)\")\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "plot_pareto(freq_t, ax1, 'English')\n",
    "#ax1.set_title(\"English\")\n",
    "#plt.legend()\n",
    "#plt.savefig(\"English-pareto.pdf\")\n",
    "#fig = plt.figure()\n",
    "\n",
    "#ax2 = fig.add_subplot(221)\n",
    "plot_pareto(freq_s, ax1, 'French')\n",
    "#ax2.set_title(\"French\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"French-pareto.pdf\")\n",
    "plt.show()\n",
    "#freqs = freq_t_d.values()\n",
    "#h, bins_edges = np.histogram(freqs, bins=vocab_size)\n",
    "#plt.hist(h, bins=bins_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the above plot, we see normalised word frequency for words of each rank, that is the score that would be assigned to a word given the number of times it occurs in the text. For example, the word \"the\" occurs the most number of times in the English dataset and so is given rank of 1, etc. We see the famous Pareto distribution of words, the straight line indicating that there is a particular power law scaling. In this case the power law states that the rank 1 word is twice as common as the rank 2 word, the rank 2 word is three times as common as the rank 3 word and so on, which is shown nicely in the printed statistics above. The trailing off for uncommon words (large word rank) is due to the finite size of the dataset.\n",
    "\n",
    "We can also see that the total vocabulary used for English and French are 140873 and 184012, with over half of the vocabulary words only occurring once in the entire corpus (known hapax legomenon). These are fairly useless, as there are not enough different contexts of a word for the algorithm to learn it's meaning, particularly as they are likely to be obscure words/jargon anyway. This means that straight away, 75-95,000 words can be removed, which is a good start, but probably not enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr vocab size of 25000 restricts to 13.59 percent of total vocab.\n",
      "\tPercentage of whole fr corpus covered by vocab: 93.99\n",
      "fr vocab size of 12500 restricts to 6.79 percent of total vocab.\n",
      "\tPercentage of whole fr corpus covered by vocab: 89.30\n",
      "fr vocab size of 207 restricts to 0.11 percent of total vocab.\n",
      "\tPercentage of whole fr corpus covered by vocab: 50.01\n",
      "\n",
      "en vocab size of 350 restricts to 0.25 percent of total vocab.\n",
      "\tPercentage of whole en corpus covered by vocab: 50.01\n",
      "en vocab size of 20000 restricts to 14.20 percent of total vocab.\n",
      "\tPercentage of whole en corpus covered by vocab: 94.92\n",
      "en vocab size of 10000 restricts to 7.10 percent of total vocab.\n",
      "\tPercentage of whole en corpus covered by vocab: 89.27\n"
     ]
    }
   ],
   "source": [
    "def covered_with_vocab(freq_dist, vocab_size, lang):\n",
    "    total_words = len(freq_dist.keys())\n",
    "    total_length = sum(freq_dist.values())\n",
    "    print \"{} vocab size of {} restricts to {:0.2f} percent of total vocab.\".format(lang, vocab_size, 100*(float(vocab_size)/total_words))\n",
    "    vocab = dict(freq_dist.most_common(vocab_size))\n",
    "    #print \"vocab size= \",len(vocab.keys())\n",
    "    covered_with_vocab = sum(vocab.values()) # Total of frequencies\n",
    "    print \"\\tPercentage of whole {} corpus covered by vocab: {:0.2f}\".format(lang,\n",
    "                                            100*(covered_with_vocab/float(total_length)))\n",
    "    return\n",
    "vs_s1 = 25000\n",
    "vs_s2 = 12500\n",
    "vs_t1 = 20000\n",
    "vs_t2 = 10000\n",
    "covered_with_vocab(freq_s, vs_s1, lang_s)\n",
    "covered_with_vocab(freq_s, vs_s2, lang_s)\n",
    "covered_with_vocab(freq_s, 207, lang_s)\n",
    "print\n",
    "covered_with_vocab(freq_t, 350, lang_t)\n",
    "covered_with_vocab(freq_t, vs_t1, lang_t)\n",
    "covered_with_vocab(freq_t, vs_t2, lang_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab difference = 30.6226175349\n",
      "core usage difference = 40.8571428571\n"
     ]
    }
   ],
   "source": [
    "print \"vocab difference =\", ((184012-140873.)/140873)*100\n",
    "print \"core usage difference =\",((350-207)/350.)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it takes only around 14% of the vocabulary to cover 94-95% of the entire corpora and 7% of the vocabulary to cover 89%. In fact, with just 207 and 350 words you can cover 50% of the French and English corpora, respectively - this is the remarkable scaling properties of word frequency in action. This also shows an interesting insight, although French has 31% more words in its vocabulary, far fewer words (41% fewer!) make up its _core_ than English.\n",
    "\n",
    "94-95% of the corpus being covered by the vocabulary means that around 5 or 6 words in 100 will be replaced by an `<UNK>` token, this is likely to be sufficient to maintain enough information in source and target phrases for the model to learn from, while reducing the number of parameters in the model substantially. Given this, the vocabulary sizes for French and English will be 25,000 and 20,000, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Replacing word tokens with IDs\n",
    "\n",
    "Now I will create dictionaries mapping from word tokens to their respective ID and vice versa. This will make it easy to define a sparse vector representation for each word as well as making it possible to index embedding arrays (dense representations) efficiently. We must remember to reserve IDs 0, 1 and 2 for `<PAD>`, `<EOS>` and `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3745 meteorological\n"
     ]
    }
   ],
   "source": [
    "vocab_size_s = 25000\n",
    "vocab_size_t = 20000\n",
    "def word_to_ids(freq_dist, vocab_size, lang):\n",
    "    vocab = dict(freq_dist.most_common(vocab_size))\n",
    "    # IDs begin at 3 because <EOS>=1 and <UNK>=2\n",
    "    word_to_ids = dict([(word, i+3) for i, word in enumerate(vocab.keys())])\n",
    "    word_to_ids[u'<UNK>'] = 2\n",
    "    word_to_ids[u'<EOS>'] = 1\n",
    "    word_to_ids[u'<PAD>'] = 0\n",
    "    id_to_words = dict((idx, word) for word, idx in word_to_ids.items())\n",
    "    return word_to_ids, id_to_words\n",
    "\n",
    "\n",
    "word2id_s, id2word_s = word_to_ids(freq_s, vocab_size_s, lang_s)\n",
    "word2id_t, id2word_t = word_to_ids(freq_t, vocab_size_t, lang_t)\n",
    "print word2id_t['hello'], id2word_t[2345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the `word2id` dictionaries to swap out the various words for their IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsource_ids = replace_with_word_id(source, word2id_s, lang_s)\\ntarget_ids = replace_with_word_id(target, word2id_t, lang_t)\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def replace_id_func(sequence, **kwargs):\n",
    "    # Parallelisable version of word to ID replacer\n",
    "    word2id=kwargs['wtid']\n",
    "    ids_sent = [] # sentence with words replaced by ids\n",
    "    sequence+=[u'<EOS>']\n",
    "    for token in sequence:\n",
    "        if token not in word2id.keys():\n",
    "            if token == u'<EOS>':\n",
    "                ids_sent.append(1)\n",
    "            else:\n",
    "                ids_sent.append(2)\n",
    "        else:\n",
    "            ids_sent.append(word2id[token])\n",
    "    return ids_sent\n",
    "\n",
    "def replace_with_word_id(text, wtid, lang):\n",
    "    '''\n",
    "    take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK> whilst giving each\n",
    "    token a numerical ID, return new list of lists'''\n",
    "    ti = time.time()\n",
    "    prep_text = []\n",
    "    pool = Pool(processes = 3)\n",
    "    kwargs = {'wtid': wtid}\n",
    "    # Parallelise process, set buffer size to restrict memory usage\n",
    "    id_text = [i for i in pool.imap(partial(replace_id_func, **kwargs), text, chunksize=20000)]\n",
    "    print \"Replacing {} words with IDs took {} seconds.\".format(lang, time.time()-ti)\n",
    "    return id_text\n",
    "\n",
    "source_ids = replace_with_word_id(source, word2id_s, lang_s)\n",
    "target_ids = replace_with_word_id(target, word2id_t, lang_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA saved\n"
     ]
    }
   ],
   "source": [
    "from utils import load_obj, save_obj\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\"\"\"\n",
    "source_train, source_test, target_train, target_test = train_test_split(\n",
    "                            source_ids.split('\\n'), target_ids.split('\\n'),\n",
    "                            test_size=test_fraction, random_state=42)\n",
    "\n",
    "data_dic = {\"s_train\": source_train, \"t_train\": target_train,\n",
    "                \"s_test\": source_test, \"t_test\": target_test,\n",
    "                \"word2id_s\": word2id_s, \"word2id_t\": word2id_t,\n",
    "                \"id2word_s\": id2word_s, \"id2word_t\": id2word_t}\n",
    "save_obj(data_dic, \"DATA/data_dic\")\n",
    "print \"DATA saved\"\n",
    "\"\"\"\n",
    "\n",
    "data_dic = load_obj(\"DATA/data_dic\")\n",
    "print \"DATA loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
