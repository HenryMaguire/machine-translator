{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the preprocessed datasets\n",
    "Need the preprocessed data, the id_to_word dictionaries and word_to_id dictionaries for both source and target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 6003.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_obj\n",
    "\n",
    "word2id_s = load_obj(\"DATA/preprocess/word2id_s\")\n",
    "word2id_t = load_obj(\"DATA/preprocess/word2id_t\")\n",
    "id2word_s = load_obj(\"DATA/preprocess/id2word_s\")\n",
    "id2word_t = load_obj(\"DATA/preprocess/id2word_t\")\n",
    "\n",
    "text_source = load_obj(\"DATA/preprocess/source_train\") # Preprocessed already\n",
    "text_target = load_obj(\"DATA/preprocess/target_train\") # Preprocessed already\n",
    "\n",
    "#raw_source_test = load_obj(\"DATA/preprocess/raw_source_test\")\n",
    "#raw_source_test = [seq.split(\" \") for seq in raw_source_test]\n",
    "\n",
    "#raw_target_test = load_obj(\"DATA/preprocess/raw_target_test\")\n",
    "#raw_target_test = [seq.split(\" \") for seq in raw_target_test]\n",
    "\"\"\"\n",
    "\n",
    "def format_text(text):\n",
    "    text = text.replace(\" &apos;\", \"'\")\n",
    "    return text\n",
    "\n",
    "#raw_source_test = eval(\"test_\"+lang_s).read()\n",
    "#raw_target_test = eval(\"test_\"+lang_t).read()\n",
    "print len(raw_source_test.split(\"\\n\")), len(raw_target_test.split(\"\\n\"))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "source_train, source_test = text_source[0:10000], text_source[10000:500]\n",
    "target_train, target_test = text_target[0:10000], text_target[10000:500]\n",
    "print \"Vocab size is {}.\".format(len(word2id_s.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6003\n",
      "[1191, 280, 736, 1035, 2, 2262, 3089, 3086, 1784, 2, 4379, 1830, 3089, 2344, 5835, 5304, 1835, 1918, 3152, 736, 3853, 1784, 3628, 5233, 681, 2105, 243, 1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5f946db14687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlang_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mids_to_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mids_to_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/henrymaguire/Work/machine-learning/projects/machine-translator/utils.py\u001b[0m in \u001b[0;36mids_to_phrases\u001b[0;34m(idx_list, id_to_word)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0midx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mphrase\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mu' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def ids_to_phrases(idx_list, id_to_word):\n",
    "    # Takes list of word ids and returns a string of words\n",
    "    # Mainly for use in analysis\n",
    "    phrase = u''\n",
    "    id_dict = id_to_word\n",
    "    i=0\n",
    "    if len(idx_list)>0:\n",
    "        while idx_list[i] not in (1,0):\n",
    "            print idx_list[i]\n",
    "            phrase+= id_dict[idx_list[i]]+u' '\n",
    "            i+=1\n",
    "    return phrase.encode(\"utf-8\")\n",
    "\n",
    "import traceback\n",
    "from utils import ids_to_phrases\n",
    "print len(word2id_s.keys())\n",
    "vocab_size_t = len(word2id_t.keys())\n",
    "vocab_size_s = len(word2id_s.keys())\n",
    "lang_s = 'fr'\n",
    "lang_t = 'en'\n",
    "print ids_to_phrases(target_train[60], id2word_t)\n",
    "\n",
    "print ids_to_phrases(source_train[60], id2word_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation metric:     BLEU Score\n",
    "\n",
    "Here I use the NLTK implementation of BLEU score to measure how successful the machine translation was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "from utils import remove_EOS_PAD\n",
    "\n",
    "p_phrase1 = [4,5,4,5,4,5, 1, 0]\n",
    "t_phrase = [4,5,6,34,8,76, 87, 1]\n",
    "# Truncate sequences at [1]\n",
    "t_phrase = remove_EOS_PAD(t_phrase)\n",
    "p_phrase1 = remove_EOS_PAD(p_phrase1)\n",
    "\n",
    "print \"BLEU1 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [p_phrase1], weights=([1])))\n",
    "print \"BLEU2 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [p_phrase1], weights=([0.5,0.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Benchmark model\n",
    "\n",
    "Here I use the [GoogleTrans]() python package to translate the corpus by translating each individual word in the text. There seems to be an issue with the number of JSON requests the model makes to the Google Translate service, so it needs to work in passes. If a phrase is skipped due to a JSON error, keep it stored and try again later (with a new Translator instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON object could be decoded for phrase 46\n",
      "No JSON object could be decoded for phrase 87\n",
      "No JSON object could be decoded for phrase 132\n",
      "No JSON object could be decoded for phrase 166\n",
      "No JSON object could be decoded for phrase 217\n",
      "No JSON object could be decoded for phrase 263\n",
      "No JSON object could be decoded for phrase 316\n",
      "No JSON object could be decoded for phrase 373\n",
      "No JSON object could be decoded for phrase 430\n",
      "No JSON object could be decoded for phrase 479\n",
      "No JSON object could be decoded for phrase 526\n",
      "No JSON object could be decoded for phrase 581\n",
      "No JSON object could be decoded for phrase 630\n",
      "No JSON object could be decoded for phrase 683\n",
      "No JSON object could be decoded for phrase 742\n",
      "No JSON object could be decoded for phrase 798\n",
      "No JSON object could be decoded for phrase 1025\n",
      "No JSON object could be decoded for phrase 1074\n",
      "No JSON object could be decoded for phrase 1117\n",
      "No JSON object could be decoded for phrase 1329\n",
      "No JSON object could be decoded for phrase 1519\n",
      "No JSON object could be decoded for phrase 1729\n",
      "No JSON object could be decoded for phrase 1933\n",
      "No JSON object could be decoded for phrase 2115\n",
      "No JSON object could be decoded for phrase 2159\n",
      "No JSON object could be decoded for phrase 2221\n",
      "No JSON object could be decoded for phrase 2303\n",
      "No JSON object could be decoded for phrase 2519\n",
      "No JSON object could be decoded for phrase 2572\n",
      "No JSON object could be decoded for phrase 2763\n",
      "No JSON object could be decoded for phrase 3025\n",
      "No JSON object could be decoded for phrase 3296\n",
      "No JSON object could be decoded for phrase 3428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-5897d8416c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrans_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mBM_translated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_word_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_source_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Corpus translated from {} to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-5897d8416c9f>\u001b[0m in \u001b[0;36mtranslate_word_by_word\u001b[0;34m(source_text)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Need to keep track of phrase ordering by labelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msource_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrans_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipped_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogletrans_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     print \"There are {} phrases which could not be translated first time around.\".format(\n\u001b[1;32m     24\u001b[0m                                                                     len(skipped_phrases))\n",
      "\u001b[0;32m<ipython-input-75-5897d8416c9f>\u001b[0m in \u001b[0;36mgoogletrans_pass\u001b[0;34m(s_text, target_lang)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             trans_corpus.append((i, [trans.text for \n\u001b[0;32m---> 11\u001b[0;31m                             trans in translator.translate(phrase, dest=target_lang)]))\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Making a new Translator instance seems to help JSON errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/googletrans/client.pyc\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/googletrans/client.pyc\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/googletrans/client.pyc\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                     token=token)\n\u001b[1;32m     60\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    516\u001b[0m         }\n\u001b[1;32m    517\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                 )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     self.__class__)\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "def googletrans_pass(s_text, target_lang='fr'):\n",
    "    translator = Translator()\n",
    "    trans_corpus = []\n",
    "    skipped_phrases = []\n",
    "    for i, phrase in s_text:\n",
    "        try:\n",
    "            trans_corpus.append((i, [trans.text for \n",
    "                            trans in translator.translate(phrase, dest=target_lang)]))\n",
    "        except ValueError as err:\n",
    "            # Making a new Translator instance seems to help JSON errors\n",
    "            translator = Translator()\n",
    "            skipped_phrases.append((i, phrase))\n",
    "            print \"{} for phrase {}\".format(err, i)\n",
    "    return trans_corpus, skipped_phrases\n",
    "\n",
    "def translate_word_by_word(source_text):\n",
    "    # Need to keep track of phrase ordering by labelling\n",
    "    source_text = [(i, phr) for i, phr in enumerate(source_text)]\n",
    "    trans_text, skipped_phrases = googletrans_pass(source_text, target_lang=lang_t)\n",
    "    print \"There are {} phrases which could not be translated first time around.\".format(\n",
    "                                                                    len(skipped_phrases))\n",
    "    # Keep making passes until there are no more untranslated phrases left\n",
    "    j = 2\n",
    "    while len(skipped_phrases)>0:\n",
    "        translated_corpus = []\n",
    "        tc_s, skipped_phrases = googletrans_pass(skipped_phrases, target_lang=lang_t)\n",
    "        trans_text += tc_s\n",
    "        print \"There are {} phrases which could not be translated in pass {}.\".format(\n",
    "                                                            len(skipped_phrases), j)\n",
    "        j+=1\n",
    "    # Sort the phrases via their labels.\n",
    "    # Sorted function gives ([indices], [phrases]) so just need 2nd element\n",
    "    trans_text = zip(*sorted(zip(*BM_translated)))[1]\n",
    "    return trans_text\n",
    "\n",
    "BM_translated = translate_word_by_word(raw_source_test)\n",
    "print \"Corpus translated from {} to {}\".format(lang_s, lang_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une des raisons de l&apos; échec de &quot; The Hunting Party &quot; est que l&apos; on ne laisse simplement pas Simon Hunt être le cynique déserté doté d&apos; un cœur , comme il nous est présenté au début .\n",
      "A of the reasons from l&apos; failure from &quot; The Hunting Party &quot; East what l&apos; on born leash simply not Simon Hunt be the cynical deserted with d&apos; a heart , as the we East present the beginning .\n",
      "One of the reasons why &quot; The Hunting Party &quot; fails is that Simon Hunt isn &apos;t allowed to simply be the run-down cynic with a heart of gold we &apos;re introduced to at the beginning .\n"
     ]
    }
   ],
   "source": [
    "print \" \".join(raw_source_test[60])\n",
    "print \" \".join(BM_translated[60])\n",
    "print \" \".join(raw_target_test[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_targets = [[ids_to_phrases(phrase,id_to_word_t)]for phrase in test_t]\n",
    "\n",
    "BM_BLEU4 = bleu_score.corpus_bleu(test_targets, BM_translated, weights=(0.25,0.25, 0.25,0.25))\n",
    "BM_BLEU2 = bleu_score.corpus_bleu(test_targets, BM_translated, weights=(0.5,0.5))\n",
    "BM_BLEU1 = bleu_score.corpus_bleu(test_targets, BM_translated, weights=([1]))\n",
    "\n",
    "print \"Actual: \\n\", (\" \".join(test_targets[10])).encode('utf-8')\n",
    "print \"Prediction: \\n\", (\" \".join(BM_translated_corp[10])).encode('utf-8')\n",
    "print \"Unigram BLEU score is {}.\".format(BM_BLEU1)\n",
    "print \"Bigram BLEU score is {}.\".format(BM_BLEU2)\n",
    "print \"4-Gram BLEU: {}\".format(BM_BLEU4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word IDs and word-to-vec vectors\n",
    "\n",
    "Since we are interested in the process of learning weights within the RNNs to predict seq2seq mappings rather than embeddings I have chosen to use pretrained word embeddings ino order to cut down training time. [THis is](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Download-the-Embeddings) where the embeddings are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "def get_embeddings(id_to_word, lang):\n",
    "    # We load pretrained word2vec embeddings from polyglot to save on training time\n",
    "    filename ='DATA/polyglot-'+lang+'.pkl'\n",
    "    pretrain_vocab, pretrain_embed = pickle.load(open(filename, 'rb'))\n",
    "    embed_vocab = [pretrain_embed[pretrain_vocab.index('<PAD>')], pretrain_embed[pretrain_vocab.index('</S>')]]\n",
    "    skip_count = 0\n",
    "    skipped_words = []\n",
    "    for idx, word in sorted(id_to_word.items()[2::]):\n",
    "        try:\n",
    "            pretrain_idx = pretrain_vocab.index(word)\n",
    "            embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # it could be that the word is a name which needs to \n",
    "                # be capitalized. Try this...\n",
    "                pretrain_idx = pretrain_vocab.index(str(word.title()))\n",
    "                embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # it could be that the word is an achronym which needs to \n",
    "                    # be upper case. Try this...\n",
    "                    pretrain_idx = pretrain_vocab.index(word.upper())\n",
    "                    embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "                except ValueError:\n",
    "                    # Give up trying to find an embedding.\n",
    "                    # How many words are skipped? Which ones?\n",
    "                    skip_count +=1\n",
    "                    skipped_words.append(word)\n",
    "                    # Let's just initialise the embedding to a random normal distribution\n",
    "                    embed_vocab.append(np.random.normal(loc=0.0, scale=np.sqrt(2)/4, size=64))\n",
    "    embed_vocab = np.array(embed_vocab, dtype=np.float32)\n",
    "    print \"The embedding matrix for {} has {} columns and {} rows.\".format(lang, \n",
    "                                                embed_vocab.shape[0], embed_vocab.shape[1])\n",
    "    print \"{} vocab words were not in the {} embeddings file.\".format(skip_count, lang)\n",
    "    return embed_vocab, skipped_words\n",
    "# the ith word in words corresponds to the ith embedding \n",
    "\n",
    "embed_vocab_s, skipped_s = get_embeddings(id2word_s, lang=lang_s)\n",
    "embed_vocab_t, skipped_t = get_embeddings(id2word_t, lang=lang_t)\n",
    "print skipped_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** We can see above ** that the English words which were not in the embedding files are fairly specialist words or numerical values (which are the same in French) so hopefully they won't be too much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from utils import format_batch\n",
    "test_x = [[5,2,3],[2], [4,2], [1,2]]\n",
    "# it's going to go from the number of cols being the sequence length/ num of rows being batch size\n",
    "# to the number of rows being the max sequence length/ num cols being batch size\n",
    "# Essentially like a padding and then transpose\n",
    "def format_batch(x):\n",
    "    seq_lengths = [len(row) for row in x]\n",
    "    n_batches = len(x)\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    outputs = np.zeros(shape=(max_seq_length, n_batches),dtype=np.int32)\n",
    "    for i in range(len(seq_lengths)):\n",
    "        for j in range(seq_lengths[i]):\n",
    "            outputs[j][i] = x[i][j]\n",
    "    return outputs\n",
    "\n",
    "print format_batch(test_x)\n",
    "print np.array(format_batch(source_train[0:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Very pleasing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "input_embedding_size = 64 # Fixed due to pretrained embedding files\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = encoder_hidden_units # Must be the same at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "print encoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_s, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_t, decoder_inputs)\n",
    "print encoder_inputs_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded,\n",
    "                                                         dtype=tf.float32, time_major=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "                                decoder_cell, decoder_inputs_embedded,\n",
    "                                initial_state=encoder_final_state,\n",
    "                                dtype=tf.float32, time_major=True, \n",
    "                                scope=\"plain_decoder\")\n",
    "print decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size_t], -0.5, 0.5), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size_t]), dtype=tf.float32)\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "# why do we only flatten the tensor so it's rank 2?\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#feed flattened tensor through projection\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "# make the logits the shape of the \n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size_t))\n",
    "\n",
    "print decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size_t)\n",
    "#print decoder_logits_2\n",
    "decoder_prediction = tf.argmax(decoder_logits, axis=2)\n",
    "print decoder_prediction\n",
    "#help(tf.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestep_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size_t, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "print timestep_cross_entropy\n",
    "# loss is the mean of the cross entropy\n",
    "loss = tf.reduce_mean(timestep_cross_entropy)\n",
    "print loss\n",
    "# We use AdaM which combines AdaGrad (parameters updated less often get updated more strongly)\n",
    "# and momentum (updates depend on the slope of previous updates - avoiding local minima)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test format_batch and make sure that the decoder\n",
    "# and encoder accepts inputs with a forward pass\n",
    "\n",
    "batch_ = [[2,124,243], [24,523,23], [9, 82]]\n",
    "\n",
    "batch_ = format_batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_ = format_batch(np.ones(shape=(3, 4), dtype=np.int32))\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from utils import ids_to_phrases\n",
    "\n",
    "def batch_source_target(source, target, batch_size):\n",
    "    assert len(source) == len(target)\n",
    "    for start in range(0, len(source), batch_size):\n",
    "        end = min(start + batch_size, len(source))\n",
    "        #print type(source[start:end])\n",
    "        #print len(target[start:end])\n",
    "        yield source[start:end], target[start:end]     \n",
    "\n",
    "\n",
    "def make_feed_dict(fd_keys, s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_inputs_ = format_batch([[1]+sequence[0:-1] for sequence in t_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def make_test_feed_dict(fd_keys,s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    # At testing time, we can't supervise the decoder layer with\n",
    "    # the 'gold truth' example as input, so we instead feed in\n",
    "    # word generated at  previous timestep. This is (apparently)\n",
    "    # equivalent to feeding in zeros for the decoder inputs\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    decoder_inputs_ = format_batch([[0]*len(sequence) for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test everything is working okay\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for s_sample_batch, t_sample_batch in batch_source_target(source_train[0:2], target_train[0:2], batch_size):\n",
    "    fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "    fd = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch)\n",
    "    fd_r = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= True)\n",
    "    fd_t = make_test_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= False)\n",
    "    assert len(fd.values()[0].T[0]) == len(fd_r.values()[0]) # reversed list must be the same length as original\n",
    "    print fd.keys()[0]\n",
    "    print ids_to_phrases(np.array(fd.values()[0]).T[0], id2word_s)\n",
    "    print np.array(fd.values()[0]).T[0], \"\\n\"\n",
    "    print \"Reversed as in Sutskever et al. \"\n",
    "    print np.array(fd_r.values()[0]).T[0]\n",
    "    assert len(fd.values()[1].T[0]) == len(fd.values()[1].T[1]) # decoder inputs and targets must be the same\n",
    "    \n",
    "    for i in range(1, len(fd.keys())):\n",
    "        print fd.keys()[i]\n",
    "        ph = ids_to_phrases(np.array(fd.values()[i]).T[0], id2word_t)\n",
    "        print ph\n",
    "        print len(ph)\n",
    "        print np.array(fd.values()[i]).T[0]\n",
    "    \n",
    "    print \"Decoder inputs at test time\"\n",
    "    print np.array(fd_t.values()[1]).T[0]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there will be 64 samples in the final batch\n",
      "training has begun...\n",
      "epoch 1\n",
      "batch 0\n",
      "loss: 2.37081074715\n",
      "  sample 2:\n",
      "    input     > [4273, 2, 3144, 3285, 2, 2, 3144, 2642, 2, 1607, 3830, 1092, 1961, 5955, 4561, 3830, 1513, 3144, 5135, 10, 1707, 1692, 226, 1] \n",
      " le <UNK> , une <UNK> <UNK> , est <UNK> fixé à générations véhicule logistique lourd à roues , surtout lors du lancement . \n",
      "    actual     > [1190, 3249, 2, 1041, 1783, 5198, 5235, 2343, 3198, 4399, 310, 1783, 5944, 5235, 1783, 3696, 5918, 3255, 2343, 2920, 804, 3089, 5209, 642, 3152, 707, 5011, 1783, 213, 5235, 4673, 244, 3696, 2, 2343, 4520, 2303, 5235, 4115, 5064, 3977, 243, 1] \n",
      " athlete it <UNK> wildlife the rose discussed documentation testing promoted for the growth discussed the sights sky id documentation e-mail dress crew accepted rest seem at factors the result discussed tom much sights <UNK> documentation facilities amount discussed consumption rent extremely . \n",
      "    predicted     > [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] \n",
      " february <UNK> enables identify <UNK> <UNK> enables focused <UNK> them destination prosecution heroes rare voice destination announced enables period lord w. pursuing carbon \n",
      "  sample 2:\n",
      "    input     > [1171, 2700, 5293, 1124, 2, 2008, 2, 3144, 2008, 5227, 2, 3407, 1720, 2095, 2, 226, 1] \n",
      " placée gens sont discuter <UNK> et <UNK> , et on <UNK> très bien leur <UNK> . \n",
      "    actual     > [707, 706, 4090, 5235, 1783, 1587, 2, 2, 3089, 3250, 1326, 1035, 2, 2054, 1783, 1587, 2, 3101, 1989, 2097, 2, 1783, 5853, 5235, 1783, 2709, 4026, 3089, 4492, 3089, 5883, 1783, 1114, 2, 3101, 3089, 5808, 286, 460, 3324, 1783, 5865, 5232, 831, 3101, 5235, 1783, 2, 1878, 3089, 2, 736, 2, 1783, 5865, 5235, 2, 5232, 831, 3101, 5235, 1783, 4546, 3089, 2, 3089, 5734, 3089, 736, 1019, 2186, 3250, 707, 706, 2, 5235, 2, 310, 5077, 243, 1] \n",
      " at as google discussed the greek <UNK> <UNK> crew defeated jan bc <UNK> that the greek <UNK> bone not only <UNK> the joy discussed the nationals simple crew repeated crew choices the iii <UNK> bone crew = has unacceptable lies the environments ok roots bone discussed the <UNK> zone crew <UNK> tail <UNK> the environments discussed <UNK> ok roots bone discussed the estimated crew <UNK> crew panic crew tail yields using defeated at as <UNK> discussed <UNK> for cleared . \n",
      "    predicted     > [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] \n",
      " outcomes front settlement continued <UNK> et <UNK> enables et provides <UNK> french terms title <UNK> carbon \n",
      "epoch 1\n",
      "batch 100\n",
      "loss: 2.06408762932\n",
      "  sample 2:\n",
      "    input     > [3724, 823, 3144, 2, 3675, 2, 3144, 1187, 2, 2, 3285, 4646, 5753, 2, 5313, 1171, 5574, 2575, 3285, 1518, 2, 162, 2200, 5445, 3285, 3449, 2, 1707, 13, 2354, 189, 946, 1310, 5313, 4275, 2, 1197, 3851, 5050, 1703, 2, 226, 1] \n",
      " 4 oxygène , <UNK> arts <UNK> , l&apos; <UNK> <UNK> une nouvelle comédie <UNK> sur placée questions auxquelles une communauté <UNK> doit faire dans une ville <UNK> du saskatchewan a fait patrimoine manchester sur la <UNK> trouvent des millions de <UNK> . \n",
      "    actual     > [5789, 3244, 5818, 708, 86, 703, 2, 5235, 103, 5064, 3518, 5499, 3091, 2, 196, 4189, 243, 1] \n",
      " shell seven goals follow-up how am <UNK> discussed wine rent americans 2.2 differently <UNK> than extreme . \n",
      "    predicted     > [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1] \n",
      " expressing mexican enables <UNK> doctor <UNK> enables customer <UNK> <UNK> identify newly holiday <UNK> ticket outcomes dancing placing identify decisions <UNK> boston exposed aims identify pure <UNK> w. regional fined nominated long-term mechanical ticket l. <UNK> lock compare season covering <UNK> carbon \n",
      "  sample 2:\n",
      "    input     > [3830, 3628, 3144, 3851, 5400, 4719, 395, 3031, 719, 1574, 1703, 4869, 4719, 510, 611, 4182, 2399, 4273, 5913, 2, 1703, 100, 5426, 1465, 1736, 4627, 2, 3851, 3700, 2, 226, 1] \n",
      " à nouveau , des journalistes - queue fois at nombre de sept - onu moyenne arrêtés selon le motif <UNK> de fil blanc qu&apos; ils auraient <UNK> des écrits <UNK> . \n",
      "    actual     > [2, 2616, 2, 310, 2, 3910, 736, 2, 2844, 5004, 2, 2844, 1351, 5972, 1] \n",
      " <UNK> excluded <UNK> for <UNK> fish tail <UNK> budgetary opera <UNK> budgetary ends * \n",
      "    predicted     > [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] \n",
      " destination dates enables compare communities availability falling suitable gifts song covering fought availability spoken wants access identity february wealthy <UNK> covering democratic fees delivery iraq prevented <UNK> compare extensive <UNK> carbon \n",
      "training interrupted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def format_idx(idx):\n",
    "    # Just cuts out the padding of word index lists\n",
    "    li = []\n",
    "    for i in idx:\n",
    "        if i ==0:\n",
    "            break\n",
    "        else:\n",
    "            li.append(i)\n",
    "    return li\n",
    "\n",
    "BLEU = []\n",
    "epochs = 30 # How many times we loop over the whole training data\n",
    "batch_size = 92 # After how many sequences do we update the weights?\n",
    "print \"there will be {} samples in the final batch\".format(len(source_train)%batch_size)\n",
    "fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "try:\n",
    "    batch_n = 0\n",
    "    ti = time.time()\n",
    "    print \"training has begun...\"\n",
    "    for epoch in range(epochs):    \n",
    "        for s_batch, t_batch in batch_source_target(source_train, target_train, batch_size):\n",
    "            feed_dict = make_feed_dict(fd_keys, s_batch, t_batch)\n",
    "            _, l = sess.run([train_op, loss], feed_dict)\n",
    "            \n",
    "            #if batch_n == 0 or batch_n == 60:\n",
    "            #    \n",
    "            if (batch_n==0) or (batch_n%100) == 0:\n",
    "                loss_track.append(l)\n",
    "                print \"epoch {}\".format(epoch+1)\n",
    "                print 'batch {}'.format(batch_n)\n",
    "                print 'loss: {}'.format(sess.run(loss, feed_dict))\n",
    "                predict_ = sess.run(decoder_prediction, feed_dict)\n",
    "                #predictions = [remove_EOS_PAD(pred) for pred in predict_.T]\n",
    "                #actuals = [[remove_EOS_PAD(act)] for act in fd[decoder_targets].T]\n",
    "                #BLEU2 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.5,0.5))\n",
    "                #BLEU.append(BLEU2)\n",
    "                i =1\n",
    "                for (inp, act, pred) in zip(feed_dict[encoder_inputs].T,\n",
    "                                                         feed_dict[decoder_targets].T,\n",
    "                                                         predict_.T)[10:12]:\n",
    "                    print '  sample {}:'.format(i)\n",
    "                    print '    input     > {} \\n {}'.format(format_idx(inp), ids_to_phrases(inp, id2word_s).encode(\"utf-8\"))\n",
    "                    print '    actual     > {} \\n {}'.format(format_idx(act), ids_to_phrases(act, id2word_t).encode(\"utf-8\"))\n",
    "                    print '    predicted     > {} \\n {}'.format(format_idx(pred), ids_to_phrases(inp, id2word_t).encode(\"utf-8\"))\n",
    "                    i+=1\n",
    "            batch_n += 1\n",
    "            \n",
    "    print 'Training is complete'\n",
    "except KeyboardInterrupt:\n",
    "    print 'training interrupted'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(loss_track)), loss_track)\n",
    "#l = [s for i,s in sorted(zip([len(row) for row in l], l))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
