{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The dataset\n",
    "\n",
    "Here I will introduce the dataset, experiment with it, tokenize it, preprocess it and save it to a new set of files. I will also write a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C&apos; était le 10 novembre de l&apos; année dernière alors que le technicien de scène Kevin Monk , son épouse enseignante de jardin d&apos; enfants , Roseanna , et Geneviève se préparaient à partir en voyage en famille .\n",
      "Ce changement d&apos; époque est en même temps le problème de toute la démocratie chrétienne , et de ce fait , également celui d&apos; Angela Merker .\n",
      "Obama ? Le premier président anti-américain . Voilà pourquoi .\n",
      "Alors que les communes signalent une chute du nombre de demandes de renouvellement du permis , les autorités dans la capitale sont envahies de demandes et le nombre de demandes de renouvellement du permis est allé croissant cette semaine .\n",
      "Sur l&apos; île , l&apos; utilisation d&apos; Internet est extrêmement limitée .\n",
      "Le passé 4 novembre un dactyloscopiste du CTI de Cali a passé à l&apos; histoire comme la personne qui a confirmé l&apos; identité du chef suprême des Farc : &quot; Alfonso Cano &quot; , en confrontant ses empreintes digitales .\n",
      "Les deux équi\n",
      "Corpora aligned: True\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "f1_fr = codecs.open(\"DATA/en-fr_paropt/dev.tok.fr\", encoding='utf-8')\n",
    "f2_fr = codecs.open(\"DATA/en-fr_paropt/train.fr\", encoding='utf-8')\n",
    "test_fr = codecs.open(\"DATA/en-fr_paropt/test.fr\", encoding='utf-8')\n",
    "\n",
    "f1_en = codecs.open(\"DATA/en-fr_paropt/dev.tok.en\", encoding='utf-8')\n",
    "f2_en = codecs.open(\"DATA/en-fr_paropt/train.en\", encoding='utf-8')\n",
    "test_en = codecs.open(\"DATA/en-fr_paropt/test.en\", encoding='utf-8')\n",
    "\n",
    "lang_t = 'en'\n",
    "lang_s = 'fr'\n",
    "\n",
    "raw_source_train = eval(\"f1_\"+lang_s).read()+\"\\n\"+eval(\"f2_\"+lang_s).read()\n",
    "raw_target_train = eval(\"f1_\"+lang_t).read()+\"\\n\"+eval(\"f2_\"+lang_t).read()\n",
    "\n",
    "raw_source_test = eval(\"test_\"+lang_s).read()\n",
    "raw_target_test = eval(\"test_\"+lang_t).read()\n",
    "\n",
    "print raw_source_train[0:1000]\n",
    "print \"Corpora aligned: {}\".format(len(raw_source_train.split('\\n')) == len(raw_target_train.split('\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenizing and preprocessing\n",
    "Here we can see that sentences are not really split over lines since \"Here is why ?\" is technically it's own sentence - however, splitting these cases up will lead to misalignment between the english and french corpora so it's too risky. What I will do then is just tokenize each line, remove capitalisation and then replace the final punctuation with `<EOS` and unknown characters with `<UNK>` or `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fix a small pathology with the datasets, whereby spaces are put between words\n",
    "# and end of sentence tokens\n",
    "def format_eos(text):\n",
    "    #text = text.replace(' .', '.')\n",
    "    #text = text.replace('?')\n",
    "    #text = text.replace('!', '.')\n",
    "    #text = text.replace(\"idn &apos;t\", 'did not')\n",
    "    #text = text.replace(\"houldn &apos;t\", 'should not')\n",
    "    #text = text.replace(\"t &apos;s\", 'it is')\n",
    "    #text = text.replace('%', 'percent')\n",
    "    return text\n",
    "\n",
    "#text_fr = format_eos(text_fr)\n",
    "#text_en = format_eos(text_en)\n",
    "#print \"still aligned:\", len(raw_source.split('\\n')) == len(raw_target.split('\\n'))\n",
    "#print raw_source[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&quot; Le canot , et le guidon , et le héraut &quot; sont reçus avec mépris par les Iroquois , qui huent les émissaires , tirent des flèches sur leurs embarcations , arborent le scalp d&apos; un Algonquin allié aux Français .\n",
      "The &quot; canoe , guidon and herald &quot; were received with scorn by the Iroquois , who hooted at the emissaries , waved the scalp of an Algonquin allied with the French , and shot arrows at the French boats .\n",
      "------------\n",
      "Il a aussi laissé entendre qu&apos; il opterait pour un assouplissement des négociations avec les Etats-Unis .\n",
      "He also hinted at more flexibility in negotiations with the United States .\n",
      "------------\n",
      " Judging by keywords and rudimentary knowledge of French, I suspect the corpora are still aligned.\n",
      "corpora still same length: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk.data\n",
    "import random\n",
    "# First split up into lines using the Punkt tokenizer\n",
    "#fr_sent_detector = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "#en_sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#fr_sents = fr_sent_detector.tokenize(text_fr)\n",
    "#en_sents = en_sent_detector.tokenize(text_en)\n",
    "\n",
    "# Split the text up into lines\n",
    "# Randomise the lists but maintain parallel ordering\n",
    "s = zip(raw_source_train.split('\\n'), raw_target_train.split('\\n'))\n",
    "np.random.shuffle(s)\n",
    "raw_source_train, raw_target_train = zip(*s)\n",
    "\n",
    "print \"\\n\".join([raw_source_train[0],raw_target_train[0]])\n",
    "print \"------------\"\n",
    "print \"\\n\".join([raw_source_train[302],raw_target_train[302]])\n",
    "print \"------------\\n Judging by keywords and rudimentary knowledge of French, I suspect the corpora are still aligned.\"\n",
    "print \"corpora still same length:\", len(source_phrases)== len(target_phrases), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data tokenised. There are 1013625 sequences for the NMT to learn from.\n",
      "[[u'&quot;', u'le', u'canot', u',', u'et', u'le', u'guidon', u',', u'et', u'le', u'h\\xe9raut', u'&quot;', u'sont', u're\\xe7us', u'avec', u'm\\xe9pris', u'par', u'les', u'iroquois', u',', u'qui', u'huent', u'les', u'\\xe9missaires', u',', u'tirent', u'des', u'fl\\xe8ches', u'sur', u'leurs', u'embarcations', u',', u'arborent', u'le', u'scalp', u'd&apos;', u'un', u'algonquin', u'alli\\xe9', u'aux', u'fran\\xe7ais', u'.', u'<EOS>'], [u'pourtant', u',', u'les', u'scandales', u'survenus', u'dans', u'le', u'pass\\xe9', u'ont', u'inqui\\xe9t\\xe9', u'et', u'effray\\xe9', u'les', u'consommateurs', u'au', u'plus', u'haut', u'point', u'.', u'<EOS>'], [u'le', u'patrimoine', u'arm\\xe9nien', u'd&apos;', u'arm\\xe9nie', u'est', u'menac\\xe9', u':', u'ainsi', u'dans', u'la', u'ville', u'de', u'djoulfa', u',', u'en', u'r\\xe9publique', u'de', u'nakhitchevan', u',', u'qui', u'est', u'encore', u'sous', u'contr\\xf4le', u'de', u'l&apos;', u'azerba\\xefdjan', u',', u'le', u'cimeti\\xe8re', u'm\\xe9di\\xe9val', u'a', u'\\xe9t\\xe9', u'totalement', u'd\\xe9truit', u'en', u'1998', u'.', u'<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def tokenize_sentences(text):\n",
    "    new_text = []\n",
    "    flat = []\n",
    "    for sentence in text:\n",
    "        # This splits up tokens within a sentence\n",
    "        tok_sent = (sentence.lower()).split(' ')\n",
    "        flat += tok_sent\n",
    "        # I'm keeping the final punctuation and appending the \n",
    "        # <EOS> tag after it\n",
    "        new_text.append(tok_sent+[u'<EOS>'])\n",
    "    freq = nltk.FreqDist(flat)\n",
    "    return new_text, freq\n",
    "\n",
    "tok_source_test, freq_s = tokenize_sentences(raw_source_test)\n",
    "tok_target_test, freq_t = tokenize_sentences(raw_target_test)\n",
    "\n",
    "tok_source_train, freq_s = tokenize_sentences(raw_source_train)\n",
    "tok_target_train, freq_t = tokenize_sentences(raw_target_train)\n",
    "print \"All data tokenised. There are {} sequences for the NMT to learn from.\".format(len(tok_source_train))\n",
    "print tok_source_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name ):\n",
    "    with open(name + '.pickle', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vocabularies, Sequences and Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I need to:\n",
    "- Find the frequencies of words, limiting the vocabulary size to some number of the most common words and replacing out of vocabulary words with `<UNK>`\n",
    "- Give each token an ID, where `<EOS>` =1, `<UNK>` =2 and the remaining tokens are assigned numbers from 3 to vocab_size+1 at random. ID=0 is kept for padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr vocab size restricts to 1.66873404273 percent of total vocab.\n",
      "Percentage of whole fr corpus covered by vocab: 85.7582948507\n",
      "en vocab size restricts to 2.01004351744 percent of total vocab.\n",
      "Percentage of whole en corpus covered by vocab: 86.3594803393\n",
      "[(0, u'<PAD>'), (1, u'<EOS>'), (2, u'<UNK>'), (3, u'1,5'), (4, u'four'), (5, u'accueille'), (6, u'travaux'), (7, u'fourni'), (8, u's\\xe9curit\\xe9'), (9, u'devenir'), (10, u'lors'), (11, u'c\\xe9l\\xe9bration'), (12, u'lord'), (13, u'saskatchewan'), (14, u'touristes'), (15, u'consid\\xe9rable'), (16, u'efficacement'), (17, u'fond\\xe9e'), (18, u'succession'), (19, u'humeur')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def word_to_ids(freq_dist, vocab_size, lang):\n",
    "    total_words = len(freq_dist.keys())\n",
    "    total_length = sum(freq_dist.values())\n",
    "    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/total_words))\n",
    "    vocab = dict(freq_dist.most_common(vocab_size+1))\n",
    "    covered_with_vocab = sum(vocab.values()) # Total of frequencies\n",
    "    print \"Percentage of whole {} corpus covered by vocab: {}\".format(lang, \n",
    "                                            100*(covered_with_vocab/float(total_length)))\n",
    "    # The identities begin at 3, since <EOS>=1 and <UNK>=2\n",
    "    word_to_ids = dict([(word, i+3) for i, word in enumerate(vocab.keys())])\n",
    "    word_to_ids[u'<UNK>'] = 2\n",
    "    word_to_ids[u'<EOS>'] = 1\n",
    "    word_to_ids[u'<PAD>'] = 0\n",
    "    id_to_words = {}\n",
    "    for word, idx in word_to_ids.items():\n",
    "        id_to_words[idx] = word\n",
    "    return word_to_ids, id_to_words\n",
    "\n",
    "def replace_with_word_id(text, word_to_ids, lang):\n",
    "    '''\n",
    "    take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK> whilst giving each\n",
    "    token a numerical ID, return new list of lists'''\n",
    "    prep_text = []\n",
    "    print len(text)\n",
    "    i = 0\n",
    "    for sequence in text:\n",
    "        ids_sent = [] # sentence with words replaced by ids\n",
    "        for token in sequence:\n",
    "            if token not in word_to_ids.keys():\n",
    "                if token == '<EOS>':\n",
    "                    ids_sent.append(1)\n",
    "                else:\n",
    "                    ids_sent.append(2)\n",
    "            else:\n",
    "                ids_sent.append(word_to_ids[token])\n",
    "        prep_text.append(ids_sent)\n",
    "        i+=1\n",
    "        if i%10000 == 0:\n",
    "            print \"Sequence number {} preprocessed\".format(i)\n",
    "    return prep_text\n",
    "\n",
    "vocab_size_s = 6000\n",
    "vocab_size_t = 6000\n",
    "\n",
    "#seq_data_source = seq_length_stats(tokenized_source, lang_s)\n",
    "#seq_data_target = seq_length_stats(tokenized_target, lang_t)\n",
    "\n",
    "\"\"\" For now, sequences form rows of the matrix, with the number of columns \n",
    "    equal to the maximum length of sequence (or timesteps)\"\"\"\n",
    "vocab_s, id_to_word_s = word_to_ids(freq_s, vocab_size_s, lang_s)\n",
    "vocab_t, id_to_word_t = word_to_ids(freq_t, vocab_size_t, lang_t)\n",
    "print id_to_word_s.items()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013625\n",
      "Sequence number 10000 preprocessed\n",
      "Sequence number 20000 preprocessed\n",
      "Sequence number 30000 preprocessed\n",
      "Sequence number 40000 preprocessed\n",
      "Sequence number 50000 preprocessed\n",
      "Sequence number 60000 preprocessed\n",
      "Sequence number 70000 preprocessed\n",
      "Sequence number 80000 preprocessed\n",
      "Sequence number 90000 preprocessed\n",
      "Sequence number 100000 preprocessed\n",
      "Sequence number 110000 preprocessed\n",
      "Sequence number 120000 preprocessed\n",
      "Sequence number 130000 preprocessed\n",
      "Sequence number 140000 preprocessed\n",
      "Sequence number 150000 preprocessed\n",
      "Sequence number 160000 preprocessed\n",
      "Sequence number 170000 preprocessed\n",
      "Sequence number 180000 preprocessed\n",
      "Sequence number 190000 preprocessed\n",
      "Sequence number 200000 preprocessed\n",
      "Sequence number 210000 preprocessed\n",
      "Sequence number 220000 preprocessed\n",
      "Sequence number 230000 preprocessed\n",
      "Sequence number 240000 preprocessed\n",
      "Sequence number 250000 preprocessed\n",
      "Sequence number 260000 preprocessed\n",
      "Sequence number 270000 preprocessed\n",
      "Sequence number 280000 preprocessed\n",
      "Sequence number 290000 preprocessed\n",
      "Sequence number 300000 preprocessed\n",
      "Sequence number 310000 preprocessed\n",
      "Sequence number 320000 preprocessed\n",
      "Sequence number 330000 preprocessed\n",
      "Sequence number 340000 preprocessed\n",
      "Sequence number 350000 preprocessed\n",
      "Sequence number 360000 preprocessed\n",
      "Sequence number 370000 preprocessed\n",
      "Sequence number 380000 preprocessed\n",
      "Sequence number 390000 preprocessed\n",
      "Sequence number 400000 preprocessed\n",
      "Sequence number 410000 preprocessed\n",
      "Sequence number 420000 preprocessed\n",
      "Sequence number 430000 preprocessed\n",
      "Sequence number 440000 preprocessed\n",
      "Sequence number 450000 preprocessed\n",
      "Sequence number 460000 preprocessed\n",
      "Sequence number 470000 preprocessed\n",
      "Sequence number 480000 preprocessed\n",
      "Sequence number 490000 preprocessed\n",
      "Sequence number 500000 preprocessed\n",
      "Sequence number 510000 preprocessed\n",
      "Sequence number 520000 preprocessed\n",
      "Sequence number 530000 preprocessed\n",
      "Sequence number 540000 preprocessed\n",
      "Sequence number 550000 preprocessed\n",
      "Sequence number 560000 preprocessed\n",
      "Sequence number 570000 preprocessed\n",
      "Sequence number 580000 preprocessed\n",
      "Sequence number 590000 preprocessed\n",
      "Sequence number 600000 preprocessed\n",
      "Sequence number 610000 preprocessed\n",
      "Sequence number 620000 preprocessed\n",
      "Sequence number 630000 preprocessed\n",
      "Sequence number 640000 preprocessed\n",
      "Sequence number 650000 preprocessed\n",
      "Sequence number 660000 preprocessed\n",
      "Sequence number 670000 preprocessed\n",
      "Sequence number 680000 preprocessed\n",
      "Sequence number 690000 preprocessed\n",
      "Sequence number 700000 preprocessed\n",
      "Sequence number 710000 preprocessed\n",
      "Sequence number 720000 preprocessed\n",
      "Sequence number 730000 preprocessed\n",
      "Sequence number 740000 preprocessed\n",
      "Sequence number 750000 preprocessed\n",
      "Sequence number 760000 preprocessed\n",
      "Sequence number 770000 preprocessed\n",
      "Sequence number 780000 preprocessed\n",
      "Sequence number 790000 preprocessed\n",
      "Sequence number 800000 preprocessed\n",
      "Sequence number 810000 preprocessed\n"
     ]
    }
   ],
   "source": [
    "source_train = replace_with_word_id(tok_source_train, id_to_word_s, lang_s)\n",
    "target_train = replace_with_word_id(tok_target_train, id_to_word_t, lang_t)\n",
    "# Obviously vocab preprocessing doesn't get to see the test data\n",
    "source_test = replace_with_word_id(tok_source_test, id_to_word_s, lang_s)\n",
    "target_test = replace_with_word_id(tok_target_test, id_to_word_t, lang_t)\n",
    "\n",
    "#train_s, test_s = source[200::], source[0:200]\n",
    "#train_t, test_t = target[200::], target[0:200]\n",
    "#print \"Corpora still aligned:\", len(train_s) == len(train_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving preprocessed objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(source_train, \"source_train\")\n",
    "save_obj(target_train, \"target_train\")\n",
    "save_obj(source_test, \"source_test\")\n",
    "save_obj(target_test, \"target_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mettre bonnes adolescents veux solution \n",
      "meat twice opportunities votes confidence \n"
     ]
    }
   ],
   "source": [
    "def ids_to_phrases(idx_list, id_to_word):\n",
    "    # Takes list of word ids and returns a string of words\n",
    "    # Mainly for use in analysis\n",
    "    phrase = ''\n",
    "    id_dict = id_to_word\n",
    "    i=0\n",
    "    while idx_list[i] not in (1,0):\n",
    "        phrase+= id_dict[idx_list[i]]+' '\n",
    "        i+=1\n",
    "    return phrase\n",
    "# Test the functionality\n",
    "print ids_to_phrases([234, 432, 102, 12,43,1], id_to_word_s)\n",
    "print ids_to_phrases([234, 432, 102, 12,43,1], id_to_word_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "I am going to use the N-gram BLEU score as my evaluation metric, changing the parameter N to see the effect it has on my model, relative to the benchmark.\n",
    "\n",
    "## Implementing the BLEU metric\n",
    "This metric will be used to test how good a prediction actually was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1 score test is 0.282160574964.\n",
      "BLEU2 score test is 0.218560641558.\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import bleu_score\n",
    "\n",
    "def nonunique_ngrams(phrase, N):\n",
    "        \n",
    "        N_grams = {}\n",
    "        for i in range(len(phrase)):\n",
    "            li = phrase[i:i+N]\n",
    "            ng = ' '.join([str(s) for s in li])\n",
    "            if len(li) == N or len(phrase)<N:\n",
    "                try:\n",
    "                    N_grams[ng] += 1\n",
    "                except KeyError:\n",
    "                    N_grams[ng] = 1\n",
    "        return N_grams, len(phrase)\n",
    "\n",
    "def remove_EOS_PAD(long_phrase):\n",
    "    i=0\n",
    "    phrase= []\n",
    "    while (long_phrase+[0])[i] not in (0,1):\n",
    "        phrase.append(long_phrase[i])\n",
    "        i+=1\n",
    "    return phrase\n",
    "\n",
    "def BLEU_metric(long_t_phrase, long_p_phrase, N):\n",
    "    # a) Find all (non-unique) N grams in target and predicted phrase and frequencies\n",
    "    # Firstly need to see how long the content is (not <EOS> or <PAD>)\n",
    "    t_phrase = remove_EOS_PAD(long_t_phrase)\n",
    "    p_phrase = remove_EOS_PAD(long_p_phrase)\n",
    "    N = min(N, len(p_phrase), len(t_phrase))\n",
    "    t_ngrams, t_len = nonunique_ngrams(t_phrase, N)\n",
    "    p_ngrams, p_len = nonunique_ngrams(p_phrase, N)\n",
    "    \n",
    "    #print \"N-gram count is {}\".format(p_ngrams)\n",
    "    p_num = sum(p_ngrams.values())\n",
    "    \n",
    "    #print p_num\n",
    "    # b) How many of the N-grams in the prediction appear in the target + frequencies\n",
    "    # d) Limit the number of correct counts of an Ngram to \n",
    "    #    the number of times it appears in the target\n",
    "    cross_count = []\n",
    "    for ng in p_ngrams.keys():\n",
    "        try:\n",
    "            cross_count.append(min((t_ngrams[ng], p_ngrams[ng])))\n",
    "        except KeyError:\n",
    "            cross_count.append(0)\n",
    "    # e) return the above number divided by the total number of (non-unique) N-grams\n",
    "    # I take the log of the BLEU scores so I can sum them \n",
    "    # and exponentiate to calculate the product (for geometric mean later on)\n",
    "    #print float(p_num)\n",
    "    return [np.log(sum(cross_count)/float(p_num)), t_len, p_len]\n",
    "\n",
    "# Test handwritten BLEU\n",
    "p_phrase1 = [4,5,4,5,4,5, 1, 0]\n",
    "t_phrase = [4,5,6,34,8,76, 87, 1]\n",
    "assert len(t_phrase) == len(p_phrase1)\n",
    "t_phrase = remove_EOS_PAD(t_phrase)\n",
    "p_phrase1 = remove_EOS_PAD(p_phrase1)\n",
    "\n",
    "print \"BLEU1 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [remove_EOS_PAD(p_phrase1)], weights=([1])))\n",
    "print \"BLEU2 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [remove_EOS_PAD(p_phrase1)], weights=([0.5,0.5])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Benchmark Model\n",
    "\n",
    "Here I will implement the dictionary based translation benchmark using a free English-to-French dictionary [text file](http://ktranslator.sourceforge.net/dictionaries.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'grossier', [u'crude', u'raw', u'rough', u'coarse', u'crude', u'harsh']), (u'bouddhisme', [u'buddhism']), (u'four', [u'furnace', u'kiln', u'oven', u'stove']), (u's\\xe9curit\\xe9', [u'safety', u'security']), (u'\\xe0 cause de rien', [u'for no reason']), (u'yougoslavie', [u'yugoslavia']), (u'lors', [u'for', u'during', u'whereas', u'while', u'whilst', u'on the occasion of']), (u'coucou', [u'cuckoo']), (u'albumen', [u'albumen']), (u'balise', [u'buoy'])]\n",
      "7673\n"
     ]
    }
   ],
   "source": [
    "filename = ''\n",
    "if lang_t == 'en':\n",
    "    filename = \"DATA/freedict-fra-eng.dic\"\n",
    "else:\n",
    "    filename = \"DATA/freedict-eng-fra.dic\"\n",
    "\n",
    "f = open(filename,'rb')\n",
    "txt = unicode(f.read(), 'utf-8')\n",
    "\n",
    "source_to_target = {}\n",
    "for line in txt.split('\\n'):\n",
    "    words = line.split('\\t')\n",
    "    try:\n",
    "        source, targets = words[0], words[1].split('; ')\n",
    "        source_to_target[source.lower()] = (', '.join(targets).lower()).split(', ')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "source_to_target.update({u'<EOS>': [u'<EOS>'], u'<PAD>': [u'<PAD>'], u'<UNK>':[u'<UNK>'], u'&quot;':[u'&quot;'], \n",
    "                 u'&apos;':[u'&apos;'], u'(':[u'('], u')':[u')'], u':':[u':'], u'%':[u'%'], u'$':[u'$']})\n",
    "\n",
    "print source_to_target.items()[0:10]\n",
    "print len(source_to_target.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some words may not have direct translations in the dictionary but may be phrasal, so rather than look up words directly like `source_to_target[source_word]` I am going to find out whether the source word is contained within any of the keys in the dictionary, then use the shortest of those keys as the chosen source phrase/word. When translating the document, if a word is come across that is not in the dictionary I'll try to find a synonym, if that fails then just return `<UNK>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON object could be decoded for phrase 39\n",
      "No JSON object could be decoded for phrase 94\n",
      "No JSON object could be decoded for phrase 151\n",
      "No JSON object could be decoded for phrase 197\n",
      "There are 4 phrases which could not be translated first time around.\n",
      "There are 0 phrases which could not be translated in pass 2.\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def googletrans_word_by_word(s_text, s_to_t, target_lang='fr'):\n",
    "    translator = Translator()\n",
    "    trans_corpus = []\n",
    "    skipped_phrases = []\n",
    "    for i, phrase in enumerate(s_text):\n",
    "        trans_phrase = []\n",
    "        # Determine if the corpus is ids or words\n",
    "        if type(phrase[0]) is int:\n",
    "            phrase = ids_to_phrases(phrase, id_to_word_s).split(' ')\n",
    "        try:\n",
    "            trans_corpus.append([trans.text for \n",
    "                                 trans in translator.translate(phrase, dest=target_lang)])\n",
    "        except ValueError as err:\n",
    "            # Making a new Translator instance seems to help JSON errors\n",
    "            translator = Translator()\n",
    "            skipped_phrases.append(phrase)\n",
    "            print \"{} for phrase {}\".format(err, i)\n",
    "    return trans_corpus, skipped_phrases\n",
    "\n",
    "BM_translated_corp, skipped_phrases = googletrans_word_by_word(test_s, source_to_target, target_lang=lang_t)\n",
    "print \"There are {} phrases which could not be translated first time around.\".format(\n",
    "                                                    len(skipped_phrases))\n",
    "i=2\n",
    "while len(skipped_phrases)>0:\n",
    "    tc_s, skipped_phrases = googletrans_word_by_word(skipped_phrases, source_to_target, target_lang=lang_t)\n",
    "    BM_translated_corp+=tc_s\n",
    "    print \"There are {} phrases which could not be translated in pass {}.\".format(\n",
    "                                                        len(skipped_phrases), i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'm.', u'<UNK>', u'know', u'good', u'what', u'it', u'<UNK>', u'always', u'a few', u'complaints', u'.', u'']\n"
     ]
    }
   ],
   "source": [
    "#tc = [(' '.join(phr)).split(' ') for phr in tc]\n",
    "#BM_translated_corp = tc\n",
    "#print \"corpora probably still aligned: {}\".format(len(tc) == len(test_t))\n",
    "print BM_translated_corp[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: \n",
      "it &apos;s not just the <UNK> <UNK> , though , but the quality of the <UNK> and the depth . \n",
      "Prediction: \n",
      "more the problem n&apos; East not the <UNK> themselves , more good the quality from the <UNK> and the depth . \n",
      "Unigram BLEU score is 0.00277178769306.\n",
      "Bigram BLEU score is 0.00934120242305.\n",
      "4-Gram BLEU: 0.0171484232441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "test_targets = [[ids_to_phrases(phrase,id_to_word_t)]for phrase in test_t]\n",
    "\n",
    "BM_BLEU4 = bleu_score.corpus_bleu(test_targets, BM_translated_corp, weights=(0.25,0.25, 0.25,0.25))\n",
    "BM_BLEU2 = bleu_score.corpus_bleu(test_targets, BM_translated_corp, weights=(0.5,0.5))\n",
    "BM_BLEU1 = bleu_score.corpus_bleu(test_targets, BM_translated_corp, weights=([1]))\n",
    "\n",
    "print \"Actual: \\n\", (\" \".join(test_targets[10])).encode('utf-8')\n",
    "print \"Prediction: \\n\", (\" \".join(BM_translated_corp[10])).encode('utf-8')\n",
    "print \"Unigram BLEU score is {}.\".format(BM_BLEU1)\n",
    "print \"Bigram BLEU score is {}.\".format(BM_BLEU2)\n",
    "print \"4-Gram BLEU: {}\".format(BM_BLEU4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word IDs and word-to-vec vectors\n",
    "\n",
    "Since we are interested in the process of learning weights within the RNNs to predict seq2seq mappings rather than embeddings I have chosen to use pretrained word embeddings ino order to cut down training time. [THis is](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Download-the-Embeddings) where the embeddings are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding matrix for fr has 3002 columns and 64 rows.\n",
      "201 vocab words were not in the fr embeddings file.\n",
      "The embedding matrix for en has 3002 columns and 64 rows.\n",
      "136 vocab words were not in the en embeddings file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "def get_embeddings(id_to_word, lang):\n",
    "    # We load pretrained word2vec embeddings from polyglot to save on training time\n",
    "    filename ='DATA/polyglot-'+lang+'.pkl'\n",
    "    pretrain_vocab, pretrain_embed = pickle.load(open(filename, 'rb'))\n",
    "    embed_vocab = [pretrain_embed[pretrain_vocab.index('<PAD>')], pretrain_embed[pretrain_vocab.index('</S>')]]\n",
    "    skip_count = 0\n",
    "    skipped_words = []\n",
    "    for idx, word in sorted(id_to_word.items()[2::]):\n",
    "        try:\n",
    "            pretrain_idx = pretrain_vocab.index(word)\n",
    "            embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # it could be that the word is a name which needs to \n",
    "                # be capitalized. Try this...\n",
    "                pretrain_idx = pretrain_vocab.index(str(word.title()))\n",
    "                embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # it could be that the word is an achronym which needs to \n",
    "                    # be upper case. Try this...\n",
    "                    pretrain_idx = pretrain_vocab.index(word.upper())\n",
    "                    embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "                except ValueError:\n",
    "                    # Give up trying to find an embedding.\n",
    "                    # How many words are skipped? Which ones?\n",
    "                    skip_count +=1\n",
    "                    skipped_words.append(word)\n",
    "                    # Let's just initialise the embedding to a random normal distribution\n",
    "                    embed_vocab.append(np.random.normal(loc=0.0, scale=np.sqrt(2)/4, size=64))\n",
    "    embed_vocab = np.array(embed_vocab, dtype=np.float32)\n",
    "    print \"The embedding matrix for {} has {} columns and {} rows.\".format(lang, \n",
    "                                                embed_vocab.shape[0], embed_vocab.shape[1])\n",
    "    print \"{} vocab words were not in the {} embeddings file.\".format(skip_count, lang)\n",
    "    return embed_vocab, skipped_words\n",
    "# the ith word in words corresponds to the ith embedding \n",
    "\n",
    "embed_vocab_s, skipped_s = get_embeddings(id_to_word_s, lang=lang_s)\n",
    "embed_vocab_t, skipped_t = get_embeddings(id_to_word_t, lang=lang_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Note: to find a word given an index we use `id_to_word_en` and vice-versa we use `vocab_en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "[u'talysh', u'100', u'90', u'95', u'1990', u'1998', u'fatulayeva', u'400', u'isn', u'wasn', u'bitcoin', u'imbongi', u'victor-gadbois', u'13', u'12', u'esitjeni', u'vice-president', u'1973', u'&#93;', u'50', u'bitcoins', u'1995', u'spijkenisse', u'pelta', u'anti-corruption', u'high-speed', u'1', u'lerik', u'svarc', u'1956', u'wentzler', u'&#91;', u'rajchl', u'2', u'11', u'10', u'15', u'14', u'17', u'16', u'19', u'18', u'mursi', u'200', u'30,000', u'mincy', u'mccain', u'no-one', u'&apos;re', u'60', u'anto', u'500', u'unasur', u'&apos;ll', u'freudenberg', u'&apos;arenberg', u'2000', u'2001', u'5', u'mexican-americans', u'--', u'vaqueros', u'20th', u'21st', u'6', u'pattloch', u'24', u'25', u'26', u'20', u'21', u'22', u'23', u'wouldn', u'shekels', u'7', u'&apos;', u'70', u'kawah', u'19th', u'8', u'left-wing', u'&apos;d', u'&apos;m', u'&apos;t', u'&apos;s', u'mcdonald', u'long-term', u'ijen', u'34', u'9', u'garavani', u'300', u'75', u'doesn', u'&quot;', u'jedlicka', u'2006', u'2007', u'2004', u'2005', u'2008', u'2009', u'15,000', u'ods', u'gausse', u'morsi', u'restall', u'31', u'30', u'37', u'35', u'baranets', u'1991', u'rakfisk', u'2011', u'2010', u'2013', u'2012', u'nku', u'700', u'80', u'&apos;ve', u'1.5', u'so-called', u'120', u'pacl\\xedk', u'1994', u'i.e.', u'mirzayeva', u'2014', u'46', u'40', u'didn', u'audatex', u'1988']\n"
     ]
    }
   ],
   "source": [
    "print vocab_t['<PAD>'], vocab_t['<EOS>'], vocab_t['<UNK>']\n",
    "print skipped_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** We can see above ** that the English words which were not in the embedding files are fairly specialist words or numerical values (which are the same in French) so hopefully they won't be too much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[970, 118, 2, 20, 1655, 2853, 414, 2477, 1165, 294, 1697, 1310, 1032, 2395, 1978, 2, 1584, 2474, 1367, 1655, 2388, 2136, 1561, 511, 1], [1010, 1414, 1555, 2318, 2, 1596, 1281, 111, 304, 1432, 163, 1365, 414, 969, 2, 511, 1], [1844, 224, 2474, 176, 1655, 2969, 2136, 1985, 999, 2, 275, 841, 414, 1728, 2238, 511, 1], [2784, 711, 414, 2351, 224, 2361, 2, 1655, 2485, 256, 1033, 422, 1409, 972, 2, 2292, 1584, 2477, 2538, 1655, 2, 2136, 2224, 913, 511, 1], [965, 2136, 2893, 1655, 2, 2, 224, 556, 2888, 1307, 414, 2643, 2742, 1544, 2, 2, 1655, 2827, 2, 1596, 1575, 1426, 1033, 414, 1753, 1655, 698, 2424, 2361, 1089, 511, 1], [2052, 1806, 2931, 1555, 2, 224, 1169, 668, 897, 1831, 1840, 1309, 2167, 1596, 1396, 224, 2920, 224, 2386, 1033, 366, 511, 1], [1238, 2467, 1858, 2474, 918, 414, 2477, 695, 2, 2, 1432, 422, 2, 2, 414, 2, 224, 111, 1238, 1132, 2, 2172, 2361, 2036, 1655, 1451, 2736, 1024, 1189, 998, 511, 1]]\n"
     ]
    }
   ],
   "source": [
    "print test_s[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see a couple of things:\n",
    "- The list of lists above does not have consistent lengths of rows (it's not a matrix)\n",
    "- In order to process large amounts of data we need to break data up into batches of sequences\n",
    "\n",
    "The format that I need for the seq-to-seq model is a matrix - we do this by padding shorter sequences in a batch with the `<PAD>` token (represented already as the 0th column of the embedding matrix). the of dimension `(max sequence length in batch, batch size)`, so sequences are represented as the columns of the input matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 2 4 1]\n",
      " [2 0 2 2]\n",
      " [3 0 0 0]]\n",
      "[[ 602 1255 2269 1099  228  590 2434]\n",
      " [ 304 1566  961 2019  908    2 1894]\n",
      " [   2  590 2827 2639 2196 2438 2987]\n",
      " [2811 1056 1322  228 2569  234    2]\n",
      " [1566 2438  700 1123 1440 2207 1894]\n",
      " [3000  590 2178  323 1370 2774    2]\n",
      " [2361 1148 2994  211 1196  211    2]\n",
      " [ 550 2019  963  451 2207    2   64]\n",
      " [ 590 2226  463  590 1304 1454    2]\n",
      " [1316  211  908 2961 2634    2 2931]\n",
      " [2405    2 2611  211 1992  908    1]\n",
      " [ 590  251 1717 2634 2218 2544    0]\n",
      " [2021 2136    2  108 1890  590    0]\n",
      " [2338    2  769 1440    2    2    0]\n",
      " [ 554 2241 2269  590 2931 2438    0]\n",
      " [ 590 1482  451 1584    1    2    0]\n",
      " [1846  183    2 2438    0 2931    0]\n",
      " [2931 2587    2    2    0    1    0]\n",
      " [   1 2728  211 2222    0    0    0]\n",
      " [   0 2611    2 2438    0    0    0]\n",
      " [   0  509    2  590    0    0    0]\n",
      " [   0 1742 2817    2    0    0    0]\n",
      " [   0  554 2931 2438    0    0    0]\n",
      " [   0  590    1  590    0    0    0]\n",
      " [   0 2455    0 1306    0    0    0]\n",
      " [   0  916    0 2931    0    0    0]\n",
      " [   0 2438    0    1    0    0    0]\n",
      " [   0 2503    0    0    0    0    0]\n",
      " [   0  211    0    0    0    0    0]\n",
      " [   0  740    0    0    0    0    0]\n",
      " [   0    2    0    0    0    0    0]\n",
      " [   0  251    0    0    0    0    0]\n",
      " [   0 2611    0    0    0    0    0]\n",
      " [   0  509    0    0    0    0    0]\n",
      " [   0 1419    0    0    0    0    0]\n",
      " [   0  554    0    0    0    0    0]\n",
      " [   0  105    0    0    0    0    0]\n",
      " [   0  652    0    0    0    0    0]\n",
      " [   0 2434    0    0    0    0    0]\n",
      " [   0 1270    0    0    0    0    0]\n",
      " [   0 2931    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "test_x = [[5,2,3],[2], [4,2], [1,2]]\n",
    "# it's going to go from the number of cols being the sequence length/ num of rows being batch size\n",
    "# to the number of rows being the max sequence length/ num cols being batch size\n",
    "# Essentially like a padding and then transpose\n",
    "def format_batch(x):\n",
    "    seq_lengths = [len(row) for row in x]\n",
    "    n_batches = len(x)\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    outputs = np.zeros(shape=(max_seq_length, n_batches),dtype=np.int32)\n",
    "    for i in range(len(seq_lengths)):\n",
    "        for j in range(seq_lengths[i]):\n",
    "            outputs[j][i] = x[i][j]\n",
    "    return outputs\n",
    "\n",
    "print format_batch(test_x)\n",
    "print np.array(format_batch(train_t[0:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Very cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "input_embedding_size = 64 # Fixed due to pretrained embedding files\n",
    "encoder_hidden_units = 256\n",
    "decoder_hidden_units = encoder_hidden_units # Must be the same at the moment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we make placeholders for the encoder inputs and decoder targets & inputs which will have the shapes:\n",
    "- `encoder_inputs` int32 tensor is shaped `[encoder_max_time, batch_size]`\n",
    "- `decoder_targets` int32 tensor is shaped `[decoder_max_time, batch_size]`\n",
    "- `decoder_inputs` int32 tensor is shaped `[decoder_max_time, batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "print encoder_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find the embedding vector representations each word in each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_s, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_t, decoder_inputs)\n",
    "print encoder_inputs_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define the encoder RNN using `tf.nn.dynamics_rnn` which allows variable length sequences to fed in, `time_major=True` means that the sequences run over columns rather than rows. We use an LSTM cell to account for long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded,\n",
    "                                                         dtype=tf.float32, time_major=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 256) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 256) dtype=float32>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For now, we use a dynamics RNN in the decoder layer as well - however we could do with defining this manually with `raw_rnn` to allow the decoder outputs of previous timesteps to be fed in as the decoder inputs at current timesteps. The initial decoder state is set to the final encoder state, which is the magic step of the encoder-decoder architecture.\n",
    "\n",
    "- `decoder_outputs` should have shape `[max_steps, batch_size, hidden_dim]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"plain_decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "                                decoder_cell, decoder_inputs_embedded,\n",
    "                                initial_state=encoder_final_state,\n",
    "                                dtype=tf.float32, time_major=True, \n",
    "                                scope=\"plain_decoder\")\n",
    "print decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Projection layer \n",
    "To apply the linear projection layer I need to define weights and biases, then flatten `decoder_outputs` into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack_3:2\", shape=(), dtype=int32)\n",
      "Tensor(\"Reshape_7:0\", shape=(?, ?, 3000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size_t], -0.5, 0.5), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size_t]), dtype=tf.float32)\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "# why do we only flatten the tensor so it's rank 2?\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#feed flattened tensor through projection\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "# make the logits the shape of the \n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size_t))\n",
    "\n",
    "    print decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?, ?), dtype=int64)\n",
      "Tensor(\"Reshape_7:0\", shape=(?, ?, 3000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#decoder_logits_2 = tf.contrib.layers.linear(decoder_outputs, vocab_size_t)\n",
    "#print decoder_logits_2\n",
    "decoder_prediction = tf.argmax(decoder_logits, axis=2)\n",
    "print decoder_prediction\n",
    "#help(tf.argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimisation\n",
    "In order to determine whether the decoder RNN has predicted the next letter correctly, I will use a simple cross-entropy calculation which determines how strongly correlated the two vectors are. In reality the target word will be a simple one-hot encoded word vector. (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_16:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "timestep_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size_t, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "print timestep_cross_entropy\n",
    "# loss is the mean of the cross entropy\n",
    "loss = tf.reduce_mean(timestep_cross_entropy)\n",
    "print loss\n",
    "# We use AdaM which combines AdaGrad (parameters updated less often get updated more strongly)\n",
    "# and momentum (updates depend on the slope of previous updates - avoiding local minima)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[  2  24   9]\n",
      " [124 523  82]\n",
      " [243  23   0]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "decoder predictions:\n",
      "[[1627 2076 1627]\n",
      " [1627 1627 1627]\n",
      " [1627 1627 1627]\n",
      " [1627 1627 1627]]\n"
     ]
    }
   ],
   "source": [
    "# Test format_batch and make sure that the decoder\n",
    "# and encoder accepts inputs with a forward pass\n",
    "\n",
    "batch_ = [[2,124,243], [24,523,23], [9, 82]]\n",
    "\n",
    "batch_ = format_batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_ = format_batch(np.ones(shape=(3, 4), dtype=np.int32))\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_inputs:0\", shape=(?, ?), dtype=int32)\n",
      "[ 844  223  586    2 1860 2396   58 2468 1426 1657    2  113 1026 1534  414\n",
      " 1010    2  812 1308    2  223  292 1699 1309  659 2478    2  512    1    0\n",
      "    0    0    0    0]\n",
      "Reversed as in Sutskever et al. \n",
      "[ 512    2 2478  659 1309 1699  292  223    2 1308  812    2 1010  414 1534\n",
      " 1026  113    2 1657 1426 2468   58 2396 1860    2  586  223  844    1    0\n",
      "    0    0    0    0]\n",
      "Tensor(\"decoder_inputs:0\", shape=(?, ?), dtype=int32)\n",
      "[   1  467  211 2707 2784 2317  685  552 1688 2782 1068 2440  588    2 1098\n",
      "  421 1467    2    2 1740    2  211 1784 2722  552 2911  588  643 2930    0\n",
      "    0    0    0    0    0]\n",
      "Tensor(\"decoder_targets:0\", shape=(?, ?), dtype=int32)\n",
      "[ 467  211 2707 2784 2317  685  552 1688 2782 1068 2440  588    2 1098  421\n",
      " 1467    2    2 1740    2  211 1784 2722  552 2911  588  643 2930    1    0\n",
      "    0    0    0    0    0]\n",
      "Decoder inputs at test time\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def batch_source_target(source, target, batch_size):\n",
    "    assert len(source) == len(target)\n",
    "    for start in range(0, len(source), batch_size):\n",
    "        end = min(start + batch_size, len(source))\n",
    "        #print type(source[start:end])\n",
    "        #print len(target[start:end])\n",
    "        yield source[start:end], target[start:end]     \n",
    "\n",
    "\n",
    "def make_feed_dict(fd_keys, s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_inputs_ = format_batch([[1]+sequence[0:-1] for sequence in t_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def make_test_feed_dict(fd_keys,s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    # At testing time, we can't supervise the decoder layer with\n",
    "    # the 'gold truth' example as input, so we instead feed in\n",
    "    # word generated at  previous timestep. This is (apparently)\n",
    "    # equivalent to feeding in zeros for the decoder inputs\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    decoder_inputs_ = format_batch([[0]*len(sequence) for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test everything is working okay\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for s_sample_batch, t_sample_batch in batch_source_target(train_s[0:2], train_t[0:2], batch_size):\n",
    "    fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "    fd = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch)\n",
    "    fd_r = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= True)\n",
    "    fd_t = make_test_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= False)\n",
    "    assert len(fd.values()[0].T[0]) == len(fd_r.values()[0]) # reversed list must be the same length as original\n",
    "    print fd.keys()[0]\n",
    "    print np.array(fd.values()[0]).T[0]\n",
    "    print \"Reversed as in Sutskever et al. \"\n",
    "    print np.array(fd_r.values()[0]).T[0]\n",
    "    assert len(fd.values()[1].T[0]) == len(fd.values()[1].T[1]) # decoder inputs and targets must be the same\n",
    "    \n",
    "    for i in range(len(fd.keys())-1):\n",
    "        print fd.keys()[i+1]\n",
    "        print np.array(fd.values()[i+1]).T[0]\n",
    "    \n",
    "    print \"Decoder inputs at test time\"\n",
    "    print np.array(fd_t.values()[1]).T[0]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there will be 17 samples in the final batch\n",
      "training has begun...\n",
      "epoch 0\n",
      "batch 53\n",
      "loss: 2.46927428246\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2   2   2   2   2   2   2   2   2   2   2   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [588   2   2   2   2   2   2 588   2   2   2   2   2   2   2   2   2   2\n",
      "   2 588   2 588   2 588   2   2   2   1   2   2   2   2   2   2   2   2\n",
      "   1   2   0   0   0   0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588 588   2   2   2   2   2   2   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2    2    2    2  211    2    2    2    2\n",
      "    2    2 2930    2    2  588    2    2    2    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 1\n",
      "batch 106\n",
      "loss: 2.33920693398\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2   2   2   2 211   2   2   2 211   2 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2  588    2    2    2    2    2    2 2930\n",
      "    2    2    2    2  588    2  588    2  588    2    2    1    1    2    2\n",
      "    2    2    2    2    2    2    1    2    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588    2    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930    2  211  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 2\n",
      "batch 159\n",
      "loss: 2.26724815369\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2 548   2   2   2 211   2   2   2 211 211 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2  588    2    2    2    2    2    2 2440\n",
      "    2    2    2    2  588    2  588    2  588 2930    2    1    1    2    2\n",
      "    2    2    2    2    2    2    1    2    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588    2    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930 2930 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 3\n",
      "batch 212\n",
      "loss: 2.20529389381\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2 548   2   2   2 211   2   2   2 211 211 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2  588    2    2    2    2    2    2 2440\n",
      "    2 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "    2    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588    2    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930 2930 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 4\n",
      "batch 265\n",
      "loss: 2.14418053627\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2 548   2   2   2 211   2   2   2   2   2 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1567  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "    2 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "    2    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588  211    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930 2930 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 5\n",
      "batch 318\n",
      "loss: 2.08552145958\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2    2    2  211    2    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1567  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 6\n",
      "batch 371\n",
      "loss: 2.03075218201\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 7\n",
      "batch 424\n",
      "loss: 1.97993254662\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 8\n",
      "batch 477\n",
      "loss: 1.93114614487\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  588 2198  588    2  588    2  588 2930    2 2930    2    2  211    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2  211 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 9\n",
      "batch 530\n",
      "loss: 1.88868868351\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 1567    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2    2    2    2  211    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 10\n",
      "batch 583\n",
      "loss: 1.83119356632\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198 1717    2  588    2    2    2    2    2    2  211  211    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  211    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 11\n",
      "batch 636\n",
      "loss: 1.77818632126\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2  211    2  211  211    2    2\n",
      "  211    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  211    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 12\n",
      "batch 689\n",
      "loss: 1.72839796543\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  211    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 13\n",
      "batch 742\n",
      "loss: 1.67876577377\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 14\n",
      "batch 795\n",
      "loss: 1.63055193424\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  588 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2 2930 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 15\n",
      "batch 848\n",
      "loss: 1.58262419701\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2    2    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  588 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 16\n",
      "batch 901\n",
      "loss: 1.53509712219\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  588 2198  588    2  588    2  588  211    2  211    2 1722  211 2827    2\n",
      "  548 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 17\n",
      "batch 954\n",
      "loss: 1.487803936\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "    2 2198  588    2  588    2  588  211    2  211    2 1722  211 2954    2\n",
      "  548 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 18\n",
      "batch 1007\n",
      "loss: 1.4385509491\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "    2 2198  588    2  588    2  588  211    2  211    2 1722  211 2954  588\n",
      "  548 1717 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 19\n",
      "batch 1060\n",
      "loss: 1.3912255764\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272    2 1098  588    2  211    2  588    2  588 2954  548    2    2 1098\n",
      "    2 2198  588    2  588    2  588    2    2    2    2 1722  211 2954    2\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "    2  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 20\n",
      "batch 1113\n",
      "loss: 1.34631216526\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272    2 1098  588    2  211    2  588    2  588 2954  548    2    2 1098\n",
      "    2 2198  588    2  588    2  588    2    2    2    2 1722  211 2954    2\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 21\n",
      "batch 1166\n",
      "loss: 1.30122160912\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272    2 1098  588    2  211 1567    2    2  588 2954  548    2    2 1098\n",
      "  699 2198 1717    2  588    2  588  211    2    2    2 1722  211 2954    2\n",
      "  548 1717 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272 2614    2    2    2    2 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 22\n",
      "batch 1219\n",
      "loss: 1.26043343544\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272    2 1098  588  422  211 1098    2    2  588 2954  548    2    2 1098\n",
      " 2272 2198 1717    2  588 2440  588  211    2    2 2272 1722 2930 1717    2\n",
      " 2930 1717 2930 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272 2614    2    2    2    2 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "  588  211  211 2930 2930  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 23\n",
      "batch 1272\n",
      "loss: 1.20832753181\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272 1690 1098  588  422  211 1098  588 1098  588 2954  548    2    2 1098\n",
      "  699 2198 1717    2  588 2440  588  211    2  588    2 1722 1722  588    2\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272 2614    2    2    2    2 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "  588  211  552  552  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 24\n",
      "batch 1325\n",
      "loss: 1.15952539444\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272 1690 1098  588  422  211 1098  588 1098  588 2954  548 1717    2 1098\n",
      " 2784 2198 1717    2 1717 2440  588  211    2  588    2 1722  211 2827    2\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "    2  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 25\n",
      "batch 1378\n",
      "loss: 1.11476588249\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  548  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272 1690 1098  588  422  211 1098    2 1098  548 2954  548 1717    2 1098\n",
      " 2784 2198 1717    2 1717 2440  588  211    2  588    2 1722  211 2827 1913\n",
      "  548 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2 1315 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      " 2384  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 26\n",
      "batch 1431\n",
      "loss: 1.06696856022\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [1193 1690 1098  588  422  211 1098 2762 1098  548 2954  548 1717 1317 1098\n",
      " 2784 2198 1717    2 1717 2440  588  211    2  588    2 1722  211 2827 1913\n",
      "  548 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 549 2614    2    2    2 1315 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2 1861  548    2  588    2    2\n",
      " 2384  211 1561    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 27\n",
      "batch 1484\n",
      "loss: 1.01974534988\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [1193 1690 1098  588  422  211 1098 2762 1098  548 2436  548 1717 1317 1098\n",
      " 2784 2198 1717    2  588 2440  588  211    2  588    2 1722 1722  588 1913\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 549 2198    2    2    2 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2 1861  548    2  588    2    2\n",
      " 2384 1044 1561 1029  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 28\n",
      "batch 1537\n",
      "loss: 0.97433668375\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [1193 1690 1098  588  422  211 1098 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1717 1567  588 2440  588  211    2  588  588 1722 1722  588 1913\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 549 2198    2    2    2 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2 1861  548    2  588    2 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 29\n",
      "batch 1590\n",
      "loss: 0.933820605278\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2636 2877    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [1193 1690 1098  588  422  552 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1717  650  588 2440  588  211    2  588  588 1722 1722 2530 1913\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 549  147    2    2    2 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [1124    2  211  588  588  211  588    2    2 1861  548    2  588    2 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "def format_idx(idx):\n",
    "    # Just cuts out the padding of word index lists\n",
    "    li = []\n",
    "    for i in idx:\n",
    "        if i ==0:\n",
    "            break\n",
    "        else:\n",
    "            li.append(i)\n",
    "    return li\n",
    "\n",
    "BLEU = []\n",
    "epochs = 30 # How many times we loop over the whole training data\n",
    "batch_size = 92 # After how many sequences do we update the weights?\n",
    "print \"there will be {} samples in the final batch\".format(len(train_s)%batch_size)\n",
    "fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "try:\n",
    "    batch_n = 0\n",
    "    print \"training has begun...\"\n",
    "    for epoch in range(epochs):    \n",
    "        for s_batch, t_batch in batch_source_target(train_s, train_t, batch_size):\n",
    "            feed_dict = make_feed_dict(fd_keys, s_batch, t_batch)\n",
    "            _, l = sess.run([train_op, loss], feed_dict)\n",
    "            \n",
    "            #if batch_n == 0 or batch_n == 60:\n",
    "            #    batch_n += 1\n",
    "            batch_n +=1\n",
    "        loss_track.append(l)\n",
    "        print \"epoch {}\".format(epoch+1)\n",
    "        print 'batch {}'.format(batch_n)\n",
    "        print 'loss: {}'.format(sess.run(loss, feed_dict))\n",
    "        predict_ = sess.run(decoder_prediction, feed_dict)\n",
    "        #predictions = [remove_EOS_PAD(pred) for pred in predict_.T]\n",
    "        #actuals = [[remove_EOS_PAD(act)] for act in fd[decoder_targets].T]\n",
    "        #BLEU2 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.5,0.5))\n",
    "        #BLEU.append(BLEU2)\n",
    "        for (inp, act, pred) in zip(feed_dict[encoder_inputs].T,\n",
    "                                                 feed_dict[decoder_targets].T,\n",
    "                                                 predict_.T):\n",
    "            print '  sample {}:'.format(i + 1)\n",
    "            print '    input     > {} \\n {}'.format(format_idx(inp), ' '.join(ids_to_phrases(inp, lang=lang_s)))\n",
    "            #)\n",
    "            print '    actual     > {} \\n {}'.format(format_idx(act), ' '.join(ids_to_phrases(act, lang=lang_s)))\n",
    "            print '    predicted     > {} \\n {}'.format(format_idx(pred), ' '.join(ids_to_phrases(inp, lang=lang_s)))\n",
    "            \n",
    "            \n",
    "    print 'Training is complete'\n",
    "except KeyboardInterrupt:\n",
    "    print 'training interrupted'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(, loss_track)\n",
    "#l = [s for i,s in sorted(zip([len(row) for row in l], l))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x129030050>]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAH/CAYAAADnvTNXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e2dbdd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 255,
       "width": 375
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.plot(range(len(loss_track)), loss_track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing has begun...\n",
      "Testing is complete\n",
      "NMT Corpus BLEU4: 0.0508386153591 \t BLEU2: 0.191773071831 \t BLEU1: 0.362270117695\n",
      "\n",
      "Benchmark BLEU4 : 0.0193589553311 \t BLEU2: 0.0105182907614 \t BLEU1: 0.00310507204007\n",
      "\n",
      "the government <UNK> an order of the senate to explain under what terms and conditions the mexican ambassador in japan signed the <UNK> trade agreement , known by its <UNK> <UNK> , according to the mexican institute of <UNK> property , and the matter has already been <UNK> . \n",
      "the <UNK> of <UNK> <UNK> to <UNK> <UNK> and <UNK> the the <UNK> of <UNK> of <UNK> government , the , to <UNK> of and , <UNK> as the <UNK> <UNK> <UNK> <UNK> to the <UNK> <UNK> , the <UNK> . <UNK> <UNK> <UNK> of been <UNK> <UNK> . \n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions, actuals = [], []\n",
    "batch_size = 120\n",
    "fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "try:\n",
    "    #batch_n = 0\n",
    "    print \"testing has begun...\"  \n",
    "    for s_batch, t_batch in batch_source_target(test_s, test_t, batch_size):\n",
    "        feed_dict = make_feed_dict(fd_keys, s_batch, t_batch)\n",
    "        predict_ = sess.run(decoder_prediction, feed_dict)\n",
    "        for i, (inp, act, pred) in enumerate(zip(feed_dict[encoder_inputs].T,\n",
    "                                                 feed_dict[decoder_targets].T,\n",
    "                                                 predict_.T)):\n",
    "            actuals.append([remove_EOS_PAD(act)])\n",
    "            predictions.append(remove_EOS_PAD(pred))\n",
    "    BLEU4 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    BLEU2 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.5, 0.5))\n",
    "    BLEU1 = bleu_score.corpus_bleu(actuals, predictions, weights=([1]))\n",
    "    print'Testing is complete\\nNMT Corpus BLEU4: {} \\t BLEU2: {} \\t BLEU1: {}\\n'.format(BLEU4, BLEU2, BLEU1)\n",
    "    print \"Benchmark BLEU4 : {} \\t BLEU2: {} \\t BLEU1: {}\\n\".format(BM_BLEU4, BM_BLEU2, BM_BLEU1)\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')\n",
    "\n",
    "\n",
    "print ids_to_phrases(actuals[8][0]+[0],id_to_word_t)\n",
    "print ids_to_phrases(predictions[8]+[0],id_to_word_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Observations:\n",
    "The above code generates a model with an increased BLEU score when it is trained on more data. Now I need to refactor the code so it is more original, efficient and generalisable.\n",
    "- <del>Simplify the generator batch stuff\n",
    "- not happy with tokenization: it leaves out `&apos;` etc.\n",
    "- Go through and change all variable names referring to en and fr to be source and target instead\n",
    "    - HAve to change the benchmark if I want to swap languages\n",
    "- Make it so it's not an interactive session\n",
    "- Sort phrases by length. Create batches of similar length. Randomise batches.\n",
    "- Translated language stats should be similar to target. Compare percent of text covered by vocab words (i.e. is `<UNK>` being overused?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDecommissioned functions\\n\\ndef unknown_word_replacer(tokenized_text, vocab_size, lang=\\'English\\'):\\n    \\'\\'\\' CURRENTLY UNUSED: This is the same as replace_with_word_id below \\n    but without replacing tokens with ids.\\n    We take the list of lists, find FreqDist, replace any\\n    out of vocabulary words with <UNK>, return new list of lists\\'\\'\\'\\n    flat = [item for sublist in tokenized_text for item in sublist]\\n    freq = nltk.FreqDist(flat)\\n    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/len(set(flat))))\\n    vocab = dict(freq.most_common(vocab_size))\\n    text = []\\n    for sentence in tokenized_text:\\n        new_sent = []\\n        for token in sentence:\\n            if token not in vocab.keys():\\n                new_sent.append(u\\'<UNK>\\')\\n            else:\\n                new_sent.append(token)\\n        text.append(new_sent)\\n    return text, vocab\\n\\nimport random\\nfrom PyDictionary import PyDictionary\\ndictionary = PyDictionary()\\n\\nnot_in_dictionary = []\\ndef bm_translate_word(word_list, s_to_t):\\n    global not_in_dictionary\\n    # The closer in length the candidate phrase or word is \\n    # to the actual\\n    keys = []\\n    lens = []\\n    #assert( type(word_list) == list), \"{} {}\".format(type(word_list), len(word_list))\\n    s = random.choice(word_list)\\n    for k in source_to_target.keys():\\n        if s in k:\\n            keys.append(k)\\n            lens.append(len(k))\\n        else:\\n            pass\\n    \\n    try:\\n        best_key = keys[lens.index(min(lens))]\\n        \\n        translations = s_to_t[best_key]\\n        if type(translations) != list:\\n            print best_key\\n            print translations\\n        word = random.choice(translations)\\n        return [word]\\n    except Exception as err:\\n        #print \"{} not in the dictionary\".format(s)\\n        try:\\n            words = dictionary.synonym(s)\\n        except UnicodeEncodeError:\\n            return [s]\\n        if words:\\n            return words\\n        else:\\n            \\n            not_in_dictionary.append(s)\\n            return [\\'<UNK>\\']\\n\\ns1 = \\'children\\'\\n#print bm_translate_word(s1, en_to_fr)\\n\\ndef bm_translate_corpus(s_text, s_to_t):\\n    trans_text = []\\n    for phrase in s_text:\\n        trans_phrase = []\\n        phrase = ids_to_phrases(phrase, id_to_word_s).split(\\' \\')\\n        for word in phrase:\\n            try:\\n                trans_attempt = bm_translate_word([word], s_to_t)\\n            except IndexError:\\n                print [word]\\n            while not trans_attempt:\\n                trans_attempt = bm_translate_word(trans_attempt, s_to_t)\\n            trans_phrase.append(random.choice(trans_attempt))\\n        trans_text.append(trans_phrase)\\n    return trans_text\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Decommissioned functions\n",
    "\n",
    "def unknown_word_replacer(tokenized_text, vocab_size, lang='English'):\n",
    "    ''' CURRENTLY UNUSED: This is the same as replace_with_word_id below \n",
    "    but without replacing tokens with ids.\n",
    "    We take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK>, return new list of lists'''\n",
    "    flat = [item for sublist in tokenized_text for item in sublist]\n",
    "    freq = nltk.FreqDist(flat)\n",
    "    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/len(set(flat))))\n",
    "    vocab = dict(freq.most_common(vocab_size))\n",
    "    text = []\n",
    "    for sentence in tokenized_text:\n",
    "        new_sent = []\n",
    "        for token in sentence:\n",
    "            if token not in vocab.keys():\n",
    "                new_sent.append(u'<UNK>')\n",
    "            else:\n",
    "                new_sent.append(token)\n",
    "        text.append(new_sent)\n",
    "    return text, vocab\n",
    "\n",
    "import random\n",
    "from PyDictionary import PyDictionary\n",
    "dictionary = PyDictionary()\n",
    "\n",
    "not_in_dictionary = []\n",
    "def bm_translate_word(word_list, s_to_t):\n",
    "    global not_in_dictionary\n",
    "    # The closer in length the candidate phrase or word is \n",
    "    # to the actual\n",
    "    keys = []\n",
    "    lens = []\n",
    "    #assert( type(word_list) == list), \"{} {}\".format(type(word_list), len(word_list))\n",
    "    s = random.choice(word_list)\n",
    "    for k in source_to_target.keys():\n",
    "        if s in k:\n",
    "            keys.append(k)\n",
    "            lens.append(len(k))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        best_key = keys[lens.index(min(lens))]\n",
    "        \n",
    "        translations = s_to_t[best_key]\n",
    "        if type(translations) != list:\n",
    "            print best_key\n",
    "            print translations\n",
    "        word = random.choice(translations)\n",
    "        return [word]\n",
    "    except Exception as err:\n",
    "        #print \"{} not in the dictionary\".format(s)\n",
    "        try:\n",
    "            words = dictionary.synonym(s)\n",
    "        except UnicodeEncodeError:\n",
    "            return [s]\n",
    "        if words:\n",
    "            return words\n",
    "        else:\n",
    "            \n",
    "            not_in_dictionary.append(s)\n",
    "            return ['<UNK>']\n",
    "\n",
    "s1 = 'children'\n",
    "#print bm_translate_word(s1, en_to_fr)\n",
    "\n",
    "def bm_translate_corpus(s_text, s_to_t):\n",
    "    trans_text = []\n",
    "    for phrase in s_text:\n",
    "        trans_phrase = []\n",
    "        phrase = ids_to_phrases(phrase, id_to_word_s).split(' ')\n",
    "        for word in phrase:\n",
    "            try:\n",
    "                trans_attempt = bm_translate_word([word], s_to_t)\n",
    "            except IndexError:\n",
    "                print [word]\n",
    "            while not trans_attempt:\n",
    "                trans_attempt = bm_translate_word(trans_attempt, s_to_t)\n",
    "            trans_phrase.append(random.choice(trans_attempt))\n",
    "        trans_text.append(trans_phrase)\n",
    "    return trans_text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-87a21875943c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     equal to the maximum length of sequence (or timesteps)\"\"\"\n\u001b[1;32m     80\u001b[0m source, vocab_s, id_to_word_s = replace_with_word_id(tokenized_source, \n\u001b[0;32m---> 81\u001b[0;31m                                                         vocab_size_s, lang_s)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m target, vocab_t, id_to_word_t = replace_with_word_id(tokenized_target, \n",
      "\u001b[0;32m<ipython-input-23-87a21875943c>\u001b[0m in \u001b[0;36mreplace_with_word_id\u001b[0;34m(tokenized_text, vocab_size, lang)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"{} vocab size restricts to {} percent of total vocab.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mself_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, val)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def seq_length_stats(tokenized_text, lang):\n",
    "    seq_data = dict()\n",
    "    for sublist in tokenized_text:\n",
    "        # Need to keep track of the longest sequence length to pad shorter ones with <EOS>\n",
    "        if len(sublist) not in seq_data.keys():\n",
    "            seq_data[len(sublist)] = 1\n",
    "        else:\n",
    "            seq_data[len(sublist)] += 1\n",
    "    \n",
    "    plt.scatter(seq_data.keys(), seq_data.values())\n",
    "    plt.ylim(0, max(seq_data.values()))\n",
    "    plt.xlim(0, max(seq_data.keys()))\n",
    "    plt.xlabel(\"Sequence length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    titl = 'English'\n",
    "    if lang == 'fr':\n",
    "        titl = 'French'\n",
    "    plt.title(titl)\n",
    "    plt.show()\n",
    "    \n",
    "    return seq_data\n",
    "\n",
    "def replace_with_word_id(tokenized_text, vocab_size, lang):\n",
    "    '''\n",
    "    take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK> whilst giving each\n",
    "    token a numerical ID, return new list of lists'''\n",
    "    flat = []\n",
    "    max_len = 0\n",
    "    for sublist in tokenized_text:\n",
    "        # Need to keep track of the longest sequence length to pad shorter ones with <EOS>\n",
    "        if len(sublist)>max_len:\n",
    "            max_len = len(sublist)\n",
    "        for item in sublist:\n",
    "            flat.append(item)\n",
    "    freq = nltk.FreqDist(flat)\n",
    "    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/len(set(flat))))\n",
    "    vocab = dict(freq.most_common(vocab_size+1))\n",
    "    covered_with_vocab = sum(vocab.values())\n",
    "    del vocab['<EOS>']\n",
    "    # The identities begin at 2, since <EOS>=1 and <UNK>=2\n",
    "    word_to_ids = dict([(word, i+2) for i, word in enumerate(vocab.keys())])\n",
    "    text = []\n",
    "    total_length = 0\n",
    "    for sequence in tokenized_text:\n",
    "        total_length += len(sequence)\n",
    "        # Make sure sequence is no longer than the maximum allowed\n",
    "        ids_sent = []\n",
    "        for token in sequence:\n",
    "            if token not in vocab.keys():\n",
    "                if token == '<EOS>':\n",
    "                    ids_sent.append(1)\n",
    "                else:\n",
    "                    ids_sent.append(2)\n",
    "            else:\n",
    "                ids_sent.append(word_to_ids[token])\n",
    "        text.append(ids_sent)\n",
    "    #freq.plot()\n",
    "    print \"Percentage of whole {} corpus covered by vocab: {}\".format(lang, \n",
    "                                            100*(covered_with_vocab/float(total_length)))\n",
    "    word_to_ids[u'<UNK>'] = 2\n",
    "    word_to_ids[u'<EOS>'] = 1\n",
    "    word_to_ids[u'<PAD>'] = 0\n",
    "    id_to_words = {}\n",
    "    for word, idx in word_to_ids.items():\n",
    "        id_to_words[idx] = word\n",
    "    return text, word_to_ids, id_to_words\n",
    "\n",
    "vocab_size_s = 3000\n",
    "vocab_size_t = 3000\n",
    "\n",
    "#seq_data_source = seq_length_stats(tokenized_source, lang_s)\n",
    "#seq_data_target = seq_length_stats(tokenized_target, lang_t)\n",
    "\n",
    "\"\"\" For now, sequences form rows of the matrix, with the number of columns \n",
    "    equal to the maximum length of sequence (or timesteps)\"\"\"\n",
    "source, vocab_s, id_to_word_s = replace_with_word_id(tokenized_source, \n",
    "                                                        vocab_size_s, lang_s)\n",
    "\n",
    "target, vocab_t, id_to_word_t = replace_with_word_id(tokenized_target, \n",
    "                                                        vocab_size_t, lang_t)\n",
    "\n",
    "\n",
    "train_s, test_s = source[200::], source[0:200]\n",
    "train_t, test_t = target[200::], target[0:200]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
