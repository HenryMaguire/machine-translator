{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The dataset\n",
    "\n",
    "Here I will introduce the dataset, experiment with it, tokenize it, preprocess it and save it to a new set of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C&apos; était le 10 novembre de l&apos; année dernière alors que le technicien de scène Kevin Monk , son épouse enseignante de jardin d&apos; enfants , Roseanna , et Geneviève se préparaient à partir en voyage en famille .\n",
      "Ce changement d&apos; époque est en même temps le problème de toute la démocratie chrétienne , et de ce fait , également celui d&apos; Angela Merker .\n",
      "Obama ? Le premier président anti-américain . Voilà pourquoi .\n",
      "Alors que les communes signalent une chute du nombre de demandes de renouvellement du permis , les autorités dans la capitale sont envahies de demandes et le nombre de demandes de renouvellement du permis est allé croissant cette semaine .\n",
      "Sur l&apos; île , l&apos; utilisation d&apos; Internet est extrêmement limitée .\n",
      "Le passé 4 novembre un dactyloscopiste du CTI de Cali a passé à l&apos; histoire comme la personne qui a confirmé l&apos; identité du chef suprême des Farc : &quot; Alfonso Cano &quot; , en confrontant ses empreintes digitales .\n",
      "Les deux équi\n",
      "Corpora aligned: True\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "f_fr = codecs.open(\"DATA/en-fr_paropt/dev.tok.fr\", encoding='utf-8')\n",
    "f_en = codecs.open(\"DATA/en-fr_paropt/dev.tok.en\", encoding='utf-8')\n",
    "\n",
    "\n",
    "lang_t = 'en'\n",
    "lang_s = 'fr'\n",
    "\n",
    "raw_source = eval(\"f_\"+lang_s).read()\n",
    "raw_target = eval(\"f_\"+lang_t).read()\n",
    "\n",
    "\n",
    "print raw_source[0:1000]\n",
    "print \"Corpora aligned: {}\".format(len(raw_source.split('\\n')) == len(raw_target.split('\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenizing and preprocessing\n",
    "Here we can see that sentences are not really split over lines since \"Here is why ?\" is technically it's own sentence - however, splitting these cases up will lead to misalignment between the english and french corpora so it's too risky. What I will do then is just tokenize each line, remove capitalisation and then replace the final punctuation with `<EOS` and unknown characters with `<UNK>` or `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fix a small pathology with the datasets, whereby spaces are put between words\n",
    "# and end of sentence tokens\n",
    "def format_eos(text):\n",
    "    #text = text.replace(' .', '.')\n",
    "    #text = text.replace('?')\n",
    "    #text = text.replace('!', '.')\n",
    "    #text = text.replace(\"idn &apos;t\", 'did not')\n",
    "    #text = text.replace(\"houldn &apos;t\", 'should not')\n",
    "    #text = text.replace(\"t &apos;s\", 'it is')\n",
    "    #text = text.replace('%', 'percent')\n",
    "    return text\n",
    "\n",
    "#text_fr = format_eos(text_fr)\n",
    "#text_en = format_eos(text_en)\n",
    "#print \"still aligned:\", len(raw_source.split('\\n')) == len(raw_target.split('\\n'))\n",
    "#print raw_source[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorsque vous élaborez un business plan dans une entreprise , vous avez aussi un certain but optimal et une vision .\n",
      "When you make a company business plan , you also have some ideal goal and vision .\n",
      "------------\n",
      "Je ne suis pas un billet de cent dollars pour plaire à tout le monde .\n",
      "I am not a hundred dollar bill to please all .\n",
      "------------\n",
      " Judging by keywords and rudimentary knowledge of French, I suspect the corpora are still aligned.\n",
      "corpora still same length: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk.data\n",
    "import random\n",
    "# First split up into lines using the Punkt tokenizer\n",
    "#fr_sent_detector = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "#en_sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#fr_sents = fr_sent_detector.tokenize(text_fr)\n",
    "#en_sents = en_sent_detector.tokenize(text_en)\n",
    "\n",
    "# Split the text up into lines\n",
    "# Randomise the lists but maintain parallel ordering\n",
    "s = zip(raw_source.split('\\n'), raw_target.split('\\n'))\n",
    "np.random.shuffle(s)\n",
    "source_phrases, target_phrases = zip(*s)\n",
    "\n",
    "print \"\\n\".join([source_phrases[0],target_phrases[0]])\n",
    "print \"------------\"\n",
    "print \"\\n\".join([source_phrases[302],target_phrases[302]])\n",
    "print \"------------\\n Judging by keywords and rudimentary knowledge of French, I suspect the corpora are still aligned.\"\n",
    "print \"corpora still same length:\", len(source_phrases)== len(target_phrases), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'lorsque', u'vous', u'\\xe9laborez', u'un', u'business', u'plan', u'dans', u'une', u'entreprise', u',', u'vous', u'avez', u'aussi', u'un', u'certain', u'but', u'optimal', u'et', u'une', u'vision', u'.', u'<EOS>'], [u'ce', u'13\\xe8me', u'essai', u'se', u'd\\xe9roulait', u'au', u'd\\xe9but', u'normalement', u'.', u'<EOS>'], [u'de', u'm\\xeame', u',', u'les', u'demandeurs', u'de', u'visa', u'aux', u'consulats', u'de', u'ciudad', u'ju\\xe1rez', u',', u'monterrey', u'et', u'nuevo', u'laredo', u'ne', u'paieront', u'plus', u'le', u'tarif', u'extra', u'de', u'service', u'de', u'26', u'dollars', u'.', u'<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_sentences(text):\n",
    "    new_text = []\n",
    "    for sentence in text:\n",
    "        # This splits up tokens within a sentence\n",
    "        tok_sent = (sentence.lower()).split(' ')\n",
    "        # I'm keeping the final punctuation and appending the \n",
    "        # <EOS> tag after it\n",
    "        new_text.append(tok_sent+[u'<EOS>'])\n",
    "    return new_text\n",
    "print tokenize_sentences(source_phrases[0:3])\n",
    "tokenized_source = tokenize_sentences(source_phrases)\n",
    "tokenized_target = tokenize_sentences(target_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vocabularies, Sequences and Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I need to:\n",
    "- Find the frequencies of words, limiting the vocabulary size to some number of the most common words and replacing out of vocabulary words with `<UNK>`\n",
    "- Give each token an ID, where `<EOS>` =0, `<UNK>` =1 and the remaining tokens are assigned numbers from 2 to vocab_size+1 at random\n",
    "- Make each sequence the same length by padding the end of sequences with zeros until their length matches the longest sequence. Given the long-tailed distribution of sequence length and the limited capacity of LSTMs we may be able to truncate max sequence length substantially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr vocab size restricts to 18.4717689797 percent of total vocab.\n",
      "Percentage of whole fr corpus covered by vocab: 85.4388440314\n",
      "en vocab size restricts to 22.0038139944 percent of total vocab.\n",
      "Percentage of whole en corpus covered by vocab: 86.7741159091\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def seq_length_stats(tokenized_text, lang):\n",
    "    seq_data = dict()\n",
    "    for sublist in tokenized_text:\n",
    "        # Need to keep track of the longest sequence length to pad shorter ones with <EOS>\n",
    "        if len(sublist) not in seq_data.keys():\n",
    "            seq_data[len(sublist)] = 1\n",
    "        else:\n",
    "            seq_data[len(sublist)] += 1\n",
    "    \n",
    "    plt.scatter(seq_data.keys(), seq_data.values())\n",
    "    plt.ylim(0, max(seq_data.values()))\n",
    "    plt.xlim(0, max(seq_data.keys()))\n",
    "    plt.xlabel(\"Sequence length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    titl = 'English'\n",
    "    if lang == 'fr':\n",
    "        titl = 'French'\n",
    "    plt.title(titl)\n",
    "    plt.show()\n",
    "    \n",
    "    return seq_data\n",
    "\n",
    "def replace_with_word_id(tokenized_text, vocab_size, lang):\n",
    "    '''\n",
    "    take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK> whilst giving each\n",
    "    token a numerical ID, return new list of lists'''\n",
    "    flat = []\n",
    "    max_len = 0\n",
    "    for sublist in tokenized_text:\n",
    "        # Need to keep track of the longest sequence length to pad shorter ones with <EOS>\n",
    "        if len(sublist)>max_len:\n",
    "            max_len = len(sublist)\n",
    "        for item in sublist:\n",
    "            flat.append(item)\n",
    "    freq = nltk.FreqDist(flat)\n",
    "    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/len(set(flat))))\n",
    "    vocab = dict(freq.most_common(vocab_size+1))\n",
    "    covered_with_vocab = sum(vocab.values())\n",
    "    del vocab['<EOS>']\n",
    "    # The identities begin at 2, since <EOS>=1 and <UNK>=2\n",
    "    word_to_ids = dict([(word, i+2) for i, word in enumerate(vocab.keys())])\n",
    "    text = []\n",
    "    total_length = 0\n",
    "    for sequence in tokenized_text:\n",
    "        total_length += len(sequence)\n",
    "        # Make sure sequence is no longer than the maximum allowed\n",
    "        ids_sent = []\n",
    "        for token in sequence:\n",
    "            if token not in vocab.keys():\n",
    "                if token == '<EOS>':\n",
    "                    ids_sent.append(1)\n",
    "                else:\n",
    "                    ids_sent.append(2)\n",
    "            else:\n",
    "                ids_sent.append(word_to_ids[token])\n",
    "        text.append(ids_sent)\n",
    "    #freq.plot()\n",
    "    print \"Percentage of whole {} corpus covered by vocab: {}\".format(lang, \n",
    "                                            100*(covered_with_vocab/float(total_length)))\n",
    "    word_to_ids[u'<UNK>'] = 2\n",
    "    word_to_ids[u'<EOS>'] = 1\n",
    "    word_to_ids[u'<PAD>'] = 0\n",
    "    id_to_words = {}\n",
    "    for word, idx in word_to_ids.items():\n",
    "        id_to_words[idx] = word\n",
    "    return text, word_to_ids, id_to_words\n",
    "\n",
    "vocab_size_s = 3000\n",
    "vocab_size_t = 3000\n",
    "\n",
    "#seq_data_source = seq_length_stats(tokenized_source, lang_s)\n",
    "#seq_data_target = seq_length_stats(tokenized_target, lang_t)\n",
    "\n",
    "\"\"\" For now, sequences form rows of the matrix, with the number of columns \n",
    "    equal to the maximum length of sequence (or timesteps)\"\"\"\n",
    "source, vocab_s, id_to_word_s = replace_with_word_id(tokenized_source, \n",
    "                                                        vocab_size_s, lang_s)\n",
    "\n",
    "target, vocab_t, id_to_word_t = replace_with_word_id(tokenized_target, \n",
    "                                                        vocab_size_t, lang_t)\n",
    "\n",
    "train_s, test_s = source[200::], source[0:200]\n",
    "train_t, test_t = target[200::], target[0:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpora still aligned: True\n",
      "[u'<PAD>', u'<EOS>', u'<UNK>', u'forc\\xe9', u'd\\xe9j\\xe0', u'consiste', u'chaos', u'veut', u'travaux', u'aider', u'limiter', u'boh\\xe8me', u'veux', u's\\xe9curit\\xe9', u'internes', u'utilisateur', u'voter', u'devenir', u'paris', u'l\\xe9g\\xe8rement']\n"
     ]
    }
   ],
   "source": [
    "print \"Corpora still aligned:\", len(train_s) == len(train_t)\n",
    "print id_to_word_s.values()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "victimes climatique crises veux solution \n",
      "represents bad visible votes heading \n"
     ]
    }
   ],
   "source": [
    "def ids_to_phrases(idx_list, id_to_word):\n",
    "    # Takes list of word ids and returns a string of words\n",
    "    # Mainly for use in analysis\n",
    "    phrase = ''\n",
    "    id_dict = id_to_word\n",
    "    i=0\n",
    "    while idx_list[i] not in (1,0):\n",
    "        phrase+= id_dict[idx_list[i]]+' '\n",
    "        i+=1\n",
    "    return phrase\n",
    "# Test the functionality\n",
    "print ids_to_phrases([234, 432, 102, 12,43,1], id_to_word_s)\n",
    "print ids_to_phrases([234, 432, 102, 12,43,1], id_to_word_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "I am going to use the N-gram BLEU score as my evaluation metric, changing the parameter N to see the effect it has on my model, relative to the benchmark.\n",
    "\n",
    "## Implementing the BLEU metric\n",
    "This metric will be used to test how good a prediction actually was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1 score test is 0.282160574964.\n",
      "BLEU2 score test is 0.218560641558.\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import bleu_score\n",
    "\n",
    "def nonunique_ngrams(phrase, N):\n",
    "        \n",
    "        N_grams = {}\n",
    "        for i in range(len(phrase)):\n",
    "            li = phrase[i:i+N]\n",
    "            ng = ' '.join([str(s) for s in li])\n",
    "            if len(li) == N or len(phrase)<N:\n",
    "                try:\n",
    "                    N_grams[ng] += 1\n",
    "                except KeyError:\n",
    "                    N_grams[ng] = 1\n",
    "        return N_grams, len(phrase)\n",
    "\n",
    "def remove_EOS_PAD(long_phrase):\n",
    "    i=0\n",
    "    phrase= []\n",
    "    while (long_phrase+[0])[i] not in (0,1):\n",
    "        phrase.append(long_phrase[i])\n",
    "        i+=1\n",
    "    return phrase\n",
    "\n",
    "def BLEU_metric(long_t_phrase, long_p_phrase, N):\n",
    "    # a) Find all (non-unique) N grams in target and predicted phrase and frequencies\n",
    "    # Firstly need to see how long the content is (not <EOS> or <PAD>)\n",
    "    t_phrase = remove_EOS_PAD(long_t_phrase)\n",
    "    p_phrase = remove_EOS_PAD(long_p_phrase)\n",
    "    N = min(N, len(p_phrase), len(t_phrase))\n",
    "    t_ngrams, t_len = nonunique_ngrams(t_phrase, N)\n",
    "    p_ngrams, p_len = nonunique_ngrams(p_phrase, N)\n",
    "    \n",
    "    #print \"N-gram count is {}\".format(p_ngrams)\n",
    "    p_num = sum(p_ngrams.values())\n",
    "    \n",
    "    #print p_num\n",
    "    # b) How many of the N-grams in the prediction appear in the target + frequencies\n",
    "    # d) Limit the number of correct counts of an Ngram to \n",
    "    #    the number of times it appears in the target\n",
    "    cross_count = []\n",
    "    for ng in p_ngrams.keys():\n",
    "        try:\n",
    "            cross_count.append(min((t_ngrams[ng], p_ngrams[ng])))\n",
    "        except KeyError:\n",
    "            cross_count.append(0)\n",
    "    # e) return the above number divided by the total number of (non-unique) N-grams\n",
    "    # I take the log of the BLEU scores so I can sum them \n",
    "    # and exponentiate to calculate the product (for geometric mean later on)\n",
    "    #print float(p_num)\n",
    "    return [np.log(sum(cross_count)/float(p_num)), t_len, p_len]\n",
    "\n",
    "# Test handwritten BLEU\n",
    "p_phrase1 = [4,5,4,5,4,5, 1, 0]\n",
    "t_phrase = [4,5,6,34,8,76, 87, 1]\n",
    "assert len(t_phrase) == len(p_phrase1)\n",
    "t_phrase = remove_EOS_PAD(t_phrase)\n",
    "p_phrase1 = remove_EOS_PAD(p_phrase1)\n",
    "\n",
    "print \"BLEU1 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [remove_EOS_PAD(p_phrase1)], weights=([1])))\n",
    "print \"BLEU2 score test is {}.\".format(\n",
    "    bleu_score.corpus_bleu([[t_phrase]], [remove_EOS_PAD(p_phrase1)], weights=([0.5,0.5])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Benchmark Model\n",
    "\n",
    "Here I will implement the dictionary based translation benchmark using a free English-to-French dictionary [text file](http://ktranslator.sourceforge.net/dictionaries.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'grossier', [u'crude', u'raw', u'rough', u'coarse', u'crude', u'harsh']), (u'bouddhisme', [u'buddhism']), (u'four', [u'furnace', u'kiln', u'oven', u'stove']), (u's\\xe9curit\\xe9', [u'safety', u'security']), (u'\\xe0 cause de rien', [u'for no reason']), (u'yougoslavie', [u'yugoslavia']), (u'lors', [u'for', u'during', u'whereas', u'while', u'whilst', u'on the occasion of']), (u'coucou', [u'cuckoo']), (u'albumen', [u'albumen']), (u'balise', [u'buoy'])]\n",
      "7673\n"
     ]
    }
   ],
   "source": [
    "filename = ''\n",
    "if lang_t == 'en':\n",
    "    filename = \"DATA/freedict-fra-eng.dic\"\n",
    "else:\n",
    "    filename = \"DATA/freedict-eng-fra.dic\"\n",
    "\n",
    "f = open(filename,'rb')\n",
    "txt = unicode(f.read(), 'utf-8')\n",
    "\n",
    "source_to_target = {}\n",
    "for line in txt.split('\\n'):\n",
    "    words = line.split('\\t')\n",
    "    try:\n",
    "        source, targets = words[0], words[1].split('; ')\n",
    "        source_to_target[source.lower()] = (', '.join(targets).lower()).split(', ')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "source_to_target.update({u'<EOS>': [u'<EOS>'], u'<PAD>': [u'<PAD>'], u'<UNK>':[u'<UNK>'], u'&quot;':[u'&quot;'], \n",
    "                 u'&apos;':[u'&apos;'], u'(':[u'('], u')':[u')'], u':':[u':'], u'%':[u'%'], u'$':[u'$']})\n",
    "\n",
    "print source_to_target.items()[0:10]\n",
    "print len(source_to_target.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some words may not have direct translations in the dictionary but may be phrasal, so rather than look up words directly like `source_to_target[source_word]` I am going to find out whether the source word is contained within any of the keys in the dictionary, then use the shortest of those keys as the chosen source phrase/word. When translating the document, if a word is come across that is not in the dictionary I'll try to find a synonym, if that fails then just return `<UNK>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON object could be decoded for phrase 41\n",
      "No JSON object could be decoded for phrase 81\n",
      "No JSON object could be decoded for phrase 129\n",
      "No JSON object could be decoded for phrase 176\n",
      "There are 4 phrases which could not be translated first time around.\n",
      "There are 0 phrases which could not be translated in pass 1.\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def google_translate_corpus(s_text, s_to_t, target_lang='fr'):\n",
    "    translator = Translator()\n",
    "    trans_corpus = []\n",
    "    skipped_phrases = []\n",
    "    for i, phrase in enumerate(s_text):\n",
    "        trans_phrase = []\n",
    "        # Determine if the corpus is ids or words\n",
    "        if type(phrase[0]) is int:\n",
    "            phrase = ids_to_phrases(phrase, id_to_word_s).split(' ')\n",
    "        try:\n",
    "            trans_corpus.append([trans.text for \n",
    "                                 trans in translator.translate(phrase, dest=target_lang)])\n",
    "        except ValueError as err:\n",
    "            # Making a new Translator instance seems to help JSON errors\n",
    "            translator = Translator()\n",
    "            skipped_phrases.append(phrase)\n",
    "            print \"{} for phrase {}\".format(err, i)\n",
    "    return trans_corpus, skipped_phrases\n",
    "\n",
    "tc, skipped_phrases = google_translate_corpus(test_s, source_to_target, target_lang=lang_t)\n",
    "print \"There are {} phrases which could not be translated first time around.\".format(\n",
    "                                                    len(skipped_phrases))\n",
    "i=2\n",
    "while len(skipped_phrases)>0:\n",
    "    tc_s, skipped_phrases = google_translate_corpus(skipped_phrases, source_to_target, target_lang=lang_t)\n",
    "    tc+=tc_s\n",
    "    print \"There are {} phrases which could not be translated in pass {}.\".format(\n",
    "                                                        len(skipped_phrases), i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora probably still aligned: True\n",
      "[u'-', u'and', u'when', u'<UNK>', u'including', u'what', u'you', u'<UNK>', u'be', u'painter', u'?', u'']\n"
     ]
    }
   ],
   "source": [
    "#tc = [(' '.join(phr)).split(' ') for phr in tc]\n",
    "\n",
    "print \"corpora probably still aligned: {}\".format(len(tc) == len(test_t))\n",
    "print tc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: \n",
      "to have some change from <UNK> , and gain some practical experience , we learned how to <UNK> <UNK> <UNK> <UNK> . \n",
      "Prediction: \n",
      "for what we <UNK> not only of the people who <UNK> more who have tried some chose from way convenient they we have <UNK> comment <UNK> correctly of the shoes . \n",
      "Unigram BLEU score is 0.00310507204007.\n",
      "Bigram BLEU score is 0.0105182907614.\n"
     ]
    }
   ],
   "source": [
    "test_targets = [[ids_to_phrases(phrase,id_to_word_t)]for phrase in test_t]\n",
    "print \"Actual: \\n\", (\" \".join(test_targets[10])).encode('utf-8')\n",
    "print \"Prediction: \\n\", (\" \".join(tc[10])).encode('utf-8')\n",
    "print \"Unigram BLEU score is {}.\".format(bleu_score.corpus_bleu(\n",
    "    test_targets, tc, weights=([1])))\n",
    "print \"Bigram BLEU score is {}.\".format(bleu_score.corpus_bleu(\n",
    "    test_targets, tc, weights=(0.5,0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word IDs and word-to-vec vectors\n",
    "\n",
    "Since we are interested in the process of learning weights within the RNNs to predict seq2seq mappings rather than embeddings I have chosen to use pretrained word embeddings ino order to cut down training time. [THis is](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Download-the-Embeddings) where the embeddings are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding matrix for fr has 3002 columns and 64 rows.\n",
      "199 vocab words were not in the fr embeddings file.\n",
      "The embedding matrix for en has 3002 columns and 64 rows.\n",
      "134 vocab words were not in the en embeddings file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "def get_embeddings(id_to_word, lang):\n",
    "    # We load pretrained word2vec embeddings from polyglot to save on training time\n",
    "    filename ='DATA/polyglot-'+lang+'.pkl'\n",
    "    pretrain_vocab, pretrain_embed = pickle.load(open(filename, 'rb'))\n",
    "    embed_vocab = [pretrain_embed[pretrain_vocab.index('<PAD>')], pretrain_embed[pretrain_vocab.index('</S>')]]\n",
    "    skip_count = 0\n",
    "    skipped_words = []\n",
    "    for idx, word in sorted(id_to_word.items()[2::]):\n",
    "        try:\n",
    "            pretrain_idx = pretrain_vocab.index(word)\n",
    "            embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # it could be that the word is a name which needs to \n",
    "                # be capitalized. Try this...\n",
    "                pretrain_idx = pretrain_vocab.index(str(word.title()))\n",
    "                embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # it could be that the word is an achronym which needs to \n",
    "                    # be upper case. Try this...\n",
    "                    pretrain_idx = pretrain_vocab.index(word.upper())\n",
    "                    embed_vocab.append(pretrain_embed[pretrain_idx])\n",
    "                except ValueError:\n",
    "                    # Give up trying to find an embedding.\n",
    "                    # How many words are skipped? Which ones?\n",
    "                    skip_count +=1\n",
    "                    skipped_words.append(word)\n",
    "                    # Let's just initialise the embedding to a random normal distribution\n",
    "                    embed_vocab.append(np.random.normal(loc=0.0, scale=np.sqrt(2)/4, size=64))\n",
    "    embed_vocab = np.array(embed_vocab, dtype=np.float32)\n",
    "    print \"The embedding matrix for {} has {} columns and {} rows.\".format(lang, \n",
    "                                                embed_vocab.shape[0], embed_vocab.shape[1])\n",
    "    print \"{} vocab words were not in the {} embeddings file.\".format(skip_count, lang)\n",
    "    return embed_vocab, skipped_words\n",
    "# the ith word in words corresponds to the ith embedding \n",
    "\n",
    "embed_vocab_s, skipped_s = get_embeddings(id_to_word_s, lang=lang_s)\n",
    "embed_vocab_t, skipped_t = get_embeddings(id_to_word_t, lang=lang_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Note: to find a word given an index we use `id_to_word_en` and vice-versa we use `vocab_en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "[u'talysh', u'100', u'90', u'95', u'1990', u'1998', u'fatulayeva', u'400', u'isn', u'wasn', u'bitcoin', u'victor-gadbois', u'13', u'12', u'esitjeni', u'vice-president', u'1973', u'&#93;', u'50', u'bitcoins', u'1995', u'spijkenisse', u'pelta', u'anti-corruption', u'high-speed', u'1', u'lerik', u'svarc', u'1956', u'&#91;', u'rajchl', u'2', u'11', u'10', u'15', u'14', u'17', u'16', u'19', u'18', u'mursi', u'200', u'30,000', u'mincy', u'mccain', u'no-one', u'&apos;re', u'60', u'anto', u'500', u'unasur', u'&apos;ll', u'freudenberg', u'&apos;arenberg', u'2000', u'2001', u'5', u'mexican-americans', u'--', u'vaqueros', u'20th', u'21st', u'6', u'pattloch', u'24', u'25', u'26', u'20', u'21', u'22', u'23', u'wouldn', u'shekels', u'7', u'&apos;', u'77', u'70', u'kawah', u'19th', u'8', u'left-wing', u'&apos;d', u'&apos;m', u'&apos;t', u'&apos;s', u'mcdonald', u'long-term', u'ijen', u'34', u'9', u'300', u'75', u'doesn', u'&quot;', u'jedlicka', u'2006', u'2007', u'2004', u'2005', u'2008', u'2009', u'ods', u'gausse', u'morsi', u'restall', u'31', u'30', u'37', u'35', u'baranets', u'1991', u'rakfisk', u'2011', u'2010', u'2013', u'2012', u'nku', u'700', u'80', u'&apos;ve', u'1.5', u'so-called', u'120', u'pacl\\xedk', u'1994', u'i.e.', u'mirzayeva', u'2014', u'46', u'40', u'wentzler', u'didn', u'audatex', u'1988']\n"
     ]
    }
   ],
   "source": [
    "print vocab_t['<PAD>'], vocab_t['<EOS>'], vocab_t['<UNK>']\n",
    "print skipped_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** We can see above ** that the English words which were not in the embedding files are fairly specialist words or numerical values (which are the same in French) so hopefully they won't be too much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1265, 904, 2, 2228, 2, 1893, 1860, 1426, 1622, 223, 904, 1832, 1870, 2228, 1487, 2941, 2, 1035, 1426, 1191, 512, 1], [2749, 2, 1562, 977, 2, 2785, 2525, 124, 512, 1], [414, 92, 223, 2363, 2, 414, 2642, 23, 2, 414, 2, 2, 223, 2, 1035, 2, 2, 767, 2, 700, 2475, 2, 2, 414, 1800, 414, 1726, 697, 512, 1], [1035, 2475, 2025, 2, 2066, 1080, 223, 763, 23, 2, 2, 1308, 2396, 268, 2, 512, 1], [2239, 1393, 487, 2, 1237, 2363, 438, 223, 1694, 1035, 1701, 223, 2601, 1035, 2918, 223, 2, 1035, 2, 1597, 1415, 1585, 2478, 997, 1433, 2475, 2, 1426, 2097, 2785, 2404, 2664, 223, 216, 1393, 559, 767, 2053, 764, 2469, 764, 1435, 1451, 113, 767, 1757, 1309, 2475, 1000, 414, 876, 2, 2, 1433, 2475, 1297, 414, 1010, 1000, 512, 1], [1026, 2749, 1648, 223, 559, 1699, 2798, 2475, 1555, 2139, 1181, 1426, 285, 2, 2, 1308, 2363, 707, 2659, 414, 2396, 1296, 512, 1], [1860, 1426, 2869, 414, 2, 223, 2363, 1509, 2, 767, 1586, 700, 1755, 2478, 2833, 2084, 2475, 1182, 1768, 1035, 2475, 1182, 2909, 223, 2243, 977, 1758, 1860, 2228, 379, 2139, 2, 2, 223, 113, 1924, 1393, 2243, 1586, 1628, 2139, 2, 223, 414, 2, 223, 414, 2, 2, 1433, 414, 2, 512, 1]]\n"
     ]
    }
   ],
   "source": [
    "print test_s[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see a couple of things:\n",
    "- The list of lists above does not have consistent lengths of rows (it's not a matrix)\n",
    "- In order to process large amounts of data we need to break data up into batches of sequences\n",
    "\n",
    "The format that I need for the seq-to-seq model is a matrix - we do this by padding shorter sequences in a batch with the `<PAD>` token (represented already as the 0th column of the embedding matrix). the of dimension `(max sequence length in batch, batch size)`, so sequences are represented as the columns of the input matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 2 4 1]\n",
      " [2 0 2 2]\n",
      " [3 0 0 0]]\n",
      "[[ 467  588 2272  995    2 1957 1124]\n",
      " [ 211   47  588    2 1124 1148  588]\n",
      " [2707 2440    2 1112    2  548    2]\n",
      " [2784  100 2112 1505 1910    2    2]\n",
      " [2317  211  548  588 1567    2 1237]\n",
      " [ 685  588 1001 1823    2 2930  588]\n",
      " [ 552    2  588 2440  214    2 1042]\n",
      " [1688 1618    2  588 1505  211    2]\n",
      " [2782 1098 2096 1786    2 1957    2]\n",
      " [1068 2576 2272  211  211  700 1717]\n",
      " [2440    2  821  365 2629    2 1502]\n",
      " [ 588 2765    2  606 1103    2    2]\n",
      " [   2 1807    2 2720 1194  627 1267]\n",
      " [1098 2303    2    2 1068  548 2684]\n",
      " [ 421  211  211 1098 2649 2636 2713]\n",
      " [1467 2784 2636  549 2436    2 2908]\n",
      " [   2  914 2576  110 1023 2558  588]\n",
      " [   2 1567  592 1074  211 2614 1042]\n",
      " [1740 2614  544 2795 1124 2701 2022]\n",
      " [   2  348    2  588 1650 2257 2162]\n",
      " [ 211  588  211  676  767 2782  192]\n",
      " [1784 2428 1717 1896 1503  211 2359]\n",
      " [2722 1567 1387 2280 2788  606 2930]\n",
      " [ 552 2181  157  650    2    2    1]\n",
      " [2911 2824  665    2 2440 1124    0]\n",
      " [ 588  211 1670 1340  545  588    0]\n",
      " [ 643   56 1098 2222 1455 1509    0]\n",
      " [2930  362 1990  588 2639 1445    0]\n",
      " [   1 2765  588 2455    2 1001    0]\n",
      " [   0 2576  313  332 1098  508    0]\n",
      " [   0 1659  260 2930 2784  962    0]\n",
      " [   0 1717 2930    1 1251 1874    0]\n",
      " [   0    2    1    0 2541 1567    0]\n",
      " [   0 2930    0    0 1541 1970    0]\n",
      " [   0    1    0    0  552 1112    0]\n",
      " [   0    0    0    0 2505 1717    0]\n",
      " [   0    0    0    0 2930  954    0]\n",
      " [   0    0    0    0    1   11    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0  548    0]\n",
      " [   0    0    0    0    0 1071    0]\n",
      " [   0    0    0    0    0 1597    0]\n",
      " [   0    0    0    0    0  211    0]\n",
      " [   0    0    0    0    0 2946    0]\n",
      " [   0    0    0    0    0 2636    0]\n",
      " [   0    0    0    0    0 2576    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0 2440    0]\n",
      " [   0    0    0    0    0 1957    0]\n",
      " [   0    0    0    0    0 1304    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0   56    0]\n",
      " [   0    0    0    0    0 2782    0]\n",
      " [   0    0    0    0    0 1717    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0    2    0]\n",
      " [   0    0    0    0    0 2930    0]\n",
      " [   0    0    0    0    0    1    0]]\n"
     ]
    }
   ],
   "source": [
    "test_x = [[5,2,3],[2], [4,2], [1,2]]\n",
    "# it's going to go from the number of cols being the sequence length/ num of rows being batch size\n",
    "# to the number of rows being the max sequence length/ num cols being batch size\n",
    "# Essentially like a padding and then transpose\n",
    "def format_batch(x):\n",
    "    seq_lengths = [len(row) for row in x]\n",
    "    n_batches = len(x)\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    outputs = np.zeros(shape=(max_seq_length, n_batches),dtype=np.int32)\n",
    "    for i in range(len(seq_lengths)):\n",
    "        for j in range(seq_lengths[i]):\n",
    "            outputs[j][i] = x[i][j]\n",
    "    return outputs\n",
    "\n",
    "print format_batch(test_x)\n",
    "print np.array(format_batch(train_t[0:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Very cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "input_embedding_size = 64 # Fixed due to pretrained embedding files\n",
    "encoder_hidden_units = 200\n",
    "decoder_hidden_units = encoder_hidden_units # Must be the same at the moment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we make placeholders for the encoder inputs and decoder targets & inputs which will have the shapes:\n",
    "- `encoder_inputs` int32 tensor is shaped `[encoder_max_time, batch_size]`\n",
    "- `decoder_targets` int32 tensor is shaped `[decoder_max_time, batch_size]`\n",
    "- `decoder_inputs` int32 tensor is shaped `[decoder_max_time, batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find the embedding representations for the vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_s, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embed_vocab_t, decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define the encoder RNN using `tf.nn.dynamics_rnn` which allows variable length sequences to fed in, `time_major=True` means that the sequences run over columns rather than rows. We use an LSTM cell to account for long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded,\n",
    "                                                         dtype=tf.float32, time_major=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 200) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 200) dtype=float32>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For now, we use a dynamics RNN in the decoder layer as well - however we could do with defining this manually with `raw_rnn` to allow the decoder outputs of previous timesteps to be fed in as the decoder inputs at current timesteps. The initial decoder state is set to the final encoder state, which is the magic step of the encoder-decoder architecture.\n",
    "\n",
    "- `decoder_outputs` should have shape `[max_steps, batch_size, hidden_dim]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"plain_decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "                                decoder_cell, decoder_inputs_embedded,\n",
    "                                initial_state=encoder_final_state,\n",
    "                                dtype=tf.float32, time_major=True, \n",
    "                                scope=\"plain_decoder\")\n",
    "print decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Projection layer \n",
    "To apply the linear projection layer I need to define weights and biases, then flatten `decoder_outputs` into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(?, ?, 3000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size_t], -0.5, 0.5), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size_t]), dtype=tf.float32)\n",
    "\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#feed flattened tensor through projection\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size_t))\n",
    "\n",
    "print decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?, ?), dtype=int64)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, ?, 3000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#decoder_logits_2 = tf.contrib.layers.linear(decoder_outputs, vocab_size_t)\n",
    "#print decoder_logits_2\n",
    "decoder_prediction = tf.argmax(decoder_logits, axis=2)\n",
    "print decoder_prediction\n",
    "print decoder_logits\n",
    "#help(tf.argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimisation\n",
    "In order to determine whether the decoder RNN has predicted the next letter correctly, I will use a simple cross-entropy calculation which determines how strongly correlated the two vectors are. In reality the target word will be a simple one-hot encoded word vector. (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "timestep_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size_t, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "# loss is the mean of the cross entropy\n",
    "loss = tf.reduce_mean(timestep_cross_entropy)\n",
    "# We use AdaM which combines AdaGrad (parameters updated less often get updated more strongly)\n",
    "# and momentum (updates depend on the slope of previous updates - avoiding local minima)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[  2  24   9]\n",
      " [124 523  82]\n",
      " [243  23   0]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "decoder predictions:\n",
      "[[1627 2076 1627]\n",
      " [1627 1627 1627]\n",
      " [1627 1627 1627]\n",
      " [1627 1627 1627]]\n"
     ]
    }
   ],
   "source": [
    "# Test format_batch and make sure that the decoder\n",
    "# and encoder accepts inputs with a forward pass\n",
    "\n",
    "batch_ = [[2,124,243], [24,523,23], [9, 82]]\n",
    "\n",
    "batch_ = format_batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_ = format_batch(np.ones(shape=(3, 4), dtype=np.int32))\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_inputs:0\", shape=(?, ?), dtype=int32)\n",
      "[ 844  223  586    2 1860 2396   58 2468 1426 1657    2  113 1026 1534  414\n",
      " 1010    2  812 1308    2  223  292 1699 1309  659 2478    2  512    1    0\n",
      "    0    0    0    0]\n",
      "Reversed as in Sutskever et al. \n",
      "[ 512    2 2478  659 1309 1699  292  223    2 1308  812    2 1010  414 1534\n",
      " 1026  113    2 1657 1426 2468   58 2396 1860    2  586  223  844    1    0\n",
      "    0    0    0    0]\n",
      "Tensor(\"decoder_inputs:0\", shape=(?, ?), dtype=int32)\n",
      "[   1  467  211 2707 2784 2317  685  552 1688 2782 1068 2440  588    2 1098\n",
      "  421 1467    2    2 1740    2  211 1784 2722  552 2911  588  643 2930    0\n",
      "    0    0    0    0    0]\n",
      "Tensor(\"decoder_targets:0\", shape=(?, ?), dtype=int32)\n",
      "[ 467  211 2707 2784 2317  685  552 1688 2782 1068 2440  588    2 1098  421\n",
      " 1467    2    2 1740    2  211 1784 2722  552 2911  588  643 2930    1    0\n",
      "    0    0    0    0    0]\n",
      "Decoder inputs at test time\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def batch_source_target(source, target, batch_size):\n",
    "    assert len(source) == len(target)\n",
    "    for start in range(0, len(source), batch_size):\n",
    "        end = min(start + batch_size, len(source))\n",
    "        #print type(source[start:end])\n",
    "        #print len(target[start:end])\n",
    "        yield source[start:end], target[start:end]     \n",
    "\n",
    "\n",
    "def make_feed_dict(fd_keys, s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_inputs_ = format_batch([[1]+sequence[0:-1] for sequence in t_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def make_test_feed_dict(fd_keys,s_batch, t_batch, reverse_encoder_inputs= False):\n",
    "    # At testing time, we can't supervise the decoder layer with\n",
    "    # the 'gold truth' example as input, so we instead feed in\n",
    "    # word generated at  previous timestep. This is (apparently)\n",
    "    # equivalent to feeding in zeros for the decoder inputs\n",
    "    encoder_inputs_ = format_batch(s_batch)\n",
    "    if reverse_encoder_inputs:\n",
    "        encoder_inputs_ = format_batch([sequence[-2::-1]+[1] for sequence in s_batch])\n",
    "    decoder_targets_ = format_batch([sequence for sequence in t_batch])\n",
    "    decoder_inputs_ = format_batch([[0]*len(sequence) for sequence in t_batch])\n",
    "    return {\n",
    "        fd_keys[0]: encoder_inputs_,\n",
    "        fd_keys[1]: decoder_inputs_,\n",
    "        fd_keys[2]: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test everything is working okay\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for s_sample_batch, t_sample_batch in batch_source_target(train_s[0:2], train_t[0:2], batch_size):\n",
    "    fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "    fd = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch)\n",
    "    fd_r = make_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= True)\n",
    "    fd_t = make_test_feed_dict(fd_keys, s_sample_batch, t_sample_batch, reverse_encoder_inputs= False)\n",
    "    assert len(fd.values()[0].T[0]) == len(fd_r.values()[0]) # reversed list must be the same length as original\n",
    "    print fd.keys()[0]\n",
    "    print np.array(fd.values()[0]).T[0]\n",
    "    print \"Reversed as in Sutskever et al. \"\n",
    "    print np.array(fd_r.values()[0]).T[0]\n",
    "    assert len(fd.values()[1].T[0]) == len(fd.values()[1].T[1]) # decoder inputs and targets must be the same\n",
    "    \n",
    "    for i in range(len(fd.keys())-1):\n",
    "        print fd.keys()[i+1]\n",
    "        print np.array(fd.values()[i+1]).T[0]\n",
    "    \n",
    "    print \"Decoder inputs at test time\"\n",
    "    print np.array(fd_t.values()[1]).T[0]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there will be 17 samples in the final batch\n",
      "training has begun...\n",
      "epoch 0\n",
      "batch 53\n",
      "loss: 2.46927428246\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2   2   2   2   2   2   2   2   2   2   2   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [588   2   2   2   2   2   2 588   2   2   2   2   2   2   2   2   2   2\n",
      "   2 588   2 588   2 588   2   2   2   1   2   2   2   2   2   2   2   2\n",
      "   1   2   0   0   0   0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588 588   2   2   2   2   2   2   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2    2    2    2  211    2    2    2    2\n",
      "    2    2 2930    2    2  588    2    2    2    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 1\n",
      "batch 106\n",
      "loss: 2.33920693398\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2   2   2   2 211   2   2   2 211   2 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2  588    2    2    2    2    2    2 2930\n",
      "    2    2    2    2  588    2  588    2  588    2    2    1    1    2    2\n",
      "    2    2    2    2    2    2    1    2    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588    2    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930    2  211  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 2\n",
      "batch 159\n",
      "loss: 2.26724815369\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2 548   2   2   2 211   2   2   2 211 211 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2  588    2    2    2    2    2    2 2440\n",
      "    2    2    2    2  588    2  588    2  588 2930    2    1    1    2    2\n",
      "    2    2    2    2    2    2    1    2    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588    2    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930 2930 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 3\n",
      "batch 212\n",
      "loss: 2.20529389381\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2 548   2   2   2 211   2   2   2 211 211 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2  588    2    2    2    2    2    2 2440\n",
      "    2 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "    2    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588    2    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930 2930 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 4\n",
      "batch 265\n",
      "loss: 2.14418053627\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2 548   2   2   2 211   2   2   2   2   2 211   2   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1567  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "    2 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "    2    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2  588  211    2    2    2    2  211    2  588    2    2\n",
      "    2    2 2930 2930 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 5\n",
      "batch 318\n",
      "loss: 2.08552145958\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2    2    2  211    2    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1567  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 6\n",
      "batch 371\n",
      "loss: 2.03075218201\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 7\n",
      "batch 424\n",
      "loss: 1.97993254662\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588 2930    2 2930    2    2    1    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2    2 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 8\n",
      "batch 477\n",
      "loss: 1.93114614487\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  588 2198  588    2  588    2  588 2930    2 2930    2    2  211    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588 2930 2930    2 2930  588    2  211 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 9\n",
      "batch 530\n",
      "loss: 1.88868868351\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2    2  211 1567    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2    2    2    2  211    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [588   2   2 211   2   2   2 211   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 10\n",
      "batch 583\n",
      "loss: 1.83119356632\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 1567  548    2    2    2  211    2 1567    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2    2    2  588    2    2    2    2 2440\n",
      "  548 2198 1717    2  588    2    2    2    2    2    2  211  211    2    2\n",
      "  211    2    2 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  211    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 11\n",
      "batch 636\n",
      "loss: 1.77818632126\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588    2    2    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2  211    2  211  211    2    2\n",
      "  211    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  211    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 12\n",
      "batch 689\n",
      "loss: 1.72839796543\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2  211    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211  211    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 13\n",
      "batch 742\n",
      "loss: 1.67876577377\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  548 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211    2    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2  211 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 14\n",
      "batch 795\n",
      "loss: 1.63055193424\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2 1567    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  588 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2 2930 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 15\n",
      "batch 848\n",
      "loss: 1.58262419701\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2    2    2 2930 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  588 2198  588    2  588    2  588  211    2  211    2  211  211  229    2\n",
      "  211 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2 2440  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 16\n",
      "batch 901\n",
      "loss: 1.53509712219\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "  588 2198  588    2  588    2  588  211    2  211    2 1722  211 2827    2\n",
      "  548 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 17\n",
      "batch 954\n",
      "loss: 1.487803936\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588  211  548    2    2    2  211    2    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "    2 2198  588    2  588    2  588  211    2  211    2 1722  211 2954    2\n",
      "  548 1001 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  211    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 18\n",
      "batch 1007\n",
      "loss: 1.4385509491\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [ 588    2 1098  588    2  211    2  588    2  588 2954  548    2    2 2440\n",
      "    2 2198  588    2  588    2  588  211    2  211    2 1722  211 2954  588\n",
      "  548 1717 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "  588  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "epoch 19\n",
      "batch 1060\n",
      "loss: 1.3912255764\n",
      "[[2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239\n",
      "   974    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154\n",
      "  1755  292 2888 1309 1426   30  639 2239  512    1    0    0    0    0\n",
      "     0]\n",
      " [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736\n",
      "  2540 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471\n",
      "    23 2579  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2478   88 1675 2888 2611  700    2 1035  904 1973  904 2089    2  280\n",
      "  2396 1863  512    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1035  559  568 2540 2475    2  211   70 2475 2087 1860 2478    2  414\n",
      "  2797 1860 2475  925  414 1068    2  512    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1809 2363 1337 1597 1708 1017  223 1425 2786  414  854 1597    2 1026\n",
      "  1702 2069  223  363 1393 1026 2094  223 1035 2363    2 1393  559 2197\n",
      "   223  559 2616 1702 2069 1035    2  512    1    0    0    0    0    0\n",
      "     0]\n",
      " [2363 2407  414  876 1082 1079 1247    2 1308    2    2  113 1449 2497\n",
      "  2475 1395 2667  414 2933 1860 2475    2  223    2    2 2089    2 1870\n",
      "  2118    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 414  177    2 2200 1389 2895    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 1376    2    2  414    2    2   53 1941  302 1509  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1229 2023 1821  566   92  414 1825 2475  769  512    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2154 1393 1597    2  223    2 1509 1556    2 1026 2487  223 1035  559\n",
      "  2089 1699 1034    2 1669 1170 1026 2566  414  876 1863 2351  412  223\n",
      "  1425  974 2740 2540 2396  463   23  511    2 2888    2 2157 2275  512\n",
      "     1]\n",
      " [1425 2478 2718  580 1026 1868 2478    2 1308 2228    2  512    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2425 1923  223  972 1375 1670 2888 1860 2475 1230  363 1817 2228    2\n",
      "   512    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2396 2228 1657 1269 2363  700  706 1597 1792 2888 2478 1295  414 2478\n",
      "   471  985  414 2396    2  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [1425 2718 1699 2185 2228    2  609 1860 1333  559 2089  723 2239 1238\n",
      "  2749 1393 1425    7 2239  512    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 974 1406    2 1035  694 2888 2228    2 2425  974  414 1172 2695  423\n",
      "   859  414    2  512    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "  sample 1:\n",
      "    input     > [2239 2147 1699 1247 1426 1149 1547 2239  223    2    2    2  512    1    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1896  549  699 1717 1490  240  211 1896 2877    2    2  558 2257 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [2272  211  548 1717    2    2  211 2629    2    2    2 2930 2930 2930    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 2:\n",
      "    input     > [1860 1426  175 1770 1597 2478 1481  378  223 2831 1699  235 2206 2239  974\n",
      "    2 1237  423  961 1597 2396    2 2673 2118 2800  992  767 2154 1755  292\n",
      " 2888 1309 1426   30  639 2239  512    1    0    0    0    0    0]\n",
      "    actual    > [1193 1690  552 1957  422 1081 1567 2762 1098 2272 2436  548 1717 1317 1098\n",
      " 2784 1145 1071  650 1544 2440 2258 1567    2 2674 2155 1722 2946 2530 1913\n",
      "  548  592 1202 2198    2 2930 2272    1    0    0    0    0]\n",
      "    predicted > [2272    2 1098  588    2  211    2  588    2  588 2954  548    2    2 1098\n",
      "    2 2198  588    2  588    2  588    2    2    2    2 1722  211 2954    2\n",
      "  548    2 1567 2198    2 2930    1    1    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [1794    2    2 1597 1272    2 1748  255 2895    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1765  147    2 1003 1425 1310 1315 2898    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588 2614    2    2    2    2    2 2930    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "  sample 4:\n",
      "    input     > [1035 2915  223 2425 2475  523  223 2478 1921 2478  700 2113 2888 2736 2540\n",
      " 2475  707    2 1809 2363  700  414 2375 1080  414 2480  113 2471   23 2579\n",
      "  512    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    actual    > [1124  600  211  650  547  211  588 1803    2 1861  548 1483  588 1664 1107\n",
      " 2384 1044 1561 1029  552  588 2455 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "    predicted > [ 588    2  211  588  588  211  588    2    2    2  548    2  588    2    2\n",
      "    2  211  211    2  552  588    2 1693 2930    1    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "def format_idx(idx):\n",
    "    # Just cuts out the padding of word index lists\n",
    "    li = []\n",
    "    for i in idx:\n",
    "        if i ==0:\n",
    "            break\n",
    "        else:\n",
    "            li.append(i)\n",
    "    return li\n",
    "\n",
    "BLEU = []\n",
    "epochs = 30 # How many times we loop over the whole training data\n",
    "batch_size = 92 # After how many sequences do we update the weights?\n",
    "print \"there will be {} samples in the final batch\".format(len(train_s)%batch_size)\n",
    "fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "try:\n",
    "    batch_n = 0\n",
    "    print \"training has begun...\"\n",
    "    for epoch in range(epochs):    \n",
    "        for s_batch, t_batch in batch_source_target(train_s, train_t, batch_size):\n",
    "            feed_dict = make_feed_dict(fd_keys, s_batch, t_batch)\n",
    "            _, l = sess.run([train_op, loss], feed_dict)\n",
    "            \n",
    "            #if batch_n == 0 or batch_n == 60:\n",
    "            #    batch_n += 1\n",
    "            batch_n +=1\n",
    "        loss_track.append(l)\n",
    "        print \"epoch {}\".format(epoch+1)\n",
    "        print 'batch {}'.format(batch_n)\n",
    "        print 'loss: {}'.format(sess.run(loss, feed_dict))\n",
    "        predict_ = sess.run(decoder_prediction, feed_dict)\n",
    "        #predictions = [remove_EOS_PAD(pred) for pred in predict_.T]\n",
    "        #actuals = [[remove_EOS_PAD(act)] for act in fd[decoder_targets].T]\n",
    "        #BLEU2 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.5,0.5))\n",
    "        #BLEU.append(BLEU2)\n",
    "        for i, (inp, act, pred) in enumerate(zip(feed_dict[encoder_inputs].T,\n",
    "                                                 feed_dict[decoder_targets].T,\n",
    "                                                 predict_.T)):\n",
    "            \n",
    "            print '  sample {}:'.format(i + 1)\n",
    "            print '    input     > {} \\n {}'.format(format_idx(inp), ' '.join(ids_to_phrases(inp, lang=lang_s)))\n",
    "            #)\n",
    "            print '    actual     > {} \\n {}'.format(format_idx(act), ' '.join(ids_to_phrases(act, lang=lang_s)))\n",
    "            print '    predicted     > {} \\n {}'.format(format_idx(pred), ' '.join(ids_to_phrases(inp, lang=lang_s)))\n",
    "            if i > 2:\n",
    "                break\n",
    "            \n",
    "    print 'Training is complete'\n",
    "except KeyboardInterrupt:\n",
    "    print 'training interrupted'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(, loss_track)\n",
    "#l = [s for i,s in sorted(zip([len(row) for row in l], l))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1255b0a90>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAH/CAYAAADnvTNXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3Xm81mP+x/HXVdlKWlAiSoxmJH6WqYgfkrFkjbEOkWSI\nIoxtUmOMbWQty6jIPia7LIWQJTVkjImskbXRlCRpu35/XKdfaUrndL7nfO/73K/n49HjPt/rvu9v\nn2Z6PObdZz7f6woxRiRJkiQVvlp5FyBJkiSpfAzvkiRJUpEwvEuSJElFwvAuSZIkFQnDuyRJklQk\nDO+SJElSkTC8S5IkSUXC8C5JkiQVCcO7JEmSVCQM75IkSVKRMLxLkiRJRcLwLkmSJBUJw7skSZJU\nJAzvkiRJUpHIJLyHEC4PITwdQvgkhDAnhDA9hPB6COHCEELjCt5roxDCsBDCZyGEuSGEj0IIV4cQ\nGmZRqyRJklSsQoyx8jcJ4QfgNWASMA2oB3QAfgl8BnSIMX5Wjvu0Al4B1gMeAiYD7YBOwDtAxxjj\njEoXLEmSJBWhrML76jHGectZvxg4H7ghxnhqOe7zFNAZOC3GeMNS6wOBM4CbYoynVLpgSZIkqQhl\nEt5XePMQtgbeAEbHGPdayWc3BT4APooxbrbMe2sDX5RdNokxfl8V9UqSJEmFrKofWD2g7PUf5fhs\np7LXUcu+EWOcDbwE1CWN40iSJEklp06WNwshnEWad28A7ADsTOq8X16Or7cGIvDuCt5/D9gT2AIY\nU+liJUmSpCKTaXgHzgSaLHX9BHBcjHF6Ob7boOz1mxW8v3jdXWckSZJUkjIN7zHGZgAhhPWBnUgd\n9zdCCF1ijG9U8vZh8W+z0g+GUHWD/JIkSdIyYoxh5Z+qvCqZeY8x/jvG+DDwK2Bd4PZyfG1xZ73B\nCt5fZ5nPSZIkSSUl67GZH4kxfhJCmARsE0JoHGP8z098fDKpu77FCt7/Wdnrimbil/f7l/ejUrUK\nIfj3UwXJv5sqVP7dVKEKoVoa7v+vqnebAdiw7HXhSj63+CHUXy37RtlWkR2B74Fx2ZUmSZIkFY9K\nh/cQws9CCOssZz2EEP5EeoD1pRjjN2XrdUIIrctOU/1/McYPSdtEtgwhLHug00WkXWyGu8e7JEmS\nSlWlD2kKIfQBLgVeBD4CpgNNgV2BVsDnQOcY4ztln29R9rkpMcZWy9yrFWk/9ybAI8DbpH3ddwPe\nATrGGGeUo6YIjs2ocPl//6pQ+XdThcq/mypUi8dmquuB1Sxm3p8GNieNtfwPaSvH70iz6cOB62OM\nM5f5TmQ5u8bEGD8MIexA6rTvDexDOln1GuCi5dxHkiRJKhmV7rwXIjvvKnR2kFSo/LupQuXfTRWq\n6u68V8cDq5IkSZIyYHiXctC/f/+8S5CWy7+bKlT+3ZQSx2YkSZKkVeTYjCRJkqTlMrxLkiRJRcLw\nLkmSJBUJw7skSZJUJAzvkiRJUpEwvEuSJElFwvAuSZIkFQnDuyRJklQkDO+SJElSkTC8S5IkSUXC\n8C5JkiQVCcO7JEmSVCQM75IkSVKRMLxLkiRJRcLwLkmSJBUJw7skSZJUJAzvkiRJUpEwvEuSJElF\nwvAuSZIkFQnDuyRJklQkDO+SJElSkTC8S5IkSUXC8C5JkiQVCcO7JEmSVCQM75IkSVKRMLxLkiRJ\nRcLwLkmSJBUJw7skSZJUJAzvkiRJUpEwvEuSJElFwvAuSZIkFQnDuyRJklQkDO+SJElSkTC8S5Ik\nSUXC8C5JkiQVCcO7JEmSVCQM75IkSVKRMLxLkiRJRaJGh/d33827AkmSJCk7NTq8H3UUzJuXdxWS\nJElSNmp0eH/tNejXL+8qJEmSpGyEGGPeNWQuhBABatWKLFoEo0dD5855VyVJkqSaJoQAQIwxVMfv\nV6M77/37p9djj4Wvv863FkmSJKmyanTnff78yO67w4svwgEHwEMPQaiWfxNJkiSpFNh5z1CdOnDX\nXdCgATzyCNx4Y94VSZIkSauuRnfeF//Z7rsPDj8c1lwTJkyArbbKtTxJkiTVEEXXeQ8hNA4h9Agh\nPBBCeC+EMCeEMDOEMDaE0D2Eig2qhBC6hBBGhRCmlt3rgxDCfSGEDqta42GHQffuMHcuHHkkfP/9\nqt5JkiRJyk+lO+8hhJOAG4HPgTHAJ0BToCvQEBgRYzysnPe6HDgb+Bp4qOx1c+AAYDXgmBjj3eW4\nz4867wCzZ8N228F778Fpp8F115X7jyhJkiQtV3V33rMI77sB9WKMI5dZbwJMAJoDh8YYH1zJfZoC\nnwHTgLYxxulLvbcr6R8GH8YYNy9HTf8V3iHt+77jjjB/Pjz2GHTpUo4/oCRJkrQCRTc2E2N8btng\nXrY+DbgJCMBu5bhVi7J6Xl06uJfd63ngW2D9ytS6/fbwpz+ln487Dr74ojJ3kyRJkqpXVe82M7/s\ndUE5PvseMA9oF0JYd+k3Qgj/C9QHRle2oDPPhD32SPu+H3ccLFpU2TtKkiRJ1aPKdpsJIdQG3gC2\nBPaOMa40eIcQegNXAdNJM+/TSTPv+wPPkWbeV3rc0orGZhb7/HPYemuYPh0GDoS+fcv1R5IkSZJ+\npOhm3ld44xCuBPoCj8UYD6jA9w4EhpEedl3sfaB/jPHect7jJ8M7pH3fDzwQVlsNXn0Vtt22vBVK\nkiRJSdHNvC9PWQe9LzAJOLYC3/sdMIIU3jcD6gHbAx8Bd4cQLsuqxgMOgFNOSQ+vHnkkfPddVneW\nJEmSqkbm4T2E0Au4BngL6BRjnFnO7+0KXAY8FGM8O8Y4JcY4N8b4BnAwaSeaM0MILStQywp/DRgw\ngCuvhDZtYPJkOP30iv5JJUmSVAoGDBiwwkxZ3TIdmwkhnE6aWX8T6Fye+fSlvvtnUre+d4xx8HLe\nvx84iPJtO7nSsZnF/vlP+OUv4Ycf4G9/g0MPLW/FkiRJKnVFOzYTQjiHFNxfB3avSHAvs0bZ64q2\ng1y8Pm8Vyluhtm3hyivTzyeeCFOnZnl3SZIkKTuZhPcQQj/gUtKhTJ1jjDN+4rN1QgitQwitlnlr\nLGlP+J4hhA2X+c4+QEdgLvByFjUvrVevdGDTzJnwm9/AwoVZ/w6SJElS5WVxwmo34FbSXu6DgG+W\n87EpMcbhZZ9vQXoAdUqMsdVS9wnAk0BnYDbwIPAlaavJxWeh9okxDipHTeUem1ns3/9O20d++SVc\nfDFccEG5vypJkqQSVXRbRYYQ+gMXruRjz8cYO5V9vgXwISm8b7bMvWoDvYAjSKG9LvAf4FXguhjj\nM+WsqcLhHWDUKNhrL6hdG158ETp0qNDXJUmSVGKKLrwXolUN7wBnnZUObtp0U3jjDVhnnczLkyRJ\nUg1heM9AZcL7Dz/AjjvCxIlp/v2OOzIvT5IkSTVE0e42U1OssQbccw/UrQt33pl+SZIkSYXA8L4c\nrVvDtdemn085BT78MN96JEmSJDC8r9AJJ8Ahh8C338JRR8H8+XlXJEmSpFJneF+BEOAvf4HmzeHV\nV+EPf8i7IkmSJJU6H1hdiRdegN12Sz+PGQO77lrpW0qSJKmG8IHVAvO//5sObIox7T7zn//kXZEk\nSZJKlZ33cpg/P4X4ceOga1cYMSKN1UiSJKm02XkvQKutBnfdBfXrwwMPwNCheVckSZKkUmR4L6dW\nreDGG9PPffrAO+/kW48kSZJKj+G9Ao4+Os29z5mTto/84Ye8K5IkSVIpMbxX0ODBsOmmMHFiepBV\nkiRJqi4+sLoKxo2DnXeGhQvhqafgV7+qkt9GkiRJBc4HVotAhw5LDm069liYNi3feiRJklQa7Lyv\nooULoVOndIhTly7w6KNuHylJklRq7LwXidq14c47oVEjGDkSBg3KuyJJkiTVdHbeK+n+++HQQ2GN\nNWD8eNh66yr/LSVJklQg7LwXmUMOgRNPTNtGHnkkfP993hVJkiSppjK8Z+Dqq6F1a5g0Cc46K+9q\nJEmSVFM5NpORiROhfXuYPx8efhgOOKDafmtJkiTlxLGZIrXttnDZZenn7t3h88/zrUeSJEk1j533\nDC1aBPvsA6NGwR57pNda/vNIkiSpxrLzXsRq1YLhw2H99eGZZ2DgwLwrkiRJUk1i570KjBwJ++0H\nderAK6/ADjvkUoYkSZKqmJ33GqBLFzjtNFiwII3P3H9/3hVJkiSpJjC8V5ErroCDD4ZZs9IhTn36\npL3gJUmSpFVleK8ia66ZOu7XXAOrrQbXXQe77AIffZR3ZZIkSSpWzrxXg/Hj4fDDYcoUaNAAbr01\ndeUlSZJU3Jx5r4HatYPXX4eDDoJvvoGuXeH002HevLwrkyRJUjGx816NYoRrr4Wzz04Ps/7yl3Df\nfdCyZd6VSZIkaVVUd+fd8J6DV19NYzQffwwNG6YxmoMOyrsqSZIkVZRjMyWgffs0RnPAATBzZpp/\n79vXMRpJkiT9NDvvOYox7Ubzu9+lMZp27eCvf3WMRpIkqVg4NpOBYgnvi736Khx2GHzySRqjue02\nOPDAvKuSJEnSyjg2U4Lat4eJE2H//dMYzUEHwZlnOkYjSZKkH7PzXkBihKuvhnPOSWM07dunMZoW\nLfKuTJIkScvj2EwGijW8L/bKK2k3mqlToVEjGD48deUlSZJUWBybETvuCG+8AfvtBzNmpF1pzjoL\n5s/PuzJJkiTlyc57AYsRBg6Ec8+FhQuhQ4c0RrPJJnlXJkmSJHBsJhM1Jbwv9vLLcMQRS8Zobr89\ndeUlSZKUL8dm9F922intRtOlSxqj2X9/OPtsx2gkSZJKjZ33IrJoURqjOe+8NEaz445pjGbjjfOu\nTJIkqTQ5NpOBmhreF3v55bQbzaefQuPGaYymS5e8q5IkSSo9js1opRaP0ey7L/znP2n+/ZxzHKOR\nJEmq6ey8F7FFi+DKK+H889MYzU47wb33OkYjSZJUXRybyUCphPfFXnopjdF89lkao7njjtSVlyRJ\nUtVybEYV1rFjOtRp773TGE2XLmlveMdoJEmSahY77zXIokVwxRXw+9+nMZqOHdMYTfPmeVcmSZJU\nMzk2k4FSDe+LjR0LRx6ZxmjWXTeN0eyzT95VSZIk1TxFNzYTQmgcQugRQngghPBeCGFOCGFmCGFs\nCKF7WPwnqtg99wghPBhC+CKEMDeE8FkI4ckQwt6VrbcU7LJL2o1m771h+vQ0/37qqTB7dt6VSZIk\nqTIq3XkPIZwE3Ah8DowBPgGaAl2BhsCIGONhFbjfFcBZwFTgCeBrYH1gO+CZGOO55bhHSXfeF1s8\nRnPhhWn+vVUruO22FO4lSZJUeUU3NhNC2A2oF2Mcucx6E2AC0Bw4NMb4YDnudSJwM3ArcFKMccEy\n79eOMS4sx30M70v5xz+gW7f0GgL06QOXXAJrrZV3ZZIkScWt6ML7T948hPOAPwHXxxj7rOSzq5O6\n7XOAny0b3Cv4+xrelzFvHlx8cQrtCxfCFlvA8OHQoUPelUmSJBWvopt5X4nFmxWWJ4jvSRqPuR+I\nIYQuIYTfhRB6hxCMmJW0+upw0UUwbhxsuSW8+27ajeacc2Du3LyrkyRJUnlUWXgPIdQGugEReLIc\nX/ll2WfnAROBR4FLgauBl0MIz4UQ1quickvGDjvAa6+l0A5pJn777eHvf8+3LkmSJK1cVXbeLwfa\nACNjjKPL8fkmQADOBhYBHYH6wNbAU8D/AvdVTamlZc014bLL0smsW2wBkyal8Zl+/dJ4jSRJkgpT\nlYT3EEJvoC8wCTi2nF+rXfY6H9g/xvhKjHFOjPFfwMHAp8CuIYT2mRdcojp0SCeznnFG2pnm4ouh\nXbv0YKskSZIKT+bhPYTQC7gGeAvoFGOcWc6vzih7nRhjnLr0GzHGuaTuO0C7CtSywl8DBgwo721q\ntLXWgquuguefh802S8F9hx3gj39M20tKkiSVugEDBqwwU1a3THebCSGcDlwFvAl0jjF+XYHvHg8M\nBZ6IMXZZzvtXAGcC58UYr1jJvdxtZhV8912ahR88OF1vv33akaZNm3zrkiRJKlRFu9tMCOEcUnB/\nHdi9IsG9zDOkB1a3XMH7W5W9frRqFWpl6tWDQYPgmWegRYv0YOt228Hll6ftJSVJkpSvTMJ7CKEf\naWeYCaSO+4yf+GydEELrEEKrpddjjJ+QdpjZpKyDv/R3fgXsRRqtKc/ONaqETp3gzTfhxBPTA6zn\nngs77wyTJ+ddmSRJUmnL4oTVbqQTURcAg4BvlvOxKTHG4WWfb0Hqnk+JMf4owIcQNgJeAjYGniVt\nGdkKOJC0A83hMcaHylGTYzMZeeopOOEE+OyztEvNJZdA795Qu/bKvytJklTTFd0JqyGE/sCFK/nY\n8zHGTmWfbwF8SArvmy3nfuuW3e8AoBkwC3gBuCzGWK7dyA3v2Zo5M+1Ic9tt6XrnndPPm/3Xf3uS\nJEmlpejCeyEyvFeNRx+Fnj3hyy+hbt10wNPJJ0Otqj6nV5IkqUAZ3jNgeK8606ensZm7707XnTrB\n0KHQsmWuZUmSJOWiaHebUWlYd1246y4YMQLWXx+efRbatoVbbgH/rSRJklS17LxrlU2bBqecAvff\nn6732guGDIHmzfOtS5IkqbrYeVfRaNIE/vY3uOceaNw47Uyz1VbpYVb/3SRJkpQ9O+/KxJdfpodZ\nH300Xe+/P9x8MzRrlm9dkiRJVcnOu4rSBhvAww/D8OHQoEEK8W3apAdb/TeUJElSNuy8K3OffppO\nZ32y7Czcrl3hxhvTmI0kSVJNYuddRa95c3j88bQDTf368MADqQs/YkTelUmSJBU3O++qUh9/DN27\npy0lAX79axg0yC68JEmqGey8q0Zp0QJGj4YbboB69dLuNG3awH33OQsvSZJUUXbeVW2mTIETTljS\nhT/kEBg8GJo2zbUsSZKkVWbnXTVWy5bw9NNw002w9trpcKc2beDee+3CS5IklYedd+Xi44+hR48U\n5gEOPjiN1mywQb51SZIkVYSdd5WEFi1g1Cj4y1/SjjQPPui+8JIkSStj5125++STtC/8qFHp+sAD\n077wns4qSZIKnZ13lZxNNkkHOg0ZAuusk05qbdMG7rzTLrwkSdLS7LyroEydCj17Ljmddf/90wOu\nG26Yb12SJEnLY+ddJW3jjdPprMOGQYMG8OijqQt/++124SVJkuy8q2B9+imcdFIK8wBdusDNN8NG\nG+VblyRJ0mJ23qUyzZvDY4/BbbelLvzIkakLf9ttduElSVJpsvOuovDZZ/Db36YwD7DPPmmbyebN\n861LkiSVNjvv0nJstBE88kiafW/YEJ54InXhhw2zCy9JkkqHnXcVnS++SF34Rx5J13vtBbfckh52\nlSRJqk523qWVaNYMHnoo7QPfqBE89VTqwg8ZYhdekiTVbHbeVdS+/BJOPjmFeYA990whfpNN8q1L\nkiSVBjvvUgVssAE88ADcfTc0bgyjR8NWW6WHWf23myRJqmnsvKvG+OorOOWUFOYBOndOs/AtW+Za\nliRJqsHsvEurqGlTGDEC7r0X1l0Xnn4a2raFm26CRYvyrk6SJKny7LyrRpo2DXr1SmEeoFOnNAu/\n6ab51iVJkmoWO+9SBpo0gb/9De67D9ZbD559NnXhBw+2Cy9JkoqXnXfVeP/+N5x6agrykGbhhw1z\nX3hJklR5dt6ljK2/Pvz1r6kTv956S2bh77zTHWkkSVJxsfOukvLll9CzJzz6aLo+5JD0QOt66+Vb\nlyRJKk523qUqtMEG8PDDMHQo1K8P99+f9oVfHOYlSZIKmeFdJScE6N4d3nwTdt017Q9/wAHQowfM\nmpV3dZIkSStmeFfJatky7UIzcCCssUbqxm+zDbzwQt6VSZIkLZ/hXSWtVi3o2xdeew222w6mTIHd\ndoOzzoK5c/OuTpIk6ccM7xLQpg2MGwf9+qVAP3AgbL89vP563pVJkiQt4W4z0jLGj4djjoF334U6\ndaB/fzj33PSzJEnS0qp7txnDu7Qcc+bAeefBddel63bt4PbboXXrfOuSJEmFxa0ipQJQty5cey2M\nHg3Nm6du/LbbwqBBsGhR3tVJkqRSZeddWomZM6F3b7jjjnTduTMMGwYbb5xvXZIkKX923qUC07Bh\nGpm5//50EuvTT0PbtnDnneC/DyVJUnUyvEvl1LUrvPUW7L8/fPNNeqj117+Gf/8778okSVKpMLxL\nFdC0KTz8cBqbqV8/dePbtoVHH827MkmSVAoM71IFhQDHHw9vvgm77gpffQUHHAA9esCsWXlXJ0mS\najLDu7SKWraEZ5+Fq66CNdaAoUNhm23g+efzrkySJNVUhnepEmrVgjPOSCexbrcdTJkCu+8OZ50F\nc+fmXZ0kSappDO9SBrbcEsaNgwsvTIF+4EDYfvsU6iVJkrJS6fAeQmgcQugRQngghPBeCGFOCGFm\nCGFsCKF7WLz55ard+5gQwqKyX90rW6tUlVZbDf7wB3j5ZdhiC5g0Cdq3h4svhgUL8q5OkiTVBFl0\n3n8N/AVoB4wDrgZGAG2AIcBfV+WmIYSNgeuAbwF301bRaNcOJk5MBzstWAD9+kHHjjB5ct6VSZKk\nYpdFeJ8M7B9jbB5jPCbGeEGMsQfwc2AqcEgI4eBVuO+twNfATRnUKFWrunXh2mvTgU7Nm8P48bDt\ntjBoECxalHd1kiSpWFU6vMcYn4sxjlzO+jRS8A7AbhW5ZwihT9l3jgfmVLZGKS977AH//Ccceyx8\n/z2cdhrstRd8+mnelUmSpGJU1Q+szi97LffEbwjhF8ClwDUxxherpCqpGjVsCMOHpwOd1lsvdeO3\n3hoeeCDvyiRJUrGpsvAeQqgNdCPNqz9Zge/cAUwBLqiq2qQ8dO0Kb70FXbrAjBlwyCHQsyd8913e\nlUmSpGJRlZ33y0kPrY6MMY4u53f6A9sAx8UYf6iyyqScNG0Kjz4K11+fDna65Za0peTEiXlXJkmS\nikGVhPcQQm+gLzAJOLac32kHnAdcGWMcXxV1SYUgBDj1VJgwAdq0SbvQtG+fTmr1YVZJkvRTMg/v\nIYRewDXAW0CnGOPMcnxn8bjMZODCZd+uRC0r/DVgwIBVva2UibZtU4Dv1Qvmz4czz4R99oEvv8y7\nMkmStLQBAwasMFNWtxBjdluohxBOB64C3gQ6xxi/Luf3GgAzSPPxy/tPYen1a2KMfVdyvwiQ5Z9N\nqkqPPALdu8P06bD++nDrrWk2XpIkFbbFAT7GWC1JPrPwHkI4h7RLzOvAnjHGGRX47pqkA5mWZztg\nW+BFUmd+dIzxbyu5n+FdRefzz9OWks88k65POw2uuALWXDPfuiRJ0ooVZXgPIfQD/gBMAPb6qVGZ\nEEIdYDNgfozxw3Lcuz9plObEGOOwctZjeFdRWrQIBg6E889Pp7O2bQv33JNm4yVJUuGp7vBep7I3\nCCF0IwX3BcBLQJ/lzP9MiTEOL/t5I+Bt0naQrcr721S2TqkY1KoFZ58Nu+8ORx2VDnjaYYcU6E8+\nOT3sKkmSSlelwzvQkjSTXhvos4LPPA8MX+o6lv0qL1voKik77ACvvw59+sCwYemh1qeegqFD00FP\nkiSpNGX6wGqhcGxGNcl996XDnL75Bpo1gzvugD32yLsqSZIE1T82U5WHNEnKwGGHwT/+ATvvDF98\nAXvuCeecA/Pm5V2ZJEmqboZ3qQi0aAFjxsBFF6W5+CuugJ12gnffzbsySZJUnRybkYrMyy/D0UfD\nlClQrx5cfz0cd5wPs0qSlAfHZiT9pJ12gjfegCOOgO++S4c7HXEEzFzpWcaSJKnY2XmXilSM6eHV\nXr1g9mzYZBO46640Gy9JkqqHnXdJ5RJCOpF14kT45S/hk09g112hf/90wJMkSap5DO9Skdt8c3jp\nJTjvvNSNv+iiFOKnTMm7MkmSlDXHZqQaZMwYOOYY+OwzWGcduPnmNA8vSZKqhmMzklbZ7runPeEP\nOghmzYIjj4Ru3eDbb/OuTJIkZcHwLtUw664LDzyQuu5rrQW33w7bbgvjx+ddmSRJqizDu1QDhQA9\ne8Jrr8E228AHH0DHjnDppbBwYd7VSZKkVWV4l2qwX/wCxo2D009PO9Ccfz7suSd8+mnelUmSpFXh\nA6tSiXjiiXQS67Rp0LgxDBkCBx+cd1WSJBU3H1iVVCX22QfefBP23hv+8x/o2hWOPhqmT8+7MkmS\nVF6Gd6mENG0KI0fCtdemh1nvvhu23BLuvz/vyiRJUnk4NiOVqPffhxNOgBdeSNeHHgqDB0OTJvnW\nJUlSMXFsRlK12HzzdKjT4MFQrx6MGJG68HffnU5qlSRJhcfOuySmTIETT4Snn07XBxwAN90EzZrl\nWpYkSQXPzrukateyJYwaBbfcAuusA488krrwt91mF16SpEJi513Sj0ydCiedlLaWhLQ7zV/+Ahtv\nnG9dkiQVIjvvknK18cZpR5rhw6FhQ3jySWjTJgV4/z0sSVK+7LxLWqEvvoBTToGHHkrXe+yRRms2\n3TTfuiRJKhR23iUVjGbN4IEH4J57YN114ZlnoG1bGDQIFi3KuzpJkkqPnXdJ5TJtGpx2Gtx3X7re\nZRcYOhR+9rN865IkKU923iUVpCZN4K9/TaexNm0KY8fCNtvAVVfBwoV5VydJUmmw8y6pwqZPhzPO\ngDvuSNcdOsCwYfCLX+RblyRJ1c3Ou6SCt+66cPvt8OijsOGGMG4cbLstXHYZLFiQd3WSJNVcdt4l\nVcrMmXDWWWn+HWD77eHWW9ODrZIk1XR23iUVlYYNYcgQeOop2GQTeO21FOAvugjmzcu7OkmSahY7\n75Iy8+23cM45cOON6XrrrVMXfrvt8q1LkqSqYuddUtGqXx9uuAHGjIFWreDNN6FdO7jgAvjhh7yr\nkySp+Nl5l1QlvvsuhfbrroMYYcst04407dvnXZkkSdmx8y6pRqhXD665Ju0Hv8UWMGkS7LQTnH02\nfP993tVJklScDO+SqlTHjvDGG/C736XrK69Mhzu9+GK+dUmSVIwcm5FUbcaPh+7d4V//ghDgtNPg\nkktSl16SpGLk2IykGqtdu7SV5O9/D7VqpXn4rbaCp5/OuzJJkoqDnXdJuZg4MXXh33gjXR93HAwc\nCI0b51rP8xBzAAAgAElEQVSWJEkVYuddUknYdts0RnPJJbDGGnDbbWlHmhEj0u40kiTpv9l5l5S7\nyZPhxBPTzjQABx0EgwfDhhvmW5ckSStj511SyWndGp57Lp3MWr8+PPRQ6sIPGWIXXpKkpdl5l1RQ\nPv0UTj4ZHnssXe++O/zlL7D55vnWJUnS8th5l1TSmjeHRx6Be+6B9deHMWOgbdu0P/yCBXlXJ0lS\nvuy8SypYX38NZ5wBd96ZrrffHoYOTYc8SZJUCOy8S1KZ9daDO+6Axx+HTTZJe8TvsANccAHMnZt3\ndZIkVT8775KKwrffwvnnp11oYkwPuQ4ZAjvvnHdlkqRSZuddkpajfn24/np48UX4+c/T9pK77AK9\nesGsWXlXJ0lS9TC8SyoqO+2UTmXt1w/q1IEbboA2bWDkyLwrkySp6jk2I6lovfkm9OgBEyak66OO\ngmuuSbvUSJJUHYpubCaE0DiE0COE8EAI4b0QwpwQwswQwtgQQvew+E9UTfeRVDq23hpeeQUGDoS1\n1oK7706HO911l4c7SZJqpkp33kMIJwE3Ap8DY4BPgKZAV6AhMCLGeFh13afsXnbepRLzwQfQsyc8\n+2y63ndfuOkm2HjjfOuSJNVs1d15zyK87wbUizGOXGa9CTABaA4cGmN8sDruU/Ydw7tUgmKEYcPg\nzDPhm29g7bXhssvSia21fMJHklQFim5sJsb43LKBu2x9GnATEIDdqus+kkpXCHDCCfD229C1K8ye\nDaeeCrvuCu+8k3d1kiRVXlX3ouaXvVb2UPOs7iOpBDRrBvffDyNGwAYbpO0lt9kG/vQnmD9/5d+X\nJKlQVVl4DyHUBroBEXgy7/tIKj2HHAKTJkH37jBvHvz+9+mE1r//Pe/KJElaNVXZeb8caAOMjDGO\nLoD7SCpBjRrB0KHw9NPQqlXaXrJ9ezj7bJgzJ+/qJEmqmCrZ5z2E0Bu4BpgE7BxjnFmd9/GBVUnL\n8913cOGFaS/4RYtgs83glltg993zrkySVKyK7oHVZYUQepEC91tAp0oE90rfJ4Swwl8DBgxYlbIk\nFbF69dKe8K+8Am3bpu0lO3VKBz199VXe1UmSCtWAAQNWmCmrW6ad9xDC6cBVwJtA5xjj13ncx867\npJWZNw8uvxwuvjj9XL8+nHsunHFGOvBJkqTyKLp93v//RiGcA1wKvA7sGWOckdd9DO+Syuudd9L8\n+2OPpeuNN4ZLLoGjjnJveEnSyhXl2EwIoR8pcE8gdcpXGLhDCHVCCK1DCK0qcx9JysLPfw6PPpoe\naN1mG5g6FY45Jj3UOnZs3tVJkvRjWZyw2g24lbQH+yDgm+V8bEqMcXjZ51sAH5WttVrV+6ykJjvv\nkips4UK4/Xa44AL44ou0dvDBabzmZz/LtzZJUmEqurGZEEJ/4MKVfOz5GGOnss+3AD4kBfHNVvU+\nK6nJ8C5plX33HVx5JVxxRdpOsk4d6NUr7VTTuHHe1UmSCknRhfdCZHiXlIXPP4d+/eDWWyFGaNgw\nXffqBWuskXd1kqRCYHjPgOFdUpb+8Q8466w0Fw/psKfLL08nuOawS5gkqYAY3jNgeJeUtRjhiSdS\niH/77bTWsSNcdRW0a5dvbZKk/BTlbjOSVNOFAPvuC2++CTfeCOuvDy+9lHalOeoo+PjjvCuUJJUC\nO++StApmzYJLL4Wrr4Yffkgz8KefDuedBw0a5F2dJKm6ODaTAcO7pOry8cdw/vlw993pev314Q9/\ngBNPTLvUSJJqNsN7Bgzvkqrb+PHQt28apQH4xS/gz39OozY+1CpJNZcz75JUhNq1SyeyjhgBm22W\nHmrdbz/Yc8+0W40kSVkwvEtSRkJI20dOmpR2oWnUCJ55BrbdFrp3T/vGS5JUGY7NSFIV+c9/4I9/\nhMGDYf58qFsXzj47/apXL+/qJElZcOY9A4Z3SYXk/ffhnHPggQfSdbNm8Kc/wbHHQu3a+dYmSaoc\nw3sGDO+SCtHYsXDmmTBhQrreZhsYOBD22CPfuiRJq84HViWphtplFxg3Du68EzbeOD3I2rlzerB1\n8amtkiT9FMO7JFWjWrXg6KNh8mS45BKoXx9GjoS2beGkk3yoVZL00xybkaQcTZsG/fvDLbfAwoWw\n1lrQpw/87ndptxpJUmFz5j0DhndJxeadd+D3v4f770/XjRrBuefCaaelQC9JKkyG9wwY3iUVq/Hj\nU2gfMyZdb7QRDBgAxx0HderkWZkkaXkM7xkwvEsqZjHC6NEpxE+cmNZat07bS3btmg6DkiQVBneb\nkaQSFwL86lfw97/DvffC5punB1wPPRTat4dnn827QklSXgzvklSgatWCww+HSZPghhtggw3SHvF7\n7AF77QWvv553hZKk6ubYjCQVie++g2uvhcsvh1mz0trhh8PFF6fuvCSp+jnzngHDu6SabPp0uOwy\nuP56+OGH9CBrjx5w4YXQrFne1UlSaTG8Z8DwLqkUTJ2adqK57TZYtChtKXn66WmP+IYN865OkkqD\n4T0DhndJpeTtt+GCC+DBB9N1o0Zw/vnQq5d7xEtSVTO8Z8DwLqkUjRuXtpd8/vl03bx56sx36+Ye\n8ZJUVQzvGTC8SypVMcJTT8F558Ebb6S1n/887RF/8MHuES9JWXOfd0nSKgsB9t4bXnsN7r4bWrWC\nd96BQw6BDh2WnNwqSSpOhndJqoFq1YIjj0zz8IMHQ9OmMH48dOqUwv3ik1slScXFsRlJKgGzZ8M1\n18AVV8C336a1I45Ie8Rvtlm+tUlSMXPmPQOGd0lavq+/hksvhUGDYN689CBrz57Qr186wVWSVDGG\n9wwY3iXpp33yCfTvD7ffnvaIr1sXzjgDzj4bGjTIuzpJKh6G9wwY3iWpfP71r7RH/MMPp+tGjeDM\nM+G002CddfKtTZKKgeE9A4Z3SaqYV15Je8S/8EK6btQI+vZNId5OvCStmOE9A4Z3Saq4GOG559LB\nTotDfMOGKcT37m2Il6TlMbxnwPAuSZWzOMQvPq21YcM0E9+7d/pZkpQY3jNgeJekbDz3HPzhD+kV\nUvf9jDOgTx9DvCSB4T0ThndJytbzz6cQv/iE1gYN4PTT0y9DvKRSZnjPgOFdkqrGCy+kEP/ss+l6\nnXWWhPhGjfKtTZLyYHjPgOFdkqrW8kJ8nz4pxDdunG9tklSdDO8ZMLxLUvUYOzaF+GeeSdf166cQ\nf8YZhnhJpcHwngHDuyRVrxdfTCH+6afTdf36aWeaM86AddfNtzZJqkqG9wwY3iUpHy+9lEL86NHp\neu21U4jv29cQL6lmMrxnwPAuSfl6+eUU4keNStdrr51Oa+3bF9ZbL9/aJClLhvcMGN4lqTC88koK\n8U89la7r1Ush/swzDfGSagbDewYM75JUWMaNSyH+ySfTdb16cOqpKcSvv36+tUlSZRjeM2B4l6TC\n9OqrKcQ/8US6rlcPevWCs84yxEsqTob3DBjeJamwjR+fQvzjj6frunWXhPgmTfKtTZIqwvCeAcO7\nJBWHCRNSiB85Ml3XrQsnn5xC/AYb5FubJJWH4T0DhndJKi5//3sK8Y89lq7XXBO6d4ezz4aWLXMt\nTZJ+kuE9A4Z3SSpOr70Gf/wjPPxwuq5dG446Cs49F7bcMt/aJGl5qju816rsDUIIjUMIPUIID4QQ\n3gshzAkhzAwhjA0hdA+L/0Tlv99GIYRhIYTPQghzQwgfhRCuDiE0rGytkqTCtv328NBD8NZb8Jvf\npLU77oA2baBr1zRmI0mlrNKd9xDCScCNwOfAGOAToCnQFWgIjIgxHlbOe7UCXgHWAx4CJgPtgE7A\nO0DHGOOMctzHzrsk1QAffQR//jMMGwY//JDWOneG88+H3XaDirWHJCl7RTc2E0LYDagXYxy5zHoT\nYALQHDg0xvhgOe71FNAZOC3GeMNS6wOBM4CbYoynlOM+hndJqkG++AKuuQZuuAFmz05r7dunEL/f\nflCr0v8/siStmqIL7z958xDOA/4EXB9j7LOSz24KfAB8FGPcbJn31ga+KLtsEmP8fiX3MrxLUg00\nYwYMGgTXXgvTp6e1rbZKM/GHHw516uRbn6TSU3Qz7ysxv+x1QTk+26nsddSyb8QYZwMvAXWBDtmU\nJkkqNo0aQb9+8PHHcPXVsNFGS+bjW7eGm2+GuXPzrlKSqk6VhfcQQm2gGxCBJ8vxldZln313Be+/\nV/a6ReWrkyQVs3r14PTT4YMPYMgQ2Hxz+PBD+O1vYdNN4cor4dtv865SkrJXlZ33y4E2wMgY4+hy\nfL5B2es3K3h/8bq7zkiSAFhjDTjhBHjnHbj3XthmG/jyy7Q/fIsW0L//kvEaSaoJqiS8hxB6A32B\nScCxWd227NVBdknSj9SunWbeJ05Mp7V27Jjm4y+6KIX4M8+Ezz7Lu0pJqrzMw3sIoRdwDfAW0CnG\nOLOcX13cWW+wgvfXWeZz5allhb8GDBhQ3ttIkopECLDvvvDii/DCC7D33vDdd3DVVdCqFfTsCe+/\nn3eVkorNgAEDVpgpq1umu82EEE4HrgLeBDrHGL+uwHdPAG4Bbo4xnryc958E9iy775iV3MvdZiRJ\nQOrGX3opjBgBMaZtJQ87DM47D7beOu/qJBW7ot0qMoRwDnAp8DqwZ3kOU1rm+62A9/nprSIDsL5b\nRUqSKmryZLjiCrj9dlhQtgfafvulEL/TTvnWJql4FeVWkSGEfqTgPoHUGV9hcA8h1AkhtC4L6/8v\nxvghaZvIliGEU5f52kVAPWD4yoK7JEnL07o1DB2adqjp3RvWWgseeyzNx++2Gzz1VOrMS1Ihy+KE\n1W7AraS93Aex/Jn0KTHG4WWfbwF8VLb2owBfFuhfApoAjwBvk/Z13w14B+hYno6+nXdJ0sr8+9/p\nsKdBg+Cbsv/l2m67dGrrwQd7aquk8im6sZkQQn/gwpV87PkYY6eyz7cAPiSF982W/WAIYSNSp31v\nYF3SuMyDwEXlffjV8C5JKq9Zs+DGG9NDrdOmpbWf/xx+9zs46qi0HaUkrUjRhfdCZHiXJFXU99/D\nsGHw5z+nE1wBmjaFU09Nhz+tt16+9UkqTIb3DBjeJUmrav58uOceGDgQ3nwzra21FnTrlk51bd06\n3/okFRbDewYM75KkyooRnn02jdM8/viS9f32S4c+7bpr2ldeUmkzvGfA8C5JytLbb8PVV6dtJn/4\nIa1tuy307Zv2jF999Xzrk5Qfw3sGDO+SpKowbRrcdBMMHrzk4dYNN4TTToOTToJGjfKtT1L1M7xn\nwPAuSapKc+fCXXelkZpJk9Ja3brQvTv06QObb55vfZKqj+E9A4Z3SVJ1iBFGjUoPt44endZCgAMP\nTHPxHTs6Fy/VdIb3DBjeJUnV7Z//THPxd90F8+altV/+Ms3FH3oo1KmTb32SqobhPQOGd0lSXr78\nMs3E33gjTJ+e1jbZBHr3hh49oEGDfOuTlC3DewYM75KkvM2ZA3fckbrxkyentbXXTgG+d2/YdNN8\n65OUDcN7BgzvkqRCsWgRPPFEmosfMyat1aoFhxySRmo6dMi3PkmVY3jPgOFdklSIJk5Mnfh77oEF\nC9LajjumEH/wwVC7dr71Sao4w3sGDO+SpEL22WcwaFDaM37mzLS26aZpm8nu3aF+/Xzrk1R+hvcM\nGN4lScVg9mwYPjx14z/4IK2tsw707Jnm4jfeON/6JK2c4T0DhndJUjFZuBAefTQd+jR2bFqrXRsO\nOwxOPx3atcu3PkkrZnjPgOFdklSsJkxInfj77kuhHqB9+zRSc8ghsPrq+dYn6ccM7xkwvEuSit0n\nn6S5+CFDYMaMtNasGZx8chqrado03/okJYb3DBjeJUk1xXffpVNbr7sO/vWvtLb66nDEEakbv912\n+dYnlTrDewYM75KkmiZGePbZFOIffTRdA3TsmB5uPfhgWG21fGuUSpHhPQOGd0lSTfbhh2mkZuhQ\nmDUrrTVvDqecAieeCOutl299UikxvGfA8C5JKgWzZ8Ptt6du/OTJaW3NNeHoo1M3fuut861PKgWG\n9wwY3iVJpWTRIhg9OoX4xx9fsr7rrmku/oADPL1VqiqG9wwY3iVJperdd9NIza23ps48QIsW0KsX\n9OgBjRrlW59U0xjeM2B4lySVulmz4Lbb4Prr4f3301rdunDMMXDaadCmTa7lSTWG4T0DhndJkpJF\ni+CJJ9JIzahRS9Y7d05z8fvu60iNVBmG9wwY3iVJ+m+TJqWRmuHDYc6ctNaqVerEH388NGiQb31S\nMTK8Z8DwLknSis2YAcOGpSA/ZUpaq1cPjjsuBfnWrfOsTiouhvcMGN4lSVq5hQvhscfg2mthzJgl\n63vvnUZq9toLatXKrz6pGBjeM2B4lySpYv75z/Rw6x13wNy5aW2LLVInvls3qF8/3/qkQmV4z4Dh\nXZKkVTN9OgwZAoMHw9Spaa1+/TRS06uXIzXSsgzvGTC8S5JUOQsWwMMPp5GasWOXrO+5Z+rGu0uN\nlBjeM2B4lyQpO//4R+rE33knfP99WmvZEk45Bbp3h3XXzbU8KVeG9wwY3iVJyt6MGenk1sGD4cMP\n09qaa8JRR8Gpp8K22+Zbn5QHw3sGDO+SJFWdhQvhySfTVpNPPrlkvWPHFOK7doXVV8+vPqk6Gd4z\nYHiXJKl6vPsu3HBD6sjPmpXWNtgATjoJevaEDTfMtz6pqhneM2B4lySpes2enWbiBw2Cf/0rrdWp\nA4cemrrxO+0EoVqijVS9DO8ZMLxLkpSPGOH551OIf+ihNGID8D//k0L8kUdC3br51ihlyfCeAcO7\nJEn5mzoVbroJ/vIX+PrrtNaoEZxwApx8MrRqlW99UhYM7xkwvEuSVDjmzoW//S2d4DphQloLAfbb\nL3XjO3eGWrXyrVFaVYb3DBjeJUkqTOPHp5Gav/4V5s1La1tskU5vPe44WGedXMuTKszwngHDuyRJ\nhW3aNBgyBG68ET79NK2tvTYce2wK8ltumW99UnkZ3jNgeJckqTgsWACPPJJGap57bsl6p05ppGb/\n/dOuNVKhMrxnwPAuSVLxeeutdHrr7bfDnDlpbeON08OtPXrA+uvnW5+0PIb3DBjeJUkqXjNnwm23\npSD//vtpbY014Ne/TkF+xx3dM16Fw/CeAcO7JEnFb9EiGDUqPeD6+ONpD3mArbeG3/4WfvMbqF8/\n3xolw3sGDO+SJNUsH34It9wCQ4fCv/+d1tZeG44+OnXjt9km3/pUugzvGTC8S5JUM/3wAzz4YNql\n5oUXlqx36JBC/K9/DWutlV99Kj2G9wwY3iVJqvkmTUonuA4fDrNmpbXGjdN+8SedlPaPl6qa4T0D\nhndJkkrHd9/Bvfembvxrry1Z32OPNBt/4IGw2mr51aeazfCeAcO7JEml6e9/TyH+nnvg++/T2gYb\npK0me/ZMW09KWSrK8B5COATYFfgfYBugPnBnjPHYVbhXF6AP8AtgXeAL4DXgqhjjuHLew/AuSVIJ\nmzkz7Rd/003w9ttprVYt6NIlzcb/6ldQu3a+NapmKNbwPhHYGpgNfAr8HLirouE9hHA5cDbwNfBQ\n2evmwAHAasAxMca7y3Efw7skSSLG9GDrjTfCAw/A/PlpvWXLNBffvTs0aZJriSpyxRredwU+jTF+\nUPbzGCrYeQ8hNAU+A6YBbWOM05e5/xjgwxjj5uW4l+FdkiT9yFdfwbBhcPPN8PHHaW211eCQQ1I3\nfpddPPxJFVfd4b1WFjeJMT4fY/ygkrdpUVbPq0sH98X3B74FPBhZkiStkqZN4bzz4IMPYORI2H9/\nWLgwPey6666w1VZw/fXwzTd5VyqtWCbhPSPvAfOAdiGEdZd+I4Twv6Q5+tF5FCZJkmqO2rVh333h\nkUfgo4/g979PD7VOmgS9e8OGG6YHXJfeuUYqFJnvNrOqYzNl3+0NXAVMJ828TyfNvO8PPEeaef+6\nHPdxbEaSJJXb/Pnw8MNpNv7ZZ5es77BDGqk54gioWze/+lS4inLm/Uc3rER4L/v+gcAwoOFSy+8D\n/WOM95bzHoZ3SZK0SiZPTnPxt90GM2aktQYNoFu3tN1kmza5lqcCU5Qz71kJIfwOGEEK75sB9YDt\ngY+Au0MIl+VYniRJKgGtW8NVV8Fnn6UA36FDmoO/7ro0F9+uXerQLw72UnUqmPBe1rG/DHgoxnh2\njHFKjHFujPEN4GDSTjRnhhBaVuCeK/w1YMCAqvhjSJKkGmKttVK3/ZVX4PXX09aS66wDEybAKadA\ns2Zw5JEwalR68FU114ABA1aYKatbwYzNhBD+DPQFescYBy/n/fuBg4BDY4wPruRejs1IkqTMzZkD\nDz6YOvLPPJP2kQdo3hyOPRaOOw5+9rM8K1R1K+WxmTXKXle0HeTi9XnVUIskSdJ/qVsXjj4aRo9O\nO9VcdBG0agWffgqXXAJbbJH2ix86FL79Nu9qVRNVe3gPIdQJIbQOIbRa5q2xQAB6hhA2XOY7+wAd\ngbnAy9VTqSRJ0oq1aAH9+sF778Fzz6Wue7168OKLaavJDTZIYzfPPQeLFuVcrGqMrE5YPZA00gKw\nAbAX8CEpkAN8HWM8u+yzLUgPoE6JMbZa6h4BeBLoDMwGHgS+BLYEupR9rE+McVA56nFsRpIkVbtv\nv4URI+DWW2Hs2CXrm26agny3btCyZW7lqQoU5VaRIYT+wIU/8ZEpMcbNyj7bghTs/39tqfvUBnoB\nR5BCe13gP8CrwHUxxmfKWY/hXZIk5er992H48PRr6tQl6506wfHHQ9eu7h1fExRleC80hndJklQo\nFi5MBz/demt62HXu3LRevz4cfngK8jvuCDlsXKIMGN4zYHiXJEmFaOZM+OtfU5B/9dUl61tskWbm\njz0WNtoot/K0CgzvGTC8S5KkQvf222nLydtvhy+/TGu1asGvfpW68QccAGuumWuJKgfDewYM75Ik\nqVgsWABPPZWC/MMPw/z5ab1Ro3QI1PHHw/bbO1ZTqAzvGTC8S5KkYjR9Otx9dxqrmThxyfpWW6Wx\nmt/8Bpo2za08LYfhPQOGd0mSVOz+8Y/Ujb/zTvj667RWpw7suy+ccEJ6rVMn1xKF4T0ThndJklRT\nzJsHI0emID9yZNq9BqBZs9SNP+EE2Gyzn7qDqpLhPQOGd0mSVBN99VV6wHXoUJg8ecn67runU127\ndvUh1+pmeM+A4V2SJNVkMcJLL8GQIXDfffD992m9UaM0F9+jB2y9db41lgrDewYM75IkqVR88w3c\nc08K8q+9tmS9XbsU4o84Ih0IpapheM+A4V2SJJWiiRPTSM2dd6ZQD1CvXjrJtUcP6NDBLSezZnjP\ngOFdkiSVsu+/h/vvT934559fsr7llinEH3MMrLdefvXVJIb3DBjeJUmSknffhWHD0m41X32V1lZb\nDQ4+OAX5PfZIJ7tq1RjeM2B4lyRJ+rH589NWk0OGwBNPwKJFab1Fi7Td5PHHQ/Pm+dZYjAzvGTC8\nS5IkrdjUqakTP3QofPxxWqtVC/beO3Xj99svdee1cob3DBjeJUmSVm7RInjmmdSNf/DB1J0HaNJk\nyQFQW2yRa4kFz/CeAcO7JElSxXz9NdxxRwry/9fevcfoVdd5HH9/Ba0WEVBgWborWFQaRAw162Kb\ntFAtNF7RwiKMSjSiWfci6JLdxAsNa+LuJl6irhKz3oAWqCioqVIWwRYLiEYqahdUSpWKQcqWm2Ws\ntN/943dmZ3yY6YU5c85znnm/ksnp8zvnOfNt8+tvPs+Z3/mdDRtG2xcsKFfjly6FmTPbq69fGd5r\nYHiXJEl6cjLhlltKiL/8cti2rbQfcAAMDZUgf/zx7dbYTwzvNTC8S5IkTd7DD8MVV5Qgf+uto+1z\n55YbXM84Aw45pL36+oHhvQaGd0mSpHrdfnu5wfWSS2Dr1tK2zz5w8snlivypp5YHQk03hvcaGN4l\nSZKmxvBwubn10kth9WrYsaO0z5xZAvzQECxePH1WqzG818DwLkmSNPXuvx9WroTly+Hmm0fbDz64\nTKkZGoITToBoJNa2w/BeA8O7JElSszZuhBUrSpC/447R9tmz4ayzSpCfM6e9+qaK4b0GhndJkqR2\nZML69SXEX3YZ3Hvv6L65c0uQP/NMOPzw9mqsk+G9BoZ3SZKk9u3YAWvWlCB/5ZVl9Roo02hOOqlc\njV+6tCxD2VWG9xoY3iVJkvrL8DCsWlWC/KpVsH17aZ8xA17zmhLkX/Wq8rpLDO81MLxLkiT1r61b\n4atfLUF+zZoy1QbgwAPhtNPK1JqFC+EpT2m3zj1heK+B4V2SJKkbNm8uT3JdvrzMlR8xa1aZGz80\nBC95Sf+uWGN4r4HhXZIkqXs2bCghfsUK2LRptP2YY0qIP+ssOPLItqobn+G9BoZ3SZKk7sqEm24q\nQX7lSnjggdF98+eXIH/66WU9+bYZ3mtgeJckSRoM27fDtdeWIP/1r8Njj5X2ffeFJUvgvPNg0aL2\n6jO818DwLkmSNHgeeQSuvroE+euuK0tRfu5zcM457dVkeK+B4V2SJGmw3XdfmVLz5jfDQQe1V4fh\nvQaGd0mSJDWh6fDegdUzJUmSJIHhXZIkSeoMw7skSZLUEYZ3SZIkqSMM75IkSVJHGN4lSZKkjjC8\nS5IkSR1heJckSZI6wvAuSZIkdYThXZIkSeoIw7skSZLUEYZ3SZIkqSMM75IkSVJHGN4lSZKkjjC8\nS5IkSR1heJckSZI6opbwHhFLI+KTEbE2Ih6KiJ0RcfEkzveKiLgqIn4bEcMR8ZuIuCYiltRRr9S2\nZcuWtV2CNC77pvqVfVMqIjMnf5KI24DjgEeBzcAcYHlmvvVJnOs/gH8C7gG+DWwBDgHmAt/JzH/Z\ng3MkQB1/N2kqRIT9U33Jvql+Zd9Uv4oIADIzmvh++9Z0nnOBzZl5V0QsBG54MieJiHMowf2LwLsy\n8/Ge/ftMulJJkiSpo2oJ75m5ZrLniIinAR8GfsU4wb36Pjsm+30kSZKkrqrrynsdFlOmx3wMyIh4\nNUGV+z8AAAdsSURBVPAiYBi4NTNvabM4SZIkqW39FN7/CkhgO3AbcGz1GiAiYi1wWmZuaak+SZIk\nqVX9tFTkoUAA5wM7gfnA/pQbYVcDC4CVrVUnSZIktayfrryP3Iz6R+C1mXlP9fpnEfEG4OfAwoj4\n68z8/p6ccOTuX6kf2T/Vr+yb6lf2Tam/rrxvrba3jQnuAGTmMOXqO8DLGq1KkiRJ6hP9dOX9zmr7\n4AT7R8L9M3Z3oqbW2ZQkSZKa1E9X3r9DuUH1mAn2H1tt726mHEmSJKm/NB7eI2LfiDg6ImaPbc/M\nXwPfBJ4bEef2vOdk4BTK1fdrGitWkiRJ6iNRx6OGI+L1wKnVy8MoQXsjcGPVtiUzz6+OPYJy9XxT\nZs7uOc8sYB3wl8D1lCUjZwOvp6xAc0ZmXj3pgiVJkqQOqiu8XwB8aBeHbMrMo6pjj6AE+/9v6znX\nc6pzvQ74c+BhYC3wb5n5w0kXK0mSJHVULeFdkiRJ0tTrpxtWJUmSJO2C4V2SJEnqiIEK7xExKyK+\nEBG/iYjhiLg7Ij4eEQe2XZumt4jYFBE7J/i6t+36NNgiYmlEfDIi1kbEQ1W/u3g375kXEd+KiAci\n4vcR8eOIeE9EDNTPDbVvb/pnRByxi7F0Z0SsaLp+Da6IeHZEvCMivhYRv4iIbRHxYETcGBFvjwke\n+TvV42c/PaRpUqqlJ28GDgaupjz06WXAe4BTImJ+Zm7dxSmkqZSUB5B9HOj9z/5o8+VomvkAcByl\nr20G5uzq4GoFsSuBx4ArgP8FXkvpv/OAM6ayWE07e9U/K+spP+t7/bTGuqTTgc8C9wI3AL8G/gx4\nI/BfwBLgb8a+oYnxc2BuWI2I1cArgX/IzM+Maf8ocB5wUWa+u636NL1FxN1A9i6PKjUhIhYCmzPz\nrurPNwCXZuZbxzl2f+AuYH9gXmbeVrU/rXrfCcCZmbmysb+ABtpe9s+R5aa/lJlvb7hUTTMRcSKw\nX2au6mk/FPgB8BfAaZl5VdXeyPg5EL/+jIjnAYspy09+pmf3BcDvgbdExDMaL06SWpaZazLzrj08\n/HTKbzAvG/nBU51jO+UKaQB/W3+Vmq72sn9KjcnM7/YG96r9d8BFlPHwxDG7Ghk/B2XazKJqe23v\njsx8NCLWUcL9CZRPPlIbZkTEEPBcygfK24G1mbmz3bKkP3ESZZrX6nH2rQW2AfMi4qmZ+cdGK5NG\nHR4R7wSeAzwA3JyZP2m5Jk0vI+Pf42PaGhk/ByW8H035x/r5BPt/QQnvL8TwrvYcBoy9CSuAuyPi\nbZm5tqWapF5HV9snjKeZuaOaAnYM5enXdzZZmDTG4uprRETEd4GzM/OedkrSdBER+wBnU7LnNWN2\nNTJ+DsS0GeCAavvQBPtH2l11Rm35AvAKSoDfD3gx5VduRwLfiogXt1ea9CccT9XPtgEXAi8FDqq+\nFgLXU6YvXOcUWTXg34EXAasy87/HtDcyfg5KeN+dkdU9BuPuXHVOZv5rNXfu/swczswN1Q3UHwNm\nAsvarVDaY46nak01hi7LzPWZ+XD19T3gFOD7wPOBd7RbpQZZRPwj8F5gA/CEm6p39/ZqO6nxc1DC\n+8gnmQMm2P+snuOkfnFRtV3QahXSKMdTdU5m7qAs3Rc4nmqKRMTfAZ+gLEm6KDMf7DmkkfFzUML7\nnZT/sC+cYP8Lqu1Ec+Kltvyu2u7XahXSqJF5mE8YT6t5ns+j3KC1scmipD1wf7V1PFXtIuJc4FOU\nxSYWVSvO9Gpk/ByU8D5yE+rJvTsi4pnAfMpi+bc0WZS0B+ZVW4OQ+sX1lIshS8bZt5AyzWudK82o\nD7282jqeqlYR8c+Uaa4/Ak7KzC0THNrI+DkQ4T0zN1KWiTwyIv6+Z/eFlE/hX87MxxovTtNeRMyJ\niJnjtB8BfJoy9+2SxguTxnclsAV4U0S8dKQxImYAH6b018+2VJumuYg4frxH0kfEIuBcSv+8tPHC\nNLAi4oPARygPZXplZm7dxeGNjJ+D9ITV2cA64FDgG8D/UNZ1PxG4A5i/m39waUpExAXA+yhrvP4K\neAQ4Cng1MANYBbwxMx+f8CTSJFSP6z61enkY5ea+jcCNVduWzDy/5/ivAH8ALqc83vt1lF8FfyUz\n39RQ6ZoG9qZ/RsQNlKmwNwGbq/3HUZ73ksAHMvMjDZWuARcRZwNfpEx1+TTjz1XflJlfHvOeKR8/\nBya8A0TELMqV9iWUBzf8FrgKuHCcmwqkRkTEAuBdwPGMLhX5ILAeuDgzl7dYnqaB6gPkh3ZxyKbM\nPKrnPS8H3k+ZivB04JfA54FP5SD94FDr9qZ/RsTbgDcAx1KeZPlU4D5KmP/PzFw3xeVqGtmDvgmw\nJjMXjW2Y6vFzoMK7JEmSNMgGYs67JEmSNB0Y3iVJkqSOMLxLkiRJHWF4lyRJkjrC8C5JkiR1hOFd\nkiRJ6gjDuyRJktQRhndJkiSpIwzvkiRJUkcY3iVJkqSOMLxLkiRJHWF4lyRJkjrC8C5JkiR1hOFd\nkiRJ6gjDuyRJktQRhndJkiSpIwzvkiRJUkf8H35vuknFWq/AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1253ec8d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 255,
       "width": 375
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.plot(range(len(loss_track)), loss_track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing has begun...\n",
      "Testing is complete, with a Corpus BLEU4 score of 4.50012438178 and BLEU2 of 18.0784586556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions, actuals = [], []\n",
    "batch_size = 120\n",
    "fd_keys = [encoder_inputs, decoder_inputs, decoder_targets]\n",
    "try:\n",
    "    #batch_n = 0\n",
    "    print \"testing has begun...\"  \n",
    "    for s_batch, t_batch in batch_source_target(test_s, test_t, batch_size):\n",
    "        feed_dict = make_feed_dict(fd_keys, s_batch, t_batch)\n",
    "        predict_ = sess.run(decoder_prediction, feed_dict)\n",
    "        for i, (inp, act, pred) in enumerate(zip(feed_dict[encoder_inputs].T,\n",
    "                                                 feed_dict[decoder_targets].T,\n",
    "                                                 predict_.T)):\n",
    "            actuals.append([remove_EOS_PAD(act)])\n",
    "            predictions.append(remove_EOS_PAD(pred))\n",
    "    BLEU4 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    BLEU2 = bleu_score.corpus_bleu(actuals, predictions, weights=(0.5, 0.5))\n",
    "    print('Testing is complete, with a Corpus BLEU4 score of {} and BLEU2 of {}'.format(BLEU4*100, BLEU2*100))\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Observations:\n",
    "The above code generates a model with an increased BLEU score when it is trained on more data. Now I need to refactor the code so it is more original, efficient and generalisable.\n",
    "- <del>Simplify the generator batch stuff\n",
    "- not happy with tokenization: it leaves out `&apos;` etc.\n",
    "- Go through and change all variable names referring to en and fr to be source and target instead\n",
    "    - HAve to change the benchmark if I want to swap languages\n",
    "- Make it so it's not an interactive session\n",
    "- Sort phrases by length. Create batches of similar length. Randomise batches.\n",
    "- Translated language stats should be similar to target. Compare percent of text covered by vocab words (i.e. is `<UNK>` being overused?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDecommissioned functions\\n\\ndef unknown_word_replacer(tokenized_text, vocab_size, lang=\\'English\\'):\\n    \\'\\'\\' CURRENTLY UNUSED: This is the same as replace_with_word_id below \\n    but without replacing tokens with ids.\\n    We take the list of lists, find FreqDist, replace any\\n    out of vocabulary words with <UNK>, return new list of lists\\'\\'\\'\\n    flat = [item for sublist in tokenized_text for item in sublist]\\n    freq = nltk.FreqDist(flat)\\n    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/len(set(flat))))\\n    vocab = dict(freq.most_common(vocab_size))\\n    text = []\\n    for sentence in tokenized_text:\\n        new_sent = []\\n        for token in sentence:\\n            if token not in vocab.keys():\\n                new_sent.append(u\\'<UNK>\\')\\n            else:\\n                new_sent.append(token)\\n        text.append(new_sent)\\n    return text, vocab\\n\\nimport random\\nfrom PyDictionary import PyDictionary\\ndictionary = PyDictionary()\\n\\nnot_in_dictionary = []\\ndef bm_translate_word(word_list, s_to_t):\\n    global not_in_dictionary\\n    # The closer in length the candidate phrase or word is \\n    # to the actual\\n    keys = []\\n    lens = []\\n    #assert( type(word_list) == list), \"{} {}\".format(type(word_list), len(word_list))\\n    s = random.choice(word_list)\\n    for k in source_to_target.keys():\\n        if s in k:\\n            keys.append(k)\\n            lens.append(len(k))\\n        else:\\n            pass\\n    \\n    try:\\n        best_key = keys[lens.index(min(lens))]\\n        \\n        translations = s_to_t[best_key]\\n        if type(translations) != list:\\n            print best_key\\n            print translations\\n        word = random.choice(translations)\\n        return [word]\\n    except Exception as err:\\n        #print \"{} not in the dictionary\".format(s)\\n        try:\\n            words = dictionary.synonym(s)\\n        except UnicodeEncodeError:\\n            return [s]\\n        if words:\\n            return words\\n        else:\\n            \\n            not_in_dictionary.append(s)\\n            return [\\'<UNK>\\']\\n\\ns1 = \\'children\\'\\n#print bm_translate_word(s1, en_to_fr)\\n\\ndef bm_translate_corpus(s_text, s_to_t):\\n    trans_text = []\\n    for phrase in s_text:\\n        trans_phrase = []\\n        phrase = ids_to_phrases(phrase, id_to_word_s).split(\\' \\')\\n        for word in phrase:\\n            try:\\n                trans_attempt = bm_translate_word([word], s_to_t)\\n            except IndexError:\\n                print [word]\\n            while not trans_attempt:\\n                trans_attempt = bm_translate_word(trans_attempt, s_to_t)\\n            trans_phrase.append(random.choice(trans_attempt))\\n        trans_text.append(trans_phrase)\\n    return trans_text\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Decommissioned functions\n",
    "\n",
    "def unknown_word_replacer(tokenized_text, vocab_size, lang='English'):\n",
    "    ''' CURRENTLY UNUSED: This is the same as replace_with_word_id below \n",
    "    but without replacing tokens with ids.\n",
    "    We take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK>, return new list of lists'''\n",
    "    flat = [item for sublist in tokenized_text for item in sublist]\n",
    "    freq = nltk.FreqDist(flat)\n",
    "    print \"{} vocab size restricts to {} percent of total vocab.\".format(lang, 100*(float(vocab_size)/len(set(flat))))\n",
    "    vocab = dict(freq.most_common(vocab_size))\n",
    "    text = []\n",
    "    for sentence in tokenized_text:\n",
    "        new_sent = []\n",
    "        for token in sentence:\n",
    "            if token not in vocab.keys():\n",
    "                new_sent.append(u'<UNK>')\n",
    "            else:\n",
    "                new_sent.append(token)\n",
    "        text.append(new_sent)\n",
    "    return text, vocab\n",
    "\n",
    "import random\n",
    "from PyDictionary import PyDictionary\n",
    "dictionary = PyDictionary()\n",
    "\n",
    "not_in_dictionary = []\n",
    "def bm_translate_word(word_list, s_to_t):\n",
    "    global not_in_dictionary\n",
    "    # The closer in length the candidate phrase or word is \n",
    "    # to the actual\n",
    "    keys = []\n",
    "    lens = []\n",
    "    #assert( type(word_list) == list), \"{} {}\".format(type(word_list), len(word_list))\n",
    "    s = random.choice(word_list)\n",
    "    for k in source_to_target.keys():\n",
    "        if s in k:\n",
    "            keys.append(k)\n",
    "            lens.append(len(k))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        best_key = keys[lens.index(min(lens))]\n",
    "        \n",
    "        translations = s_to_t[best_key]\n",
    "        if type(translations) != list:\n",
    "            print best_key\n",
    "            print translations\n",
    "        word = random.choice(translations)\n",
    "        return [word]\n",
    "    except Exception as err:\n",
    "        #print \"{} not in the dictionary\".format(s)\n",
    "        try:\n",
    "            words = dictionary.synonym(s)\n",
    "        except UnicodeEncodeError:\n",
    "            return [s]\n",
    "        if words:\n",
    "            return words\n",
    "        else:\n",
    "            \n",
    "            not_in_dictionary.append(s)\n",
    "            return ['<UNK>']\n",
    "\n",
    "s1 = 'children'\n",
    "#print bm_translate_word(s1, en_to_fr)\n",
    "\n",
    "def bm_translate_corpus(s_text, s_to_t):\n",
    "    trans_text = []\n",
    "    for phrase in s_text:\n",
    "        trans_phrase = []\n",
    "        phrase = ids_to_phrases(phrase, id_to_word_s).split(' ')\n",
    "        for word in phrase:\n",
    "            try:\n",
    "                trans_attempt = bm_translate_word([word], s_to_t)\n",
    "            except IndexError:\n",
    "                print [word]\n",
    "            while not trans_attempt:\n",
    "                trans_attempt = bm_translate_word(trans_attempt, s_to_t)\n",
    "            trans_phrase.append(random.choice(trans_attempt))\n",
    "        trans_text.append(trans_phrase)\n",
    "    return trans_text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
